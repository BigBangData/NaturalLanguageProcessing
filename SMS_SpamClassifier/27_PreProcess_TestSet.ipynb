{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Test Set\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2021-02-17\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def load_data(data):\n",
    "    raw_path = os.path.join(\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "X_train = load_data(\"X_train\") \n",
    "y_train = load_data(\"y_train\") \n",
    "X_test = load_data(\"X_test\") \n",
    "y_test = load_data(\"y_test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_int(y_array):\n",
    "    y = y_array.copy()\n",
    "    y[y=='ham'] = 0\n",
    "    y[y=='spam'] = 1\n",
    "    y = y.astype('int')\n",
    "    return y\n",
    "\n",
    "y_test_int = make_int(y_test)\n",
    "y_train_int = make_int(y_train)\n",
    "\n",
    "# load contractions map for custom cleanup\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ham', 'spam', 'ham', ..., 'ham', 'ham', 'ham'], dtype=object),\n",
       " array([0, 1, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_train_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW and Tfidf\n",
    "\n",
    "INTENTIONAL ERROR FOR COMPARISON?\n",
    "\n",
    "- test data is different\n",
    "- IDF is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom.clean_preprocess as cp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe = Pipeline([('counter', cp.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bot', cp.WordCounterToVectorTransformer(vocabulary_size=2000)), # careful here\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))]) # very careful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counter = pipe['counter'].fit_transform(X_train) \n",
    "X_train_transformer = pipe['bot'].fit(X_train_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NUM', 'i', 'you', 'u', 'me', 'not', 'my', 'your', 'am', 'have']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for (ct, w) in enumerate(X_train_transformer.vocabulary_) if ct < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bot = X_train_transformer.transform(X_train_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counter = pipe['counter'].fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bot = X_train_transformer.transform(X_test_counter) # use train transformer for same vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1672x2001 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 24657 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [60,  7,  0,  0,  1,  1,  1,  0,  0,  0],\n",
       "       [24,  0,  0,  1,  0,  0,  0,  0,  1,  0],\n",
       "       [13,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [21,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [23,  0,  0,  0,  0,  0,  0,  1,  1,  0],\n",
       "       [11,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [12,  0,  1,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [12,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [12,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bot[:10,:10].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30,  0,  1,  2,  0,  0,  0,  1,  0,  0],\n",
       "       [26,  0,  1,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [15,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [83,  0,  1,  3,  0,  2,  2,  2,  2,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 8,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [11,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [14,  0,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 8,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [31,  0,  1,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_bot[:10,:10].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<3900x2001 sparse matrix of type '<class 'numpy.intc'>'\n",
       " \twith 59102 stored elements in Compressed Sparse Row format>,\n",
       " <1672x2001 sparse matrix of type '<class 'numpy.intc'>'\n",
       " \twith 24657 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bot, X_test_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_wrong = pipe['tfidf'].fit_transform(X_bot) # WRONGEDY WRONG WRONG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minute 8 in [Pre-processing our test data](https://www.youtube.com/watch?v=XWUi7RivDJY&list=PLTJTBoU5HOCR5Vkah2Z-AU76ZYsZjGFK6&index=11), here lies the problem:\n",
    "\n",
    "- The test data contains new n-grams... this is bad, we need the same n-grams from the training data (same features)\n",
    "- Each column has to be in the same order and have the same meaning, be the same \"word\", in R:\n",
    "\n",
    "\n",
    "```\n",
    "# Ensure the test dfm has the same n-grams as the training dfm.\n",
    "#\n",
    "# NOTE - In production we should expect that new text messages will \n",
    "#        contain n-grams that did not exist in the original training\n",
    "#        data. As such, we need to strip those n-grams out.\n",
    "#\n",
    "test.tokens.dfm <- dfm_select(test.tokens.dfm, pattern = train.tokens.dfm,\n",
    "                              selection = \"keep\")\n",
    "test.tokens.matrix <- as.matrix(test.tokens.dfm)\n",
    "test.tokens.dfm\n",
    "```\n",
    "\n",
    "- While I did select 2000 ngrams above, they are different 2000 ngrams than the training data. I should, potentially, choose a much higher vocabulary to try to accommodate for the fact that training and test data have diverging vocabularies, in an attempt to rescue as much vocabulary in the trainin as I can in the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "\n",
    "def perform_SVD(X, n_components=300): \n",
    "    \n",
    "    X_array = X.asfptype()\n",
    "    U, Sigma, VT = svds(X_array.T, \n",
    "                        k=n_components)\n",
    "    # reverse outputs\n",
    "    Sigma = Sigma[::-1]\n",
    "    U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "    \n",
    "    # return V\n",
    "    return VT.T\n",
    "\n",
    "# SVD with 800 components\n",
    "X_tfidf_svd = perform_SVD(X_tfidf_wrong, # YUP. WRONG.\n",
    "                          n_components=800) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X_tfidf_svd_allcos = cosine_similarity(X_tfidf_svd)\n",
    "\n",
    "test_df = pd.DataFrame({'sms':X_test, 'target':y_test}) # change\n",
    "\n",
    "# get spam indexes\n",
    "spam_ix = test_df.loc[test_df['target']=='spam'].index # change\n",
    "\n",
    "# calculate average spam similarity on SVD\n",
    "mean_spam_sims = []\n",
    "\n",
    "for ix in range(X_tfidf_svd_allcos.shape[0]):\n",
    "    mean_spam_sims.append(np.mean(X_tfidf_svd_allcos[ix, spam_ix]))\n",
    "\n",
    "X_tfidf_svd_spamcos = sp.hstack((csr_matrix(mean_spam_sims).T, X_tfidf_svd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
