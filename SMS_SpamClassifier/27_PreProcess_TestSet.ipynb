{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Test Set\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Pre-processing the test set in an NLP project is not trivial. One needs to be mindful of how the test set needs to have the same shape and format as the training set - so we actually need to pre-process both training and test sets \"together\" in a way.\n",
    "\n",
    "More precisely, while pre-processing the training set we created a vocabulary of ngrams - this needs to be the same vocabulary, in the same order, as the test set's vocabulary. We also created a tfidf representation, where idf stands for the 'inverse document frequency'... *in the training corpus*. The idf of the test corpus is irrelevant. So both the vocabulary and the idf of the training set need to be kept - in a way, they're part of the learning process.\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2021-02-18\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_ = time.time()\n",
    "dt_object = datetime.fromtimestamp(start_)\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def load_data(data):\n",
    "    raw_path = os.path.join(\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "X_train = load_data(\"X_train\") \n",
    "y_train = load_data(\"y_train\") \n",
    "X_test = load_data(\"X_test\") \n",
    "y_test = load_data(\"y_test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_int(y_array):\n",
    "    y = y_array.copy()\n",
    "    y[y=='ham'] = 0\n",
    "    y[y=='spam'] = 1\n",
    "    y = y.astype('int')\n",
    "    return y\n",
    "\n",
    "y_test_int = make_int(y_test)\n",
    "y_train_int = make_int(y_train)\n",
    "\n",
    "# load contractions map for custom cleanup\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom.clean_preprocess as cp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe = Pipeline([('counter', cp.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bot', cp.WordCounterToVectorTransformer(vocabulary_size=2000)), # careful\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))]) # careful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counters & Transformer\n",
    "\n",
    "I create train and test counters but only need to fit the transformer on the training corpus to get the same 2,000 features in both data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counters\n",
    "X_train_counter = pipe['counter'].fit_transform(X_train) \n",
    "X_test_counter = pipe['counter'].fit_transform(X_test) \n",
    "\n",
    "X_train_transformer = pipe['bot'].fit(X_train_counter) # only on the training counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 11 words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NUM', 'i', 'you', 'u', 'me', 'not', 'my', 'your', 'am', 'have', 'call']\n"
     ]
    }
   ],
   "source": [
    "first_11_vocab = [w for (ct, w) in enumerate(X_train_transformer.vocabulary_) if ct < 11]\n",
    "print(first_11_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-upto-Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoTs\n",
    "X_train_bot = X_train_transformer.transform(X_train_counter)\n",
    "X_test_bot = X_train_transformer.transform(X_test_counter) # same transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<3900x2001 sparse matrix of type '<class 'numpy.intc'>'\n",
       " \twith 59102 stored elements in Compressed Sparse Row format>,\n",
       " <1672x2001 sparse matrix of type '<class 'numpy.intc'>'\n",
       " \twith 24657 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity checks\n",
    "X_train_bot, X_test_bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Bag of Trigrams have the same vocabulary.\n",
    "\n",
    "__Train BoT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unknown</th>\n",
       "      <th>NUM</th>\n",
       "      <th>i</th>\n",
       "      <th>you</th>\n",
       "      <th>u</th>\n",
       "      <th>me</th>\n",
       "      <th>not</th>\n",
       "      <th>my</th>\n",
       "      <th>your</th>\n",
       "      <th>am</th>\n",
       "      <th>have</th>\n",
       "      <th>call</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unknown  NUM  i  you  u  me  not  my  your  am  have  call\n",
       "0       16    0  0    0  0   0    0   0     0   0     0     0\n",
       "1       60    7  0    0  1   1    1   0     0   0     0     1\n",
       "2       24    0  0    1  0   0    0   0     1   0     0     0\n",
       "3       13    0  0    0  0   0    0   0     0   0     0     0\n",
       "4       21    1  0    0  0   0    0   0     0   0     0     0\n",
       "5       23    0  0    0  0   0    0   1     1   0     0     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train BoT\n",
    "pd.DataFrame(X_train_bot[0:6, 0:12].toarray() \n",
    "            , columns=['unknown'] + first_11_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single line with a big meaning::::: \\Miss anything 4 ur \\\"Best Life\\\" but\n"
     ]
    }
   ],
   "source": [
    "print(X_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'single': 1, 'line': 1, 'big': 1, 'meaning': 1, 'miss': 1, 'anything': 1, 'NUM': 1, 'ur': 1, 'best': 1, 'life': 1, 'but': 1, 'single_line': 1, 'line_with': 1, 'with_a': 1, 'a_big': 1, 'big_meaning': 1, 'meaning_miss': 1, 'miss_anything': 1, 'anything_NUM': 1, 'NUM_ur': 1, 'ur_best': 1, 'best_life': 1, 'life_but': 1, 'single_line_with': 1, 'line_with_a': 1, 'with_a_big': 1, 'a_big_meaning': 1, 'big_meaning_miss': 1, 'meaning_miss_anything': 1, 'miss_anything_NUM': 1, 'anything_NUM_ur': 1, 'NUM_ur_best': 1, 'ur_best_life': 1, 'best_life_but': 1})\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counter[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test BoT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unknown</th>\n",
       "      <th>NUM</th>\n",
       "      <th>i</th>\n",
       "      <th>you</th>\n",
       "      <th>u</th>\n",
       "      <th>me</th>\n",
       "      <th>not</th>\n",
       "      <th>my</th>\n",
       "      <th>your</th>\n",
       "      <th>am</th>\n",
       "      <th>have</th>\n",
       "      <th>call</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unknown  NUM  i  you  u  me  not  my  your  am  have  call\n",
       "0       30    0  1    2  0   0    0   1     0   0     0     0\n",
       "1       26    0  1    1  0   0    0   0     0   0     0     0\n",
       "2       15    0  0    0  0   0    0   0     0   0     0     0\n",
       "3       83    0  1    3  0   2    2   2     2   0     1     0\n",
       "4        6    0  0    0  0   0    0   0     0   0     0     0\n",
       "5        8    0  0    0  0   0    0   0     0   0     0     0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test BoT\n",
    "pd.DataFrame(X_test_bot[0:6, 0:12].toarray() \n",
    "            , columns=['unknown'] + first_11_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any chance you might have had with me evaporated as soon as you violated my privacy by stealing my phone number from your employer's paperwork. Not cool at all. Please do not contact me again or I will report you to your supervisor.\n"
     ]
    }
   ],
   "source": [
    "print(X_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'you': 3, 'me': 2, 'my': 2, 'your': 2, 'not': 2, 'any': 1, 'chance': 1, 'might': 1, 'have': 1, 'had': 1, 'evaporated': 1, 'soon': 1, 'violated': 1, 'privacy': 1, 'stealing': 1, 'phone': 1, 'number': 1, 'employer': 1, 'paperwork': 1, 'cool': 1, 'all': 1, 'please': 1, 'do': 1, 'contact': 1, 'again': 1, 'or': 1, 'i': 1, 'report': 1, 'supervisor': 1, 'any_chance': 1, 'chance_you': 1, 'you_might': 1, 'might_have': 1, 'have_had': 1, 'had_with': 1, 'with_me': 1, 'me_evaporated': 1, 'evaporated_as': 1, 'as_soon': 1, 'soon_as': 1, 'as_you': 1, 'you_violated': 1, 'violated_my': 1, 'my_privacy': 1, 'privacy_by': 1, 'by_stealing': 1, 'stealing_my': 1, 'my_phone': 1, 'phone_number': 1, 'number_from': 1, 'from_your': 1, 'your_employers': 1, 'employers_paperwork': 1, 'paperwork_not': 1, 'not_cool': 1, 'cool_at': 1, 'at_all': 1, 'all_please': 1, 'please_do': 1, 'do_not': 1, 'not_contact': 1, 'contact_me': 1, 'me_again': 1, 'again_or': 1, 'or_i': 1, 'i_will': 1, 'will_report': 1, 'report_you': 1, 'you_to': 1, 'to_your': 1, 'your_supervisor': 1, 'any_chance_you': 1, 'chance_you_might': 1, 'you_might_have': 1, 'might_have_had': 1, 'have_had_with': 1, 'had_with_me': 1, 'with_me_evaporated': 1, 'me_evaporated_as': 1, 'evaporated_as_soon': 1, 'as_soon_as': 1, 'soon_as_you': 1, 'as_you_violated': 1, 'you_violated_my': 1, 'violated_my_privacy': 1, 'my_privacy_by': 1, 'privacy_by_stealing': 1, 'by_stealing_my': 1, 'stealing_my_phone': 1, 'my_phone_number': 1, 'phone_number_from': 1, 'number_from_your': 1, 'from_your_employers': 1, 'your_employers_paperwork': 1, 'employers_paperwork_not': 1, 'paperwork_not_cool': 1, 'not_cool_at': 1, 'cool_at_all': 1, 'at_all_please': 1, 'all_please_do': 1, 'please_do_not': 1, 'do_not_contact': 1, 'not_contact_me': 1, 'contact_me_again': 1, 'me_again_or': 1, 'again_or_i': 1, 'or_i_will': 1, 'i_will_report': 1, 'will_report_you': 1, 'report_you_to': 1, 'you_to_your': 1, 'to_your_supervisor': 1})\n"
     ]
    }
   ],
   "source": [
    "print(X_test_counter[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Representation\n",
    "\n",
    "The `fit` method of Scikit-Learn's **TfidfTransformer** learns the idf vector, from the [source](https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/feature_extraction/text.py#L1314):\n",
    "\n",
    "```\n",
    "1429  def fit(self, X, y=None):\n",
    "1430       \"\"\"Learn the idf vector (global term weights).\n",
    "```\n",
    "\n",
    "We need to learn the same corpus, thus use only the training set to learn the idf vector of \"inverse document frequencies\" for that corpus. In short, we cannot use `fit_transform` but need to separate `fit` and `transform` so that we can fit on the training BoT and transform each BoT in turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "X_train_fit = pipe['tfidf'].fit(X_train_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.01576056 2.34733003 1.98691455 ... 7.07176363 7.07176363 7.07176363]\n"
     ]
    }
   ],
   "source": [
    "# vector of idfs\n",
    "print(X_train_fit.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "X_train_tfidf = X_train_fit.transform(X_train_bot)\n",
    "X_test_tfidf = X_train_fit.transform(X_test_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<3900x2001 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 59102 stored elements in Compressed Sparse Row format>,\n",
       " <1672x2001 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 24657 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity checks\n",
    "X_train_tfidf, X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the tfidf values to be different, of course, and a higher proportion of unkonwn tokens overall - this is the first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26829044 0.         0.         0.         0.        ]\n",
      " [0.15615824 0.20867906 0.         0.         0.08553675]\n",
      " [0.20873123 0.         0.         0.11189586 0.        ]\n",
      " [0.27765099 0.         0.         0.         0.        ]\n",
      " [0.20303381 0.11600693 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[:5,:5].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2312656  0.         0.1027846  0.19926706 0.        ]\n",
      " [0.25345525 0.         0.11643234 0.13331726 0.        ]\n",
      " [0.66482534 0.         0.         0.         0.        ]\n",
      " [0.19054668 0.         0.06878327 0.16528286 0.        ]\n",
      " [0.35660928 0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_tfidf[:5,:5].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2254668870816081\n",
      "0.23216159731868138\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X_train_tfidf[:,0:1].toarray()))\n",
    "print(np.mean(X_test_tfidf[:,0:1].toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Projection\n",
    "\n",
    "I changed the default to 800 components, also perform no scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "\n",
    "def perform_SVD(X, n_components=800): \n",
    "    \n",
    "    X_array = X.asfptype()\n",
    "    U, Sigma, VT = svds(X_array.T, \n",
    "                        k=n_components)\n",
    "    # reverse outputs\n",
    "    Sigma = Sigma[::-1]\n",
    "    U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "    \n",
    "    # return V\n",
    "    return VT.T\n",
    "\n",
    "X_train_svd = perform_SVD(X_train_tfidf)\n",
    "X_test_svd = perform_SVD(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calc_cossims(X, y, svd):\n",
    "    # all similarities\n",
    "    similarities = cosine_similarity(svd)\n",
    "    # spam similarities\n",
    "    df = pd.DataFrame({'sms':X, 'target':y}) \n",
    "    spam_ix = df.loc[df['target']=='spam'].index\n",
    "    # mean spam sims\n",
    "    mean_spam_sims = []\n",
    "    for ix in range(similarities.shape[0]):\n",
    "        mean_spam_sim = np.mean(similarities[ix, spam_ix])\n",
    "        mean_spam_sims.append(mean_spam_sim)\n",
    "    return csr_matrix(mean_spam_sims).T\n",
    "\n",
    "# calculate the mean spam similarities features\n",
    "X_train_spamcos = calc_cossims(X_train, y_train, X_train_svd)\n",
    "X_test_spamcos = calc_cossims(X_test, y_test, X_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<3900x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 3899 stored elements in Compressed Sparse Column format>,\n",
       " <1672x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1672 stored elements in Compressed Sparse Column format>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "X_train_spamcos, X_test_spamcos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack\n",
    "\n",
    "- add the spam cosine similarities feature to the SVD projection of the tfidf-ed bag-of-upto-trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack cossim feature\n",
    "X_train_processed = sp.hstack((X_train_spamcos, X_train_svd))\n",
    "X_test_processed = sp.hstack((X_test_spamcos, X_test_svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<3900x801 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 3123099 stored elements in COOrdinate format>,\n",
       " <1672x801 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1339272 stored elements in COOrdinate format>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "X_train_processed, X_test_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist(X, filename):\n",
    "    proc_dir = os.path.join(\"data\", \"2_processed\")\n",
    "    filename = ''.join([filename, '.npz'])\n",
    "    sp.save_npz(os.path.join(proc_dir, filename), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist(X_train_processed, 'X_train_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist(X_test_processed, 'X_test_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0 m 41 s\n"
     ]
    }
   ],
   "source": [
    "m, s = divmod(time.time() - start_, 60)\n",
    "print(f'Elapsed: {m:0.0f} m {s:0.0f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
