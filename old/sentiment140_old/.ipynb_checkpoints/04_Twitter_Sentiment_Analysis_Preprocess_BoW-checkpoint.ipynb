{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis \n",
    "\n",
    "---\n",
    "\n",
    "###  Pre-process cleaned data for machine learning \n",
    "\n",
    "### Part 1: Bag of Words\n",
    "\n",
    "While cleanup involved simply reformatting a Tweet's text by standardizing it and reducing the feature space (less punctuation, replacing usernames and URLs, lower casing, tokenizing, lemmatizing, etc.), pre-processing for machine learning is often more involved. It mainly consists of further data cleanup steps such as imputing NAs, but also feature engineering, and perhaps most importantly, a method of representing text in numerical form, such as [Document Term Matrices](./01_Document_Term_Matrices.ipynb), since most machine-learning algorithms do not accept text as input. This notebook explores the creation of a simple Bag of Words Document-Frequency Matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### Load cleaned TRAIN data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# for ML preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# custom (see script)\n",
    "import loading_module as lm\n",
    "\n",
    "start_time = time.time()\n",
    "X_train, y_train = lm.load_clean_data('X_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save target as npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_dir = os.path.join(\"..\",\"data\",\"3_processed\",\"sentiment140\")\n",
    "y_filepath = os.path.join(proc_dir, \"y_train.npy\")\n",
    "\n",
    "with open(y_filepath, 'wb') as f:\n",
    "    np.save(f, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1199999, 3), (1199999, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISSUE:\n",
    "\n",
    "- loading_module is not reproducible!\n",
    "- probably due to async multiprocessing, order is not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pareidoliac</td>\n",
       "      <td>@Auckland_Museum I was thinking precisely of y...</td>\n",
       "      <td>USERNAME i thinking precisely your late progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sugarloves</td>\n",
       "      <td>@hp4ever13 Something HP... I *heart* mine- The...</td>\n",
       "      <td>USERNAME something hp i heart mine theyre suga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>joshhl</td>\n",
       "      <td>So annoyed that America have The Sims 3 already</td>\n",
       "      <td>so annoyed america have sims 3 already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ValbuenaMusic</td>\n",
       "      <td>NEWEST SONG &amp;quot;Hey Mr. Bossa!!!&amp;quot; @yout...</td>\n",
       "      <td>newest song hey mr bossa USERNAME URL swing ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JackieHagerman</td>\n",
       "      <td>Got up a 5 a.m. to workout and now I'm exhaust...</td>\n",
       "      <td>got up 5 am workout now im exhausted really wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         username                                               text  \\\n",
       "0     pareidoliac  @Auckland_Museum I was thinking precisely of y...   \n",
       "1      sugarloves  @hp4ever13 Something HP... I *heart* mine- The...   \n",
       "2          joshhl   So annoyed that America have The Sims 3 already    \n",
       "3   ValbuenaMusic  NEWEST SONG &quot;Hey Mr. Bossa!!!&quot; @yout...   \n",
       "4  JackieHagerman  Got up a 5 a.m. to workout and now I'm exhaust...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  USERNAME i thinking precisely your late progra...  \n",
       "1  USERNAME something hp i heart mine theyre suga...  \n",
       "2             so annoyed america have sims 3 already  \n",
       "3  newest song hey mr bossa USERNAME URL swing ja...  \n",
       "4  got up 5 am workout now im exhausted really wi...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1199994</th>\n",
       "      <td>taylaar</td>\n",
       "      <td>3 finals this week  But Andrew is here, so I a...</td>\n",
       "      <td>3 final this week but andrew here so i am happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199995</th>\n",
       "      <td>Pewari</td>\n",
       "      <td>@carocat yeah, I have a whole list of tv shows...</td>\n",
       "      <td>USERNAME yeah i have whole list tv show i real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199996</th>\n",
       "      <td>jamieallison</td>\n",
       "      <td>Sorry you missed it. Good night!</td>\n",
       "      <td>sorry you missed good night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199997</th>\n",
       "      <td>alisha_J</td>\n",
       "      <td>Yo this 16 and pregnant show on mtv is sad to ...</td>\n",
       "      <td>yo this 16 pregnant show mtv sad me i feel bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199998</th>\n",
       "      <td>Shelbayyyyy</td>\n",
       "      <td>Bed. SAT in the am</td>\n",
       "      <td>bed sat am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             username                                               text  \\\n",
       "1199994       taylaar  3 finals this week  But Andrew is here, so I a...   \n",
       "1199995        Pewari  @carocat yeah, I have a whole list of tv shows...   \n",
       "1199996  jamieallison                  Sorry you missed it. Good night!    \n",
       "1199997      alisha_J  Yo this 16 and pregnant show on mtv is sad to ...   \n",
       "1199998   Shelbayyyyy                                Bed. SAT in the am    \n",
       "\n",
       "                                                lemmatized  \n",
       "1199994    3 final this week but andrew here so i am happy  \n",
       "1199995  USERNAME yeah i have whole list tv show i real...  \n",
       "1199996                        sorry you missed good night  \n",
       "1199997  yo this 16 pregnant show mtv sad me i feel bad...  \n",
       "1199998                                         bed sat am  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       1\n",
       "4       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1199994</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199995</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199997</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199998</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target\n",
       "1199994       0\n",
       "1199995       1\n",
       "1199996       0\n",
       "1199997       0\n",
       "1199998       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for emojis, get `NaNs` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ix = []\n",
    "emoji_ix = []\n",
    "for i, tweet in enumerate(X_train['lemmatized'][:len(X_train)]):\n",
    "    try:\n",
    "        m = re.search(r'EMOJI', tweet)\n",
    "        if m:\n",
    "            emoji_ix.append(i)\n",
    "    except TypeError as e: \n",
    "        error_ix.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43379</th>\n",
       "      <td>rooroocachoo</td>\n",
       "      <td>It will</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57035</th>\n",
       "      <td>sangofsorrow</td>\n",
       "      <td>He is...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79899</th>\n",
       "      <td>SquarahFaggins</td>\n",
       "      <td>to it!!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130300</th>\n",
       "      <td>geegeeludlow</td>\n",
       "      <td>is in IT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310320</th>\n",
       "      <td>LukeOgle</td>\n",
       "      <td>is in IT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474958</th>\n",
       "      <td>dianamra</td>\n",
       "      <td>and it was</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676761</th>\n",
       "      <td>Spacegirlspif13</td>\n",
       "      <td>Is...                                         ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929405</th>\n",
       "      <td>Jmoux</td>\n",
       "      <td>are on..</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956389</th>\n",
       "      <td>ChickWithAName</td>\n",
       "      <td>. . . . . and it's on!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028263</th>\n",
       "      <td>WMonk</td>\n",
       "      <td>It will</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                username                                               text  \\\n",
       "43379       rooroocachoo                                           It will    \n",
       "57035       sangofsorrow                                       He is...       \n",
       "79899     SquarahFaggins                                           to it!!    \n",
       "130300      geegeeludlow                                          is in IT    \n",
       "310320          LukeOgle                                          is in IT    \n",
       "474958          dianamra                                        and it was    \n",
       "676761   Spacegirlspif13  Is...                                         ...   \n",
       "929405             Jmoux                                          are on..    \n",
       "956389    ChickWithAName                            . . . . . and it's on!    \n",
       "1028263            WMonk                                           It will    \n",
       "\n",
       "        lemmatized  \n",
       "43379          NaN  \n",
       "57035          NaN  \n",
       "79899          NaN  \n",
       "130300         NaN  \n",
       "310320         NaN  \n",
       "474958         NaN  \n",
       "676761         NaN  \n",
       "929405         NaN  \n",
       "956389         NaN  \n",
       "1028263        NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only a few NaNs\n",
    "X_train.iloc[error_ix, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>PolaScheps</td>\n",
       "      <td>@ddlovato hahaha can't wait to c it! u rock gi...</td>\n",
       "      <td>USERNAME hahaha cant wait c u rock girl i real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>barbbs</td>\n",
       "      <td>at psychology class iÃÂ´m STARVING 2 DEATH i ...</td>\n",
       "      <td>psychology class i EMOJI m starving 2 death i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>bit_crusherrr</td>\n",
       "      <td>@busta_grimes I know but Asda doesnt sell Mika...</td>\n",
       "      <td>USERNAME i know but asda doesnt sell mikado li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>ligiagalvao</td>\n",
       "      <td>@tommcfly You're reading the saga of twilight ...</td>\n",
       "      <td>USERNAME youre reading saga twilight i read al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>ruoivietnam</td>\n",
       "      <td>@Poohnine: em cÃ¯Â¿Â½ t?a ?? ti?ng Anh khÃ¯Â¿Â...</td>\n",
       "      <td>USERNAME em c EMOJI ta ting anh kh EMOJI ng an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username                                               text  \\\n",
       "244     PolaScheps  @ddlovato hahaha can't wait to c it! u rock gi...   \n",
       "510         barbbs  at psychology class iÃÂ´m STARVING 2 DEATH i ...   \n",
       "651  bit_crusherrr  @busta_grimes I know but Asda doesnt sell Mika...   \n",
       "657    ligiagalvao  @tommcfly You're reading the saga of twilight ...   \n",
       "972    ruoivietnam  @Poohnine: em cÃ¯Â¿Â½ t?a ?? ti?ng Anh khÃ¯Â¿Â...   \n",
       "\n",
       "                                            lemmatized  \n",
       "244  USERNAME hahaha cant wait c u rock girl i real...  \n",
       "510  psychology class i EMOJI m starving 2 death i ...  \n",
       "651  USERNAME i know but asda doesnt sell mikado li...  \n",
       "657  USERNAME youre reading saga twilight i read al...  \n",
       "972  USERNAME em c EMOJI ta ting anh kh EMOJI ng an...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emojis\n",
    "X_train.iloc[emoji_ix[:5], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10856"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emoji_ix) # could be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute NAs created during cleanup\n",
    "\n",
    "We do not want to drop since the fact they ended up as empty strings is possibly informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username       0\n",
       "text           0\n",
       "lemmatized    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with NULL as a string\n",
    "error_ix = []\n",
    "NULL_ix = []\n",
    "for i, tweet in enumerate(X_train['lemmatized'][:len(X_train)]):\n",
    "    try:\n",
    "        m = re.search(r'NULL', tweet)\n",
    "        if m:\n",
    "            NULL_ix.append(i)\n",
    "    except TypeError as e: \n",
    "        error_ix.append(i)\n",
    "\n",
    "NA_ix = X_train.loc[X_train['lemmatized'].isnull(), ].index\n",
    "X_train['lemmatized'].loc[list(NA_ix), ] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username      0\n",
       "text          0\n",
       "lemmatized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43379</th>\n",
       "      <td>rooroocachoo</td>\n",
       "      <td>It will</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57035</th>\n",
       "      <td>sangofsorrow</td>\n",
       "      <td>He is...</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79899</th>\n",
       "      <td>SquarahFaggins</td>\n",
       "      <td>to it!!</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130300</th>\n",
       "      <td>geegeeludlow</td>\n",
       "      <td>is in IT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310320</th>\n",
       "      <td>LukeOgle</td>\n",
       "      <td>is in IT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474958</th>\n",
       "      <td>dianamra</td>\n",
       "      <td>and it was</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676761</th>\n",
       "      <td>Spacegirlspif13</td>\n",
       "      <td>Is...                                         ...</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929405</th>\n",
       "      <td>Jmoux</td>\n",
       "      <td>are on..</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956389</th>\n",
       "      <td>ChickWithAName</td>\n",
       "      <td>. . . . . and it's on!</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028263</th>\n",
       "      <td>WMonk</td>\n",
       "      <td>It will</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                username                                               text  \\\n",
       "43379       rooroocachoo                                           It will    \n",
       "57035       sangofsorrow                                       He is...       \n",
       "79899     SquarahFaggins                                           to it!!    \n",
       "130300      geegeeludlow                                          is in IT    \n",
       "310320          LukeOgle                                          is in IT    \n",
       "474958          dianamra                                        and it was    \n",
       "676761   Spacegirlspif13  Is...                                         ...   \n",
       "929405             Jmoux                                          are on..    \n",
       "956389    ChickWithAName                            . . . . . and it's on!    \n",
       "1028263            WMonk                                           It will    \n",
       "\n",
       "        lemmatized  \n",
       "43379         NULL  \n",
       "57035         NULL  \n",
       "79899         NULL  \n",
       "130300        NULL  \n",
       "310320        NULL  \n",
       "474958        NULL  \n",
       "676761        NULL  \n",
       "929405        NULL  \n",
       "956389        NULL  \n",
       "1028263       NULL  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_ix = []\n",
    "for i, tweet in enumerate(X_train['lemmatized'][:len(X_train)]):\n",
    "    try:\n",
    "        m = re.search(r'NULL', tweet)\n",
    "        if m:\n",
    "            NULL_ix.append(i)\n",
    "    except TypeError as e: \n",
    "        continue\n",
    "\n",
    "X_train.iloc[NULL_ix, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exam doneeeee\n"
     ]
    }
   ],
   "source": [
    "print(X_train.loc[4, 'lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BoW DFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatized column (2) as array, ravel will flatten the structure\n",
    "X_array = np.array(X_train.iloc[:, 2]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    please pray my house there major water leakage...\n",
       "1                                        URL bump this\n",
       "2         USERNAME just saw not part your last message\n",
       "3    USERNAME lol i put up 100 track u havent retwe...\n",
       "4    USERNAME u got ta endorse application if u gon...\n",
       "5    best night ever got spend better part half hou...\n",
       "6    moved inside i am wayyyyy too white sit sun to...\n",
       "7    USERNAME wish i could your signing nashville b...\n",
       "8                             me USERNAME hummin along\n",
       "9     USERNAME wrong suggestion doesnt translate farsi\n",
       "Name: lemmatized, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:10, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['please pray my house there major water leakage thats causing my entire house crack possibly fall apart',\n",
       "       'URL bump this', 'USERNAME just saw not part your last message',\n",
       "       'USERNAME lol i put up 100 track u havent retweeted 1 hun smh',\n",
       "       'USERNAME u got ta endorse application if u gon na our courtside tweep USERNAME im USERNAME i USERNAME',\n",
       "       'best night ever got spend better part half hour letting off firework',\n",
       "       'moved inside i am wayyyyy too white sit sun too long literally im reflective haha loungin sofa',\n",
       "       'USERNAME wish i could your signing nashville but i got sick cant go 3 hr drive',\n",
       "       'me USERNAME hummin along',\n",
       "       'USERNAME wrong suggestion doesnt translate farsi'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USERNAME u got ta endorse application if u gon na our courtside tweep USERNAME im USERNAME i USERNAME'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit A Geron\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "\n",
    "class DocumentToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, replace_numbers=True):\n",
    "        self.replace_numbers = replace_numbers\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', doc)\n",
    "            word_counts = Counter(doc.split())\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 2, 'house': 2, 'please': 1, 'pray': 1, 'there': 1, 'major': 1, 'water': 1, 'leakage': 1, 'thats': 1, 'causing': 1, 'entire': 1, 'crack': 1, 'possibly': 1, 'fall': 1, 'apart': 1}\n",
      "{'URL': 1, 'bump': 1, 'this': 1}\n",
      "{'USERNAME': 1, 'just': 1, 'saw': 1, 'not': 1, 'part': 1, 'your': 1, 'last': 1, 'message': 1}\n",
      "{'NUMBER': 2, 'USERNAME': 1, 'lol': 1, 'i': 1, 'put': 1, 'up': 1, 'track': 1, 'u': 1, 'havent': 1, 'retweeted': 1, 'hun': 1, 'smh': 1}\n",
      "{'USERNAME': 4, 'u': 2, 'got': 1, 'ta': 1, 'endorse': 1, 'application': 1, 'if': 1, 'gon': 1, 'na': 1, 'our': 1, 'courtside': 1, 'tweep': 1, 'im': 1, 'i': 1}\n",
      "{'best': 1, 'night': 1, 'ever': 1, 'got': 1, 'spend': 1, 'better': 1, 'part': 1, 'half': 1, 'hour': 1, 'letting': 1, 'off': 1, 'firework': 1}\n",
      "{'too': 2, 'moved': 1, 'inside': 1, 'i': 1, 'am': 1, 'wayyyyy': 1, 'white': 1, 'sit': 1, 'sun': 1, 'long': 1, 'literally': 1, 'im': 1, 'reflective': 1, 'haha': 1, 'loungin': 1, 'sofa': 1}\n",
      "{'i': 2, 'USERNAME': 1, 'wish': 1, 'could': 1, 'your': 1, 'signing': 1, 'nashville': 1, 'but': 1, 'got': 1, 'sick': 1, 'cant': 1, 'go': 1, 'NUMBER': 1, 'hr': 1, 'drive': 1}\n",
      "{'me': 1, 'USERNAME': 1, 'hummin': 1, 'along': 1}\n",
      "{'USERNAME': 1, 'wrong': 1, 'suggestion': 1, 'doesnt': 1, 'translate': 1, 'farsi': 1}\n"
     ]
    }
   ],
   "source": [
    "X_few = X_array[:10]\n",
    "X_few_wordcounts = DocumentToWordCounterTransformer().fit_transform(X_few)\n",
    "for i in X_few_wordcounts:\n",
    "    print(str(i).split('Counter(')[1].split(')')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 5) # minimum count\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows, cols, data = [], [], []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 105 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer()\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 2, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USERNAME': 1,\n",
       " 'i': 2,\n",
       " 'NUMBER': 3,\n",
       " 'u': 4,\n",
       " 'got': 5,\n",
       " 'my': 6,\n",
       " 'house': 7,\n",
       " 'part': 8,\n",
       " 'your': 9,\n",
       " 'im': 10,\n",
       " 'too': 11,\n",
       " 'please': 12,\n",
       " 'pray': 13,\n",
       " 'there': 14,\n",
       " 'major': 15,\n",
       " 'water': 16,\n",
       " 'leakage': 17,\n",
       " 'thats': 18,\n",
       " 'causing': 19,\n",
       " 'entire': 20,\n",
       " 'crack': 21,\n",
       " 'possibly': 22,\n",
       " 'fall': 23,\n",
       " 'apart': 24,\n",
       " 'URL': 25,\n",
       " 'bump': 26,\n",
       " 'this': 27,\n",
       " 'just': 28,\n",
       " 'saw': 29,\n",
       " 'not': 30,\n",
       " 'last': 31,\n",
       " 'message': 32,\n",
       " 'lol': 33,\n",
       " 'put': 34,\n",
       " 'up': 35,\n",
       " 'track': 36,\n",
       " 'havent': 37,\n",
       " 'retweeted': 38,\n",
       " 'hun': 39,\n",
       " 'smh': 40,\n",
       " 'ta': 41,\n",
       " 'endorse': 42,\n",
       " 'application': 43,\n",
       " 'if': 44,\n",
       " 'gon': 45,\n",
       " 'na': 46,\n",
       " 'our': 47,\n",
       " 'courtside': 48,\n",
       " 'tweep': 49,\n",
       " 'best': 50,\n",
       " 'night': 51,\n",
       " 'ever': 52,\n",
       " 'spend': 53,\n",
       " 'better': 54,\n",
       " 'half': 55,\n",
       " 'hour': 56,\n",
       " 'letting': 57,\n",
       " 'off': 58,\n",
       " 'firework': 59,\n",
       " 'moved': 60,\n",
       " 'inside': 61,\n",
       " 'am': 62,\n",
       " 'wayyyyy': 63,\n",
       " 'white': 64,\n",
       " 'sit': 65,\n",
       " 'sun': 66,\n",
       " 'long': 67,\n",
       " 'literally': 68,\n",
       " 'reflective': 69,\n",
       " 'haha': 70,\n",
       " 'loungin': 71,\n",
       " 'sofa': 72,\n",
       " 'wish': 73,\n",
       " 'could': 74,\n",
       " 'signing': 75,\n",
       " 'nashville': 76,\n",
       " 'but': 77,\n",
       " 'sick': 78,\n",
       " 'cant': 79,\n",
       " 'go': 80,\n",
       " 'hr': 81,\n",
       " 'drive': 82,\n",
       " 'me': 83,\n",
       " 'hummin': 84,\n",
       " 'along': 85,\n",
       " 'wrong': 86,\n",
       " 'suggestion': 87,\n",
       " 'doesnt': 88,\n",
       " 'translate': 89,\n",
       " 'farsi': 90}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#\n",
    "#preprocess_pipeline = Pipeline([\n",
    "#    (\"document_to_wordcount\", DocumentToWordCounterTransformer()),\n",
    "#    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "#])\n",
    "\n",
    "# lose the vocabulary_ ? WHERE IS THE VOCABULARY..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_transformed = preprocess_pipeline.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1199999x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 10232362 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 [[2 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "32 [[1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(X_train_transformed[:33,:30].todense()):\n",
    "    if i > 30:\n",
    "        print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wordcounts = DocumentToWordCounterTransformer().fit_transform(X_array)\n",
    "\n",
    "vocabulary_transformer = WordCounterToVectorTransformer()\n",
    "X_vectors = vocabulary_transformer.fit_transform(X_wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USERNAME': 1,\n",
       " 'i': 2,\n",
       " 'my': 3,\n",
       " 'you': 4,\n",
       " 'NUMBER': 5,\n",
       " 'im': 6,\n",
       " 'me': 7,\n",
       " 'so': 8,\n",
       " 'have': 9,\n",
       " 'but': 10,\n",
       " 'just': 11,\n",
       " 'not': 12,\n",
       " 'day': 13,\n",
       " 'this': 14,\n",
       " 'now': 15,\n",
       " 'good': 16,\n",
       " 'up': 17,\n",
       " 'get': 18,\n",
       " 'URL': 19,\n",
       " 'all': 20,\n",
       " 'out': 21,\n",
       " 'like': 22,\n",
       " 'go': 23,\n",
       " 'no': 24,\n",
       " 'got': 25,\n",
       " 'u': 26,\n",
       " 'love': 27,\n",
       " 'dont': 28,\n",
       " 'work': 29,\n",
       " 'do': 30,\n",
       " 'today': 31,\n",
       " 'your': 32,\n",
       " 'going': 33,\n",
       " 'too': 34,\n",
       " 'time': 35,\n",
       " 'cant': 36,\n",
       " 'back': 37,\n",
       " 'one': 38,\n",
       " 'lol': 39,\n",
       " 'know': 40,\n",
       " 'what': 41,\n",
       " 'we': 42,\n",
       " 'about': 43,\n",
       " 'can': 44,\n",
       " 'really': 45,\n",
       " 'am': 46,\n",
       " 'want': 47,\n",
       " 'had': 48,\n",
       " 'there': 49,\n",
       " 'see': 50,\n",
       " 'some': 51,\n",
       " 'well': 52,\n",
       " 'night': 53,\n",
       " 'think': 54,\n",
       " 'if': 55,\n",
       " 'still': 56,\n",
       " 'new': 57,\n",
       " 'na': 58,\n",
       " 'how': 59,\n",
       " 'need': 60,\n",
       " 'thanks': 61,\n",
       " 'home': 62,\n",
       " 'when': 63,\n",
       " 'oh': 64,\n",
       " 'miss': 65,\n",
       " 'more': 66,\n",
       " 'here': 67,\n",
       " 'much': 68,\n",
       " 'off': 69,\n",
       " 'they': 70,\n",
       " 'last': 71,\n",
       " 'feel': 72,\n",
       " 'hope': 73,\n",
       " 'make': 74,\n",
       " 'morning': 75,\n",
       " 'been': 76,\n",
       " 'then': 77,\n",
       " 'tomorrow': 78,\n",
       " 'great': 79,\n",
       " 'twitter': 80,\n",
       " 'ill': 81,\n",
       " 'or': 82,\n",
       " 'thats': 83,\n",
       " 'her': 84,\n",
       " 'haha': 85,\n",
       " 'wish': 86,\n",
       " 'again': 87,\n",
       " 'sad': 88,\n",
       " 'fun': 89,\n",
       " 'come': 90,\n",
       " 'why': 91,\n",
       " 'only': 92,\n",
       " 'right': 93,\n",
       " 'week': 94,\n",
       " 'sleep': 95,\n",
       " 'didnt': 96,\n",
       " 'bad': 97,\n",
       " 'happy': 98,\n",
       " 'would': 99,\n",
       " 'very': 100,\n",
       " 'thing': 101,\n",
       " 'sorry': 102,\n",
       " 'friend': 103,\n",
       " 'tonight': 104,\n",
       " 'did': 105,\n",
       " 'way': 106,\n",
       " 'them': 107,\n",
       " 'getting': 108,\n",
       " 'gon': 109,\n",
       " 'though': 110,\n",
       " 'look': 111,\n",
       " 'ive': 112,\n",
       " 'over': 113,\n",
       " 'nice': 114,\n",
       " 'EMOJI': 115,\n",
       " 'better': 116,\n",
       " 'watching': 117,\n",
       " 'say': 118,\n",
       " 'should': 119,\n",
       " 'wait': 120,\n",
       " 'she': 121,\n",
       " 'hate': 122,\n",
       " 'bed': 123,\n",
       " 'yeah': 124,\n",
       " 'could': 125,\n",
       " 'people': 126,\n",
       " 'school': 127,\n",
       " 'youre': 128,\n",
       " 'hour': 129,\n",
       " 'guy': 130,\n",
       " 'yes': 131,\n",
       " 'weekend': 132,\n",
       " 'him': 133,\n",
       " 'even': 134,\n",
       " 'hey': 135,\n",
       " 'take': 136,\n",
       " 'after': 137,\n",
       " 'next': 138,\n",
       " 'show': 139,\n",
       " 'who': 140,\n",
       " 'down': 141,\n",
       " 'awesome': 142,\n",
       " 'never': 143,\n",
       " 'tweet': 144,\n",
       " 'thank': 145,\n",
       " 'soon': 146,\n",
       " 'let': 147,\n",
       " 'little': 148,\n",
       " 'long': 149,\n",
       " 'life': 150,\n",
       " 'first': 151,\n",
       " 'wan': 152,\n",
       " 'working': 153,\n",
       " 'best': 154,\n",
       " 'movie': 155,\n",
       " 'doing': 156,\n",
       " 'year': 157,\n",
       " 'please': 158,\n",
       " 'being': 159,\n",
       " 'having': 160,\n",
       " 'tired': 161,\n",
       " 'sick': 162,\n",
       " 'feeling': 163,\n",
       " 'watch': 164,\n",
       " 'everyone': 165,\n",
       " 'ok': 166,\n",
       " 'girl': 167,\n",
       " 'his': 168,\n",
       " 'our': 169,\n",
       " 'wont': 170,\n",
       " 'any': 171,\n",
       " 'done': 172,\n",
       " 'always': 173,\n",
       " 'sure': 174,\n",
       " 'lot': 175,\n",
       " 'already': 176,\n",
       " 'than': 177,\n",
       " 'suck': 178,\n",
       " 'another': 179,\n",
       " 'find': 180,\n",
       " 'cool': 181,\n",
       " 'something': 182,\n",
       " 'phone': 183,\n",
       " 'ready': 184,\n",
       " 'because': 185,\n",
       " 'x': 186,\n",
       " 'made': 187,\n",
       " 'where': 188,\n",
       " 'man': 189,\n",
       " 'keep': 190,\n",
       " 'looking': 191,\n",
       " 'yay': 192,\n",
       " 'song': 193,\n",
       " 'ur': 194,\n",
       " 'doesnt': 195,\n",
       " 'went': 196,\n",
       " 'before': 197,\n",
       " 'house': 198,\n",
       " 'yet': 199,\n",
       " 'hurt': 200,\n",
       " 'help': 201,\n",
       " 'start': 202,\n",
       " 'pretty': 203,\n",
       " 'thought': 204,\n",
       " 'ever': 205,\n",
       " 'trying': 206,\n",
       " 'away': 207,\n",
       " 'sound': 208,\n",
       " 'summer': 209,\n",
       " 'old': 210,\n",
       " 'maybe': 211,\n",
       " 'finally': 212,\n",
       " 'amazing': 213,\n",
       " 'game': 214,\n",
       " 'omg': 215,\n",
       " 'early': 216,\n",
       " 'someone': 217,\n",
       " 'lost': 218,\n",
       " 'bit': 219,\n",
       " 'guess': 220,\n",
       " 'into': 221,\n",
       " 'baby': 222,\n",
       " 'left': 223,\n",
       " 'mean': 224,\n",
       " 'follow': 225,\n",
       " 'damn': 226,\n",
       " 'rain': 227,\n",
       " 'big': 228,\n",
       " 'same': 229,\n",
       " 'missed': 230,\n",
       " 'tell': 231,\n",
       " 'n': 232,\n",
       " 'havent': 233,\n",
       " 'hot': 234,\n",
       " 'nothing': 235,\n",
       " 'while': 236,\n",
       " 'try': 237,\n",
       " 'birthday': 238,\n",
       " 'wow': 239,\n",
       " 'other': 240,\n",
       " 'coming': 241,\n",
       " 'glad': 242,\n",
       " 'also': 243,\n",
       " 'pic': 244,\n",
       " 'party': 245,\n",
       " 'doe': 246,\n",
       " 'live': 247,\n",
       " 'bored': 248,\n",
       " 'weather': 249,\n",
       " 'two': 250,\n",
       " 'sun': 251,\n",
       " 'play': 252,\n",
       " 'hear': 253,\n",
       " 'stuff': 254,\n",
       " 'mom': 255,\n",
       " 'later': 256,\n",
       " 'those': 257,\n",
       " 'actually': 258,\n",
       " 'saw': 259,\n",
       " 'ya': 260,\n",
       " 'isnt': 261,\n",
       " 'might': 262,\n",
       " 'exam': 263,\n",
       " 'ta': 264,\n",
       " 'call': 265,\n",
       " 'excited': 266,\n",
       " 'waiting': 267,\n",
       " 'hard': 268,\n",
       " 'said': 269,\n",
       " 'he': 270,\n",
       " 'car': 271,\n",
       " 'since': 272,\n",
       " 'world': 273,\n",
       " 'god': 274,\n",
       " 'yesterday': 275,\n",
       " 'give': 276,\n",
       " 'until': 277,\n",
       " 'few': 278,\n",
       " 'ugh': 279,\n",
       " 'head': 280,\n",
       " 'job': 281,\n",
       " 'hi': 282,\n",
       " 'such': 283,\n",
       " 'around': 284,\n",
       " 'myself': 285,\n",
       " 'many': 286,\n",
       " 'friday': 287,\n",
       " 'sunday': 288,\n",
       " 'id': 289,\n",
       " 'late': 290,\n",
       " 'video': 291,\n",
       " 'monday': 292,\n",
       " 'luck': 293,\n",
       " 'check': 294,\n",
       " 'found': 295,\n",
       " 'NUMBERth': 296,\n",
       " 'music': 297,\n",
       " 'put': 298,\n",
       " 'talk': 299,\n",
       " 'cold': 300,\n",
       " 'read': 301,\n",
       " 'their': 302,\n",
       " 'making': 303,\n",
       " 'must': 304,\n",
       " 'beautiful': 305,\n",
       " 'follower': 306,\n",
       " 'whats': 307,\n",
       " 'may': 308,\n",
       " 'stop': 309,\n",
       " 'boy': 310,\n",
       " 'gone': 311,\n",
       " 'end': 312,\n",
       " 'least': 313,\n",
       " 'aww': 314,\n",
       " 'missing': 315,\n",
       " 'kid': 316,\n",
       " 'woke': 317,\n",
       " 'anything': 318,\n",
       " 'poor': 319,\n",
       " 'till': 320,\n",
       " 'use': 321,\n",
       " 'most': 322,\n",
       " 'family': 323,\n",
       " 'leave': 324,\n",
       " 'almost': 325,\n",
       " 'hair': 326,\n",
       " 'tho': 327,\n",
       " 'food': 328,\n",
       " 'okay': 329,\n",
       " 'far': 330,\n",
       " 'listening': 331,\n",
       " 'cute': 332,\n",
       " 'lunch': 333,\n",
       " 'picture': 334,\n",
       " 'eat': 335,\n",
       " 'wanted': 336,\n",
       " 'free': 337,\n",
       " 'book': 338,\n",
       " 'month': 339,\n",
       " 'iphone': 340,\n",
       " 'sweet': 341,\n",
       " 'class': 342,\n",
       " 'dinner': 343,\n",
       " 'shes': 344,\n",
       " 'finished': 345,\n",
       " 'funny': 346,\n",
       " 'enjoy': 347,\n",
       " 'playing': 348,\n",
       " 'forward': 349,\n",
       " 'place': 350,\n",
       " 'believe': 351,\n",
       " 'NUMBERam': 352,\n",
       " 'welcome': 353,\n",
       " 'shit': 354,\n",
       " 'r': 355,\n",
       " 'without': 356,\n",
       " 'thinking': 357,\n",
       " 'everything': 358,\n",
       " 'anyone': 359,\n",
       " 'mine': 360,\n",
       " 'cause': 361,\n",
       " 'cry': 362,\n",
       " 'which': 363,\n",
       " 'buy': 364,\n",
       " 'update': 365,\n",
       " 'totally': 366,\n",
       " 'idea': 367,\n",
       " 'dad': 368,\n",
       " 'win': 369,\n",
       " 'these': 370,\n",
       " 'real': 371,\n",
       " 'outside': 372,\n",
       " 'enough': 373,\n",
       " 'coffee': 374,\n",
       " 'o': 375,\n",
       " 'wasnt': 376,\n",
       " 'hahaha': 377,\n",
       " 'wrong': 378,\n",
       " 'stay': 379,\n",
       " 'stupid': 380,\n",
       " 'w': 381,\n",
       " 'every': 382,\n",
       " 'room': 383,\n",
       " 'through': 384,\n",
       " 'anymore': 385,\n",
       " 'probably': 386,\n",
       " 'couldnt': 387,\n",
       " 'dog': 388,\n",
       " 'fan': 389,\n",
       " 'once': 390,\n",
       " 'saturday': 391,\n",
       " 'dream': 392,\n",
       " 'eating': 393,\n",
       " 'money': 394,\n",
       " 'name': 395,\n",
       " 's': 396,\n",
       " 'd': 397,\n",
       " 'hell': 398,\n",
       " 'following': 399,\n",
       " 'p': 400,\n",
       " 'minute': 401,\n",
       " 'xx': 402,\n",
       " 'post': 403,\n",
       " 'sooo': 404,\n",
       " 'tv': 405,\n",
       " 'busy': 406,\n",
       " 'lovely': 407,\n",
       " 'ha': 408,\n",
       " 'brother': 409,\n",
       " 'headache': 410,\n",
       " 'came': 411,\n",
       " 'whole': 412,\n",
       " 'seen': 413,\n",
       " 'kinda': 414,\n",
       " 'taking': 415,\n",
       " 'beach': 416,\n",
       " 'mother': 417,\n",
       " 'run': 418,\n",
       " 'both': 419,\n",
       " 'eye': 420,\n",
       " 'face': 421,\n",
       " 'crazy': 422,\n",
       " 'took': 423,\n",
       " 'hopefully': 424,\n",
       " 'word': 425,\n",
       " 'final': 426,\n",
       " 'computer': 427,\n",
       " 'hello': 428,\n",
       " 'super': 429,\n",
       " 'news': 430,\n",
       " 'able': 431,\n",
       " 'plan': 432,\n",
       " 'true': 433,\n",
       " 'b': 434,\n",
       " 'meet': 435,\n",
       " 'half': 436,\n",
       " 'problem': 437,\n",
       " 'theyre': 438,\n",
       " 'blog': 439,\n",
       " 'hit': 440,\n",
       " 'forgot': 441,\n",
       " 'awww': 442,\n",
       " 'goodnight': 443,\n",
       " 'leaving': 444,\n",
       " 'either': 445,\n",
       " 'reading': 446,\n",
       " 'heart': 447,\n",
       " 'photo': 448,\n",
       " 'shopping': 449,\n",
       " 'part': 450,\n",
       " 'else': 451,\n",
       " 'rest': 452,\n",
       " 'ago': 453,\n",
       " 'sitting': 454,\n",
       " 'used': 455,\n",
       " 'send': 456,\n",
       " 'soo': 457,\n",
       " 'full': 458,\n",
       " 'trip': 459,\n",
       " 'ah': 460,\n",
       " 'kind': 461,\n",
       " 'seems': 462,\n",
       " 'rock': 463,\n",
       " 'sister': 464,\n",
       " 'cuz': 465,\n",
       " 'email': 466,\n",
       " 'raining': 467,\n",
       " 'office': 468,\n",
       " 'internet': 469,\n",
       " 'remember': 470,\n",
       " 'talking': 471,\n",
       " 'break': 472,\n",
       " 'mind': 473,\n",
       " 'change': 474,\n",
       " 'watched': 475,\n",
       " 'alone': 476,\n",
       " 'fuck': 477,\n",
       " 'hug': 478,\n",
       " 'stuck': 479,\n",
       " 'heard': 480,\n",
       " 'own': 481,\n",
       " 'link': 482,\n",
       " 'tried': 483,\n",
       " 'boo': 484,\n",
       " 'course': 485,\n",
       " 'started': 486,\n",
       " 'ticket': 487,\n",
       " 'pain': 488,\n",
       " 'reply': 489,\n",
       " 'site': 490,\n",
       " 'btw': 491,\n",
       " 'care': 492,\n",
       " 'seeing': 493,\n",
       " 'hehe': 494,\n",
       " 'drink': 495,\n",
       " 'using': 496,\n",
       " 'la': 497,\n",
       " 'add': 498,\n",
       " 'quite': 499,\n",
       " 'online': 500,\n",
       " 'season': 501,\n",
       " 'concert': 502,\n",
       " 'wake': 503,\n",
       " 'told': 504,\n",
       " 'dude': 505,\n",
       " 'awake': 506,\n",
       " 'loved': 507,\n",
       " 'favorite': 508,\n",
       " 'text': 509,\n",
       " 'fine': 510,\n",
       " 'til': 511,\n",
       " 'breakfast': 512,\n",
       " 'person': 513,\n",
       " 'pay': 514,\n",
       " 'cat': 515,\n",
       " 'open': 516,\n",
       " 'sunny': 517,\n",
       " 'bought': 518,\n",
       " 'hand': 519,\n",
       " 'seriously': 520,\n",
       " 'boring': 521,\n",
       " 'train': 522,\n",
       " 'youll': 523,\n",
       " 'broke': 524,\n",
       " 'study': 525,\n",
       " 'drive': 526,\n",
       " 'called': 527,\n",
       " 'facebook': 528,\n",
       " 'turn': 529,\n",
       " 'm': 530,\n",
       " 'shower': 531,\n",
       " 'starting': 532,\n",
       " 'walk': 533,\n",
       " 'aw': 534,\n",
       " 'as': 535,\n",
       " 'bring': 536,\n",
       " 'lmao': 537,\n",
       " 'move': 538,\n",
       " 'hungry': 539,\n",
       " 'june': 540,\n",
       " 'instead': 541,\n",
       " 'asleep': 542,\n",
       " 'crap': 543,\n",
       " 'anyway': 544,\n",
       " 'lucky': 545,\n",
       " 'afternoon': 546,\n",
       " 'heading': 547,\n",
       " 'lady': 548,\n",
       " 'sleeping': 549,\n",
       " 'le': 550,\n",
       " 'red': 551,\n",
       " 'test': 552,\n",
       " 'enjoying': 553,\n",
       " 'reason': 554,\n",
       " 'xd': 555,\n",
       " 'jealous': 556,\n",
       " 'bout': 557,\n",
       " 'story': 558,\n",
       " 'wonderful': 559,\n",
       " 'page': 560,\n",
       " 'meeting': 561,\n",
       " 'second': 562,\n",
       " 'NUMBERpm': 563,\n",
       " 'mad': 564,\n",
       " 'city': 565,\n",
       " 'ice': 566,\n",
       " 'set': 567,\n",
       " 'sore': 568,\n",
       " 'yea': 569,\n",
       " 'album': 570,\n",
       " 'soooo': 571,\n",
       " 'together': 572,\n",
       " 'bye': 573,\n",
       " 'homework': 574,\n",
       " 'high': 575,\n",
       " 'finish': 576,\n",
       " 'definitely': 577,\n",
       " 'dead': 578,\n",
       " 'holiday': 579,\n",
       " 'top': 580,\n",
       " 'cut': 581,\n",
       " 'running': 582,\n",
       " 'laptop': 583,\n",
       " 'congrats': 584,\n",
       " 'hoping': 585,\n",
       " 'fucking': 586,\n",
       " 'message': 587,\n",
       " 'write': 588,\n",
       " 'bday': 589,\n",
       " 'couple': 590,\n",
       " 'died': 591,\n",
       " 'happened': 592,\n",
       " 'fail': 593,\n",
       " 'sometimes': 594,\n",
       " 'store': 595,\n",
       " 'ask': 596,\n",
       " 'won': 597,\n",
       " 'goin': 598,\n",
       " 'sigh': 599,\n",
       " 'award': 600,\n",
       " 'moment': 601,\n",
       " 'dear': 602,\n",
       " 'c': 603,\n",
       " 'foot': 604,\n",
       " 'fall': 605,\n",
       " 'nap': 606,\n",
       " 'NUMBERst': 607,\n",
       " 'worry': 608,\n",
       " 'star': 609,\n",
       " 'wouldnt': 610,\n",
       " 'tour': 611,\n",
       " 'tea': 612,\n",
       " 'visit': 613,\n",
       " 'church': 614,\n",
       " 'short': 615,\n",
       " 'side': 616,\n",
       " 'water': 617,\n",
       " 'evening': 618,\n",
       " 'town': 619,\n",
       " 'dance': 620,\n",
       " 'youtube': 621,\n",
       " 'smile': 622,\n",
       " 'perfect': 623,\n",
       " 'ppl': 624,\n",
       " 'arent': 625,\n",
       " 'happen': 626,\n",
       " 'ipod': 627,\n",
       " 'point': 628,\n",
       " 'studying': 629,\n",
       " 'lil': 630,\n",
       " 'weird': 631,\n",
       " 'date': 632,\n",
       " 'ride': 633,\n",
       " 'listen': 634,\n",
       " 'close': 635,\n",
       " 'line': 636,\n",
       " 'list': 637,\n",
       " 'math': 638,\n",
       " 'hang': 639,\n",
       " 'mood': 640,\n",
       " 'yall': 641,\n",
       " 'seem': 642,\n",
       " 'min': 643,\n",
       " 'loving': 644,\n",
       " 'nite': 645,\n",
       " 'cream': 646,\n",
       " 'wonder': 647,\n",
       " 'gym': 648,\n",
       " 'catch': 649,\n",
       " 'mum': 650,\n",
       " 'knew': 651,\n",
       " 'ate': 652,\n",
       " 'english': 653,\n",
       " 'episode': 654,\n",
       " 'agree': 655,\n",
       " 'interesting': 656,\n",
       " 'writing': 657,\n",
       " 'chocolate': 658,\n",
       " 'aint': 659,\n",
       " 'clean': 660,\n",
       " 'account': 661,\n",
       " 'fb': 662,\n",
       " 'pool': 663,\n",
       " 'wedding': 664,\n",
       " 'chance': 665,\n",
       " 'window': 666,\n",
       " 'band': 667,\n",
       " 'worst': 668,\n",
       " 'london': 669,\n",
       " 'ahh': 670,\n",
       " 'saying': 671,\n",
       " 't': 672,\n",
       " 'air': 673,\n",
       " 'broken': 674,\n",
       " 'fast': 675,\n",
       " 'vote': 676,\n",
       " 'team': 677,\n",
       " 'throat': 678,\n",
       " 'unfortunately': 679,\n",
       " 'supposed': 680,\n",
       " 'moving': 681,\n",
       " 'flight': 682,\n",
       " 'via': 683,\n",
       " 'da': 684,\n",
       " 'past': 685,\n",
       " 'mr': 686,\n",
       " 'pick': 687,\n",
       " 'black': 688,\n",
       " 'cleaning': 689,\n",
       " 'xxx': 690,\n",
       " 'sent': 691,\n",
       " 'worth': 692,\n",
       " 'sat': 693,\n",
       " 'v': 694,\n",
       " 'question': 695,\n",
       " 'driving': 696,\n",
       " 'forget': 697,\n",
       " 'gave': 698,\n",
       " 'sunshine': 699,\n",
       " 'park': 700,\n",
       " 'wishing': 701,\n",
       " 'card': 702,\n",
       " 'three': 703,\n",
       " 'parent': 704,\n",
       " 'understand': 705,\n",
       " 'horrible': 706,\n",
       " 'sleepy': 707,\n",
       " 'followfriday': 708,\n",
       " 'answer': 709,\n",
       " 'yep': 710,\n",
       " 'mac': 711,\n",
       " 'jonas': 712,\n",
       " 'tweeting': 713,\n",
       " 'drinking': 714,\n",
       " 'college': 715,\n",
       " 'cake': 716,\n",
       " 'upset': 717,\n",
       " 'em': 718,\n",
       " 'leg': 719,\n",
       " 'green': 720,\n",
       " 'number': 721,\n",
       " 'beer': 722,\n",
       " 'special': 723,\n",
       " 'slow': 724,\n",
       " 'easy': 725,\n",
       " 'moon': 726,\n",
       " 'finger': 727,\n",
       " 'website': 728,\n",
       " 'comment': 729,\n",
       " 'paper': 730,\n",
       " 'tuesday': 731,\n",
       " 'project': 732,\n",
       " 'longer': 733,\n",
       " 'worse': 734,\n",
       " 'rather': 735,\n",
       " 'spent': 736,\n",
       " 'blue': 737,\n",
       " 'y': 738,\n",
       " 'bet': 739,\n",
       " 'bus': 740,\n",
       " 'hmm': 741,\n",
       " 'apparently': 742,\n",
       " 'fell': 743,\n",
       " 'shop': 744,\n",
       " 'youve': 745,\n",
       " 'vacation': 746,\n",
       " 'scared': 747,\n",
       " 'body': 748,\n",
       " 'due': 749,\n",
       " 'shoe': 750,\n",
       " 'under': 751,\n",
       " 'hows': 752,\n",
       " 'beat': 753,\n",
       " 'figure': 754,\n",
       " 'plus': 755,\n",
       " 'huge': 756,\n",
       " 'wear': 757,\n",
       " 'co': 758,\n",
       " 'fair': 759,\n",
       " 'woman': 760,\n",
       " 'dress': 761,\n",
       " 'load': 762,\n",
       " 'hanging': 763,\n",
       " 'white': 764,\n",
       " 'cousin': 765,\n",
       " 'spend': 766,\n",
       " 'kill': 767,\n",
       " 'earlier': 768,\n",
       " 'voice': 769,\n",
       " 'flu': 770,\n",
       " 'nope': 771,\n",
       " 'mtv': 772,\n",
       " 'thx': 773,\n",
       " 'join': 774,\n",
       " 'support': 775,\n",
       " 'wtf': 776,\n",
       " 'during': 777,\n",
       " 'wondering': 778,\n",
       " 'miley': 779,\n",
       " 'shame': 780,\n",
       " 'uk': 781,\n",
       " 'thursday': 782,\n",
       " 'camera': 783,\n",
       " 'forever': 784,\n",
       " 'chat': 785,\n",
       " 'cheer': 786,\n",
       " 'lazy': 787,\n",
       " 'looked': 788,\n",
       " 'stomach': 789,\n",
       " 'cd': 790,\n",
       " 'babe': 791,\n",
       " 'age': 792,\n",
       " 'light': 793,\n",
       " 'shot': 794,\n",
       " 'ahhh': 795,\n",
       " 'son': 796,\n",
       " 'slept': 797,\n",
       " 'power': 798,\n",
       " 'sadly': 799,\n",
       " 'david': 800,\n",
       " 'bike': 801,\n",
       " 'garden': 802,\n",
       " 'apple': 803,\n",
       " 'boyfriend': 804,\n",
       " 'die': 805,\n",
       " 'idk': 806,\n",
       " 'warm': 807,\n",
       " 'different': 808,\n",
       " 'learn': 809,\n",
       " 'inside': 810,\n",
       " 'NUMBERday': 811,\n",
       " 'tom': 812,\n",
       " 'especially': 813,\n",
       " 'dm': 814,\n",
       " 'fix': 815,\n",
       " 'july': 816,\n",
       " 'save': 817,\n",
       " 'meant': 818,\n",
       " 'sign': 819,\n",
       " 'google': 820,\n",
       " 'airport': 821,\n",
       " 'itll': 822,\n",
       " 'liked': 823,\n",
       " 'father': 824,\n",
       " 'l': 825,\n",
       " 'pizza': 826,\n",
       " 'case': 827,\n",
       " 'sims': 828,\n",
       " 'myspace': 829,\n",
       " 'bbq': 830,\n",
       " 'road': 831,\n",
       " 'service': 832,\n",
       " 'officially': 833,\n",
       " 'tummy': 834,\n",
       " 'safe': 835,\n",
       " 'laugh': 836,\n",
       " 'worked': 837,\n",
       " 'note': 838,\n",
       " 'hr': 839,\n",
       " 'felt': 840,\n",
       " 'rainy': 841,\n",
       " 'luv': 842,\n",
       " 'bitch': 843,\n",
       " 'chicken': 844,\n",
       " 'met': 845,\n",
       " 'shall': 846,\n",
       " 'si': 847,\n",
       " 'absolutely': 848,\n",
       " 'box': 849,\n",
       " 'shirt': 850,\n",
       " 'small': 851,\n",
       " 'doctor': 852,\n",
       " 'hospital': 853,\n",
       " 'yummy': 854,\n",
       " 'rip': 855,\n",
       " 'hill': 856,\n",
       " 'goodbye': 857,\n",
       " 'radio': 858,\n",
       " 'each': 859,\n",
       " 'proud': 860,\n",
       " 'share': 861,\n",
       " 'wit': 862,\n",
       " 'film': 863,\n",
       " 'graduation': 864,\n",
       " 'cup': 865,\n",
       " 'club': 866,\n",
       " 'decided': 867,\n",
       " 'NUMBERnd': 868,\n",
       " 'fly': 869,\n",
       " 'order': 870,\n",
       " 'except': 871,\n",
       " 'yourself': 872,\n",
       " 'played': 873,\n",
       " 'hubby': 874,\n",
       " 'lonely': 875,\n",
       " 'fact': 876,\n",
       " 'exciting': 877,\n",
       " 'twilight': 878,\n",
       " 'dvd': 879,\n",
       " 'gorgeous': 880,\n",
       " 'needed': 881,\n",
       " 'wine': 882,\n",
       " 'french': 883,\n",
       " 'alright': 884,\n",
       " 'wednesday': 885,\n",
       " 'annoying': 886,\n",
       " 'interview': 887,\n",
       " 'front': 888,\n",
       " 'glass': 889,\n",
       " 'company': 890,\n",
       " 'smell': 891,\n",
       " 'issue': 892,\n",
       " 'taken': 893,\n",
       " 'exactly': 894,\n",
       " 'bag': 895,\n",
       " 'lame': 896,\n",
       " 'bar': 897,\n",
       " 'yr': 898,\n",
       " 'storm': 899,\n",
       " 'scary': 900,\n",
       " 'living': 901,\n",
       " 'yup': 902,\n",
       " 'near': 903,\n",
       " 'xoxo': 904,\n",
       " 'business': 905,\n",
       " 're': 906,\n",
       " 'woo': 907,\n",
       " 'packing': 908,\n",
       " 'mate': 909,\n",
       " 'turned': 910,\n",
       " 'behind': 911,\n",
       " 'sold': 912,\n",
       " 'waking': 913,\n",
       " 'realized': 914,\n",
       " 'bc': 915,\n",
       " 'wife': 916,\n",
       " 'yours': 917,\n",
       " 'version': 918,\n",
       " 'daughter': 919,\n",
       " 'touch': 920,\n",
       " 'jus': 921,\n",
       " 'happens': 922,\n",
       " 'ouch': 923,\n",
       " 'killing': 924,\n",
       " 'gettin': 925,\n",
       " 'download': 926,\n",
       " 'ball': 927,\n",
       " 'guitar': 928,\n",
       " 'drunk': 929,\n",
       " 'giving': 930,\n",
       " 'lose': 931,\n",
       " 'pas': 932,\n",
       " 'mommy': 933,\n",
       " 'matter': 934,\n",
       " 'terrible': 935,\n",
       " 'revision': 936,\n",
       " 'walking': 937,\n",
       " 'event': 938,\n",
       " 'state': 939,\n",
       " 'round': 940,\n",
       " 'yo': 941,\n",
       " 'door': 942,\n",
       " 'vega': 943,\n",
       " 'along': 944,\n",
       " 'mile': 945,\n",
       " 'hold': 946,\n",
       " 'app': 947,\n",
       " 'puppy': 948,\n",
       " 'everybody': 949,\n",
       " 'single': 950,\n",
       " 'e': 951,\n",
       " 'ear': 952,\n",
       " 'deal': 953,\n",
       " 'although': 954,\n",
       " 'clothes': 955,\n",
       " 'art': 956,\n",
       " 'hasnt': 957,\n",
       " 'country': 958,\n",
       " 'enjoyed': 959,\n",
       " 'bro': 960,\n",
       " 'arm': 961,\n",
       " 'sale': 962,\n",
       " 'posted': 963,\n",
       " 'hangover': 964,\n",
       " 'plane': 965,\n",
       " 'fantastic': 966,\n",
       " 'tear': 967,\n",
       " 'hotel': 968,\n",
       " 'hahah': 969,\n",
       " 'sit': 970,\n",
       " 'asked': 971,\n",
       " 'whatever': 972,\n",
       " 'staying': 973,\n",
       " 'relaxing': 974,\n",
       " 'taste': 975,\n",
       " 'random': 976,\n",
       " 'group': 977,\n",
       " 'ahead': 978,\n",
       " 'shoot': 979,\n",
       " 'passed': 980,\n",
       " 'bb': 981,\n",
       " 'singing': 982,\n",
       " 'web': 983,\n",
       " 'indeed': 984,\n",
       " 'history': 985,\n",
       " 'vip': 986,\n",
       " 'hmmm': 987,\n",
       " 'alot': 988,\n",
       " 'upload': 989,\n",
       " 'profile': 990,\n",
       " 'freaking': 991,\n",
       " 'fit': 992,\n",
       " 'kiss': 993,\n",
       " 'camp': 994,\n",
       " 'currently': 995,\n",
       " 'completely': 996,\n",
       " 'death': 997,\n",
       " 'dark': 998,\n",
       " 'cook': 999,\n",
       " 'child': 1000}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_transformer.vocabulary_ # 1000! yay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1199999x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 10232362 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix,vals=[],[]\n",
    "for i, v in enumerate(X_vectors[4].toarray()[0]):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        if v != 0:\n",
    "            ix.append(i)\n",
    "            vals.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1001 in length, the first is how many terms are missing from the vocab in this doc\n",
    "X_vectors[4].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 i\n",
      "2 my\n",
      "6 me\n",
      "25 u\n",
      "26 love\n",
      "55 still\n",
      "58 how\n",
      "109 though\n",
      "169 wont\n",
      "264 call\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(vocabulary_transformer.vocabulary_):\n",
    "    if i in ix:\n",
    "        print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 6, 25, 26, 55, 58, 109, 169, 264], [4, 1, 1, 1, 2, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['please pray my house there major water leakage thats causing my entire house crack possibly fall apart',\n",
       "       'URL bump this', 'USERNAME just saw not part your last message',\n",
       "       'USERNAME lol i put up 100 track u havent retweeted 1 hun smh',\n",
       "       'USERNAME u got ta endorse application if u gon na our courtside tweep USERNAME im USERNAME i USERNAME',\n",
       "       'best night ever got spend better part half hour letting off firework'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BoW DFM\n",
    "bow_vectorizer_ung = CountVectorizer(max_features=10000) \n",
    "bow_vectorizer_big = CountVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "X_bow_ung = bow_vectorizer_ung.fit_transform(X_array)\n",
    "X_bow_big = bow_vectorizer_big.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1199999x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 10626030 stored elements in Compressed Sparse Row format>,\n",
       " <1199999x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 13212223 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total space <1199999x329492 sparse matrix of type '<class 'numpy.int64'>' with 11446957 stored elements\n",
    "X_bow_ung, X_bow_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 0.0885503238 % nonzero.\n"
     ]
    }
   ],
   "source": [
    "# Only 0.0028951048 % nonzero for ALL features\n",
    "def calc_sparsity(X):\n",
    "    total_space = X.shape[0] * X.shape[1]\n",
    "    total_store = X.getnnz()\n",
    "    pct_zeroes = 100 * (total_store/total_space)\n",
    "    print(f'Only {pct_zeroes:0.10f} % nonzero.')\n",
    "\n",
    "calc_sparsity(X_bow_ung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 0.1101019501 % nonzero.\n"
     ]
    }
   ],
   "source": [
    "calc_sparsity(X_bow_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 7 minute(s) and 5 second(s)\n"
     ]
    }
   ],
   "source": [
    "savepath = os.path.join(\"..\",\"data\",\"3_processed\",\"sentiment140\")\n",
    "sp.save_npz(os.path.join(savepath, 'X_bow_ung.npz'), X_bow_ung)\n",
    "sp.save_npz(os.path.join(savepath, 'X_bow_big.npz'), X_bow_big)\n",
    "\n",
    "# print total running time\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed Time: {mins:0.0f} minute(s) and {secs:0.0f} second(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
