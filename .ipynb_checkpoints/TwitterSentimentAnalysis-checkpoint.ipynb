{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Setiment Analysis in Python\n",
    "\n",
    "The code was inspired Gaurav Singhal's guide: [Building a Twitter Setiment Analysis in Python.](https://www.pluralsight.com/guides/building-a-twitter-sentiment-analysis-in-python)\n",
    "\n",
    "The data comes from Marios Michailidis' sentiment140 dataset hosted in [Kaggle.](https://www.kaggle.com/kazanova/sentiment140/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All details of cleanup steps can be found in the custom python script `cleanup_tweets.py`. \n",
    "\n",
    "Michailidis' dataset consists of 1.6 M rows evenly split into negative and positive Tweets. The labels were created automatically simply using emoticons (happy face is positive, and vice versa).  \n",
    "\n",
    "Since my cleanup function entails heavy CPU-bound processes I use multiprocessing, splitting the data into 32 50k-row chunks which are processed 8 at a time (since I have 8 logical processors). The order of processing is asynchronous.\n",
    "\n",
    "Here I just run that script by passing a command to the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cleaned up train dataset: 1\n",
      "Saving cleaned up train dataset: 8\n",
      "Saving cleaned up train dataset: 2\n",
      "Saving cleaned up train dataset: 7\n",
      "Saving cleaned up train dataset: 5\n",
      "Saving cleaned up train dataset: 4\n",
      "Saving cleaned up train dataset: 6\n",
      "Saving cleaned up train dataset: 3\n",
      "Saving cleaned up train dataset: 9\n",
      "Saving cleaned up train dataset: 10\n",
      "Saving cleaned up train dataset: 11\n",
      "Saving cleaned up train dataset: 15\n",
      "Saving cleaned up train dataset: 14\n",
      "Saving cleaned up train dataset: 13\n",
      "Saving cleaned up train dataset: 12\n",
      "Saving cleaned up train dataset: 16\n",
      "Saving cleaned up train dataset: 17\n",
      "Saving cleaned up train dataset: 19\n",
      "Saving cleaned up train dataset: 18\n",
      "Saving cleaned up train dataset: 23\n",
      "Saving cleaned up train dataset: 20\n",
      "Saving cleaned up train dataset: 21\n",
      "Saving cleaned up train dataset: 22\n",
      "Saving cleaned up train dataset: 24\n",
      "Saving cleaned up train dataset: 27\n",
      "Saving cleaned up train dataset: 25\n",
      "Saving cleaned up train dataset: 29\n",
      "Saving cleaned up train dataset: 31\n",
      "Saving cleaned up train dataset: 26\n",
      "Saving cleaned up train dataset: 28\n",
      "Saving cleaned up train dataset: 30\n",
      "Saving cleaned up train dataset: 32\n",
      "Finished in 297.78 second(s)\n"
     ]
    }
   ],
   "source": [
    "cmd = 'python cleanup_tweets.py'\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Even without compiling the regex patterns the entire dataset runs in just under 5 mins, which is good enough for me since it's a one-time process. Here I show how to revert back to the original data (which includes Tweet IDs, etc) from the cleaned data. The key is basically the list of parameters passed to the multiprocessing executor, for example, this last set of parameters indicates that the cleaned dataset 32 contains the range from 1550000 to 1600000:\n",
    "\n",
    "```\n",
    "(range(1550000, 1600000), 32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/raw/training.1600000.processed.noemoticon.csv\",\n",
    "                 encoding='latin-1', \n",
    "                 usecols=[0,5])\n",
    "\n",
    "df.columns = ['target','text']\n",
    "              \n",
    "df_clean =  pd.read_csv(\"./data/clean/train_32.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1550401</th>\n",
       "      <td>4</td>\n",
       "      <td>Going to see Ghosts of Girlfriends Past with @DANii245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550402</th>\n",
       "      <td>4</td>\n",
       "      <td>@sarah_cawood can't wait to see the movie, it looks so good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550403</th>\n",
       "      <td>4</td>\n",
       "      <td>@ScottHuska rock the boat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550404</th>\n",
       "      <td>4</td>\n",
       "      <td>@DooneyStudio me and the remaining web developer have plans to keep the company afloat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550405</th>\n",
       "      <td>4</td>\n",
       "      <td>Wooh powergun Haha washing away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550406</th>\n",
       "      <td>4</td>\n",
       "      <td>@mileycyrus NO MILEY IM NOT VOTING FOR YOU &amp;gt;=( HHAHAHAH JOKES OF COURSE I WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target  \\\n",
       "1550401       4   \n",
       "1550402       4   \n",
       "1550403       4   \n",
       "1550404       4   \n",
       "1550405       4   \n",
       "1550406       4   \n",
       "\n",
       "                                                                                            text  \n",
       "1550401                                  Going to see Ghosts of Girlfriends Past with @DANii245   \n",
       "1550402                             @sarah_cawood can't wait to see the movie, it looks so good   \n",
       "1550403                                                               @ScottHuska rock the boat   \n",
       "1550404  @DooneyStudio me and the remaining web developer have plans to keep the company afloat   \n",
       "1550405                                                         Wooh powergun Haha washing away   \n",
       "1550406       @mileycyrus NO MILEY IM NOT VOTING FOR YOU &gt;=( HHAHAHAH JOKES OF COURSE I WILL   "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1550401:1550406,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1</td>\n",
       "      <td>go see ghost girlfriend past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1</td>\n",
       "      <td>cant wait see movi look so good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1</td>\n",
       "      <td>rock boat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1</td>\n",
       "      <td>me remain web develop have plan keep compani afloat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1</td>\n",
       "      <td>wooh powergun haha wash away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1</td>\n",
       "      <td>no miley im not vote you hhahahah joke cours i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                                 text\n",
       "401       1                         go see ghost girlfriend past\n",
       "402       1                      cant wait see movi look so good\n",
       "403       1                                            rock boat\n",
       "404       1  me remain web develop have plan keep compani afloat\n",
       "405       1                         wooh powergun haha wash away\n",
       "406       1       no miley im not vote you hhahahah joke cours i"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.loc[401:406,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization with TF-IDF\n",
    "# There are other techniques as well, such as Bag of Words and N-grams\n",
    "\n",
    "# TODO: read more about this, make sure this implementation is kosher\n",
    "\n",
    "def get_feature_vector(train_fit):\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(train_fit)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Same tf vector will be used for testing sentiments on unseen trending data\n",
    "tf_vector = get_feature_vector(np.array(dataset.iloc[:, 1]).ravel())\n",
    "X = tf_vector.transform(np.array(dataset.iloc[:, 1]).ravel())\n",
    "y = np.array(dataset.iloc[:, 0]).ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n",
    "# Training Naive Bayes model\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_predict_nb = NB_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_nb))\n",
    "\n",
    "# Training Logistics Regression model\n",
    "LR_model = LogisticRegression(solver='lbfgs')\n",
    "LR_model.fit(X_train, y_train)\n",
    "y_predict_lr = LR_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is giving nearly 76% accuracy, and Logistic Regression gives nearly 79%. These accuracy figures are recorded without implementing stemming or lemmatization. Using better techniques, you might get better accuracy.\n",
    "\n",
    "Testing on Real-time Feeds\n",
    "This step is completely optional and will only apply if you have read and implemented the guide [Building a Twitter Bot with Python.](https://www.pluralsight.com/guides/building-a-twitter-bot-with-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = \"trending_tweets/08-04-2020-1586291553-tweets.csv\"\n",
    "test_ds = load_dataset(test_file_name, [\"t_id\", \"hashtag\", \"created_at\", \"user\", \"text\"])\n",
    "test_ds = remove_unwanted_cols(test_ds, [\"t_id\", \"created_at\", \"user\"])\n",
    "\n",
    "# Creating text feature\n",
    "test_ds.text = test_ds[\"text\"].apply(preprocess_tweet_text)\n",
    "test_feature = tf_vector.transform(np.array(test_ds.iloc[:, 1]).ravel())\n",
    "\n",
    "# Using Logistic Regression model for prediction\n",
    "test_prediction_lr = LR_model.predict(test_feature)\n",
    "\n",
    "# Averaging out the hashtags result\n",
    "test_result_ds = pd.DataFrame({'hashtag': test_ds.hashtag, 'prediction':test_prediction_lr})\n",
    "test_result = test_result_ds.groupby(['hashtag']).max().reset_index()\n",
    "test_result.columns = ['heashtag', 'predictions']\n",
    "test_result.predictions = test_result['predictions'].apply(int_to_string)\n",
    "\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed reading this guide. Sentiment analysis is a popular project that almost every data scientist will do at some point. It can solve a lot of problems depending on you how you want to use it.\n",
    "\n",
    "I highly recommended using different vectorizing techniques and applying feature extraction and feature selection to the dataset. Try to implement more machine learning models and you might be able to get accuracy over 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
