{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Management\n",
    "\n",
    "\n",
    "High-level summary of each notebooks main results and planning of next and possible steps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1: Management__\n",
    "\n",
    "- This notebook.\n",
    "\n",
    "## Cleanup and Pre-process\n",
    "\n",
    "__2: Split_TrainTest__\n",
    "\n",
    "- Split raw data into training and test sets. Set test set aside. Note base rate accuracy of 0.8674."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations\n",
    "\n",
    "__3: Tfidf__\n",
    "\n",
    "- Unigram Tfidf with 450 terms, logistic classifier: 0.975 accuracy\n",
    "\n",
    "__4: Bigrams__\n",
    "\n",
    "- Bag-of-upto-Bigrams with 500 terms, logistic classifier: 0.985 accuracy\n",
    "\n",
    "__5: Ngrams__\n",
    "    \n",
    "- Bag-of-upto-Trigrams (BoT) with 2,000 terms, logistic classifier: 0.986 accuracy\n",
    "    \n",
    "__6: Dimensionality Reduction__\n",
    "\n",
    "- SVD with 300 components on Tfidf: 0.9864 accuracy, 0.9089 sensitivity, 0.9982 specificity\n",
    "- SVD not clearly advantageous with a logistic classifier, but that will change with more complex models\n",
    "- Notebook 6b experiments with scaling vs not: similar results \n",
    "\n",
    "__7: Feature Engineering__\n",
    "\n",
    "- After data viz, the first feature (raw document length) is the most useful in separating the target\n",
    "- After modeling, all features but the RSR achieve best accuracy (0.8823), yet low sensitivity (0.3270)\n",
    "\n",
    "__8: Cosine Similarity__\n",
    "\n",
    "- Benefit of spam cosine similarity unclear with logistic classifier; this changes with random forests\n",
    "    \n",
    "__9: Comparing Representations__\n",
    "\n",
    "- Use logistic classifier to compare twelve possible representations\n",
    "- BoT performs well, also Tfidf with more features (except cosine similarity)\n",
    "- Sensitivity needs most improvement, cannot break 0.9069 without random forestts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "__10: Random Forests 1__\n",
    "\n",
    "- Study hyperparameters, conduct grid search on BoT\n",
    "- Consider moving the decision threshold (plots precision-recall curves) to gain sensitivity\n",
    "    - a strategy that should be reserved for the final stages, since it doesn't improve the classifier itself\n",
    "    \n",
    "```\n",
    "threshold: 0.5\n",
    "accuracy: 0.9887\n",
    "sensitivity: 0.9225\n",
    "specificity: 0.9988\n",
    "\n",
    "threshold: 0.2\n",
    "accuracy: 0.9785\n",
    "sensitivity: 0.9767\n",
    "specificity: 0.9787\n",
    "```\n",
    "\n",
    "__11: Random Forests 2__\n",
    "\n",
    "- Grid search all 12 representations using a shallower test param grid\n",
    "- Compare run times with a py script (25min notebook, 15min command line)\n",
    "\n",
    "__12: Random Forests 3__\n",
    "\n",
    "- Run grid searches on notebooks since py script fails too often\n",
    "\n",
    "__13: Random Forests 4__\n",
    "\n",
    "- Evaluate results of previous grid searches, finds issue with low sensitivity and scaling of SVD\n",
    "- Go back and study SVD scaling (Notebook 6)\n",
    "- Unscaled SVD speeds up training and improves sensitivity to 0.95 with 0.5 threshold\n",
    "\n",
    "__14: Random Forests 5__\n",
    "\n",
    "- More grid searches, best results on unscaled SVD w/ 500 components on Tfidf, w/ spam cosine similarities\n",
    "- Noticing that quick one-time predictions are too variable, using 10-fold CV and studying variation is needed\n",
    "\n",
    "```\n",
    "threshold: 0.5\n",
    "accuracy: 0.9925\n",
    "sensitivity: 0.9639\n",
    "specificity: 0.9968\n",
    "```\n",
    "\n",
    "__15: Random Forests 6__\n",
    "\n",
    "- Upping the number of components in SVD is the most helpful tactic\n",
    "- The final model uses an 800-component SVD\n",
    "- The final params are: ```{'max_depth': 8, 'max_features': 150, 'min_samples_split': 3, 'n_estimators': 100}```\n",
    "- The best decision threshold is $0.3$\n",
    "- Accuracy, specificity, and most importantly sensitivity balance out at $\\approx{99.2\\%}$ with little variation\n",
    "- Mean validation sensitivity was 0.9742 with a mean fit time of $\\approx{45 sec}$ \n",
    "\n",
    "__16: Voting Classifier__\n",
    "\n",
    "- Wisdom of the crows doesn't uphold when there's an expert\n",
    "- Random forest vastly outperforms the simpler models and combination thereof (includes LR, SVC)\n",
    "\n",
    "__17: SVM 1__\n",
    "\n",
    "- Study SVC class\n",
    "\n",
    "__18: SVM 2__\n",
    "\n",
    "- Grid search SVC varying $C$ and $\\gamma$ yields 86% sensitivity\n",
    "- Increase voting classifier's sensitivity to 93%, still well below random forest alone\n",
    "\n",
    "__19: AdaBoost 1__\n",
    "\n",
    "- 10 trees and 0.1 learning rate achieves 98.5% sensitivity, trains quickly\n",
    "\n",
    "__20: AdaBoost 2__\n",
    "\n",
    "- 10-fold CV maxes out at about 97% sensitivity, for learning rates 0.001, 0.01 and number of trees ranging from 10 to 500\n",
    "\n",
    "__21: AdaBoost 3__\n",
    "\n",
    "- increasing max_depth (2+) overfits\n",
    "\n",
    "__22: Gradient Boosting 1__\n",
    "\n",
    "- single pred with defaults gets 98.5% sensitivity again\n",
    "- test grid search with 10-fold CSV gets same 97% validation accuracy\n",
    "- early stopping based on sensitivity varies too much, not useful\n",
    "\n",
    "__23: Gradient Boosting 2__\n",
    "\n",
    "- full param grid search finds best mean validation sensitivity at 0.974\n",
    "- this is  slightly above random forests which, for the .5 threshold, had a mean validation sensitivity of 0.69\n",
    "- it overfits easily and there seems to be no clear pattern for hyperparameters\n",
    "\n",
    "__24: Gradient Boosting 3__\n",
    "\n",
    "- best mean validation sensitivity 0.9769 `{max_depth=8, max_features=300, min_samples_split=5, n_estimators=100}`\n",
    "- final params overfit badly, even rf params might be doing so\n",
    "\n",
    "__25: Model Selection__\n",
    "\n",
    "- plot learning curves for best models, make sure they're not overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning\n",
    "\n",
    "__Thoughts__\n",
    "\n",
    "- This project is more about R&D not presentation, do not use it as presentation\n",
    "- Start creating a presentation?\n",
    "\n",
    "\n",
    "__Next Steps__\n",
    "\n",
    "- Modeling:\n",
    "    - Boosting\n",
    "    - Ensembles\n",
    "    - Select a final model, select two other promising ones\n",
    "\n",
    "\n",
    "- Settle on a cleanup-preprocessing pipeline\n",
    "    - Ex. if using Tfidf, keep idf, etc. (SVD, cosine similarity)\n",
    "    - Make sure it works with the test dataset\n",
    "    - Create a script?\n",
    "    \n",
    " \n",
    "- Evaluation: \n",
    "    - Evaluate the model chosen on the test test\n",
    "    - Evaluate two others just \"for kicks\" and to verify whether I made the correct model selection\n",
    "    - Questions: \n",
    "        - is this a scientific way to evaluate final model selection?\n",
    "        - if I chose to use the model that performs the best with the test set, how much overfitting does this imply?    \n",
    "\n",
    "- Presentation:\n",
    "    - Create a succinct presentation notebook and power point\n",
    "    \n",
    "\n",
    "__Topic Modeling__\n",
    "\n",
    "- Latent Dirichlet Allocation (LDA): study, read\n",
    "- lda2Vec (word2vec): study, read\n",
    "- NER: study, read\n",
    "\n",
    "__Extra__\n",
    "\n",
    "- what about using TextBlob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
