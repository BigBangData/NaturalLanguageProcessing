{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Test Set\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2021-02-15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def load_data(data):\n",
    "    raw_path = os.path.join(\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "X_test = load_data(\"X_test\") # change\n",
    "y_test = load_data(\"y_test\") # change\n",
    "y = y_test.copy() # change\n",
    "\n",
    "# transform y_array into int type\n",
    "y[y=='ham'] = 0\n",
    "y[y=='spam'] = 1\n",
    "y = y.astype('int')\n",
    "\n",
    "# load contractions map for custom cleanup\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW and Tfidf\n",
    "\n",
    "INTENTIONAL ERROR FOR COMPARISON?\n",
    "\n",
    "- test data is different\n",
    "- IDF is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom.clean_preprocess as cp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe = Pipeline([('counter', cp.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bot', cp.WordCounterToVectorTransformer(vocabulary_size=2000)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))])\n",
    "\n",
    "X_counter = pipe['counter'].fit_transform(X_test) # change\n",
    "X_bot = pipe['bot'].fit_transform(X_counter)\n",
    "\n",
    "X_tfidf_wrong = pipe['tfidf'].fit_transform(X_bot) # WRONGEDY WRONG WRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 2001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[26,  0,  1, ...,  0,  0,  0],\n",
       "        [24,  0,  1, ...,  0,  0,  0],\n",
       "        [15,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [10,  0,  0, ...,  0,  0,  0],\n",
       "        [57,  3,  1, ...,  0,  0,  0],\n",
       "        [27,  1,  1, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bot.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minute 8 in [Pre-processing our test data](https://www.youtube.com/watch?v=XWUi7RivDJY&list=PLTJTBoU5HOCR5Vkah2Z-AU76ZYsZjGFK6&index=11), here lies the problem:\n",
    "\n",
    "- The test data contains new n-grams... this is bad, we need the same n-grams from the training data (same features)\n",
    "- Each column has to be in the same order and have the same meaning, be the same \"word\", in R:\n",
    "\n",
    "\n",
    "```\n",
    "# Ensure the test dfm has the same n-grams as the training dfm.\n",
    "#\n",
    "# NOTE - In production we should expect that new text messages will \n",
    "#        contain n-grams that did not exist in the original training\n",
    "#        data. As such, we need to strip those n-grams out.\n",
    "#\n",
    "test.tokens.dfm <- dfm_select(test.tokens.dfm, pattern = train.tokens.dfm,\n",
    "                              selection = \"keep\")\n",
    "test.tokens.matrix <- as.matrix(test.tokens.dfm)\n",
    "test.tokens.dfm\n",
    "```\n",
    "\n",
    "- While I did select 2000 ngrams above, they are different 2000 ngrams than the training data. I should, potentially, choose a much higher vocabulary to try to accommodate for the fact that training and test data have diverging vocabularies, in an attempt to rescue as much vocabulary in the trainin as I can in the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "\n",
    "def perform_SVD(X, n_components=300): \n",
    "    \n",
    "    X_array = X.asfptype()\n",
    "    U, Sigma, VT = svds(X_array.T, \n",
    "                        k=n_components)\n",
    "    # reverse outputs\n",
    "    Sigma = Sigma[::-1]\n",
    "    U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "    \n",
    "    # return V\n",
    "    return VT.T\n",
    "\n",
    "# SVD with 800 components\n",
    "X_tfidf_svd = perform_SVD(X_tfidf_wrong, # YUP. WRONG.\n",
    "                          n_components=800) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X_tfidf_svd_allcos = cosine_similarity(X_tfidf_svd)\n",
    "\n",
    "test_df = pd.DataFrame({'sms':X_test, 'target':y_test}) # change\n",
    "\n",
    "# get spam indexes\n",
    "spam_ix = test_df.loc[test_df['target']=='spam'].index # change\n",
    "\n",
    "# calculate average spam similarity on SVD\n",
    "mean_spam_sims = []\n",
    "\n",
    "for ix in range(X_tfidf_svd_allcos.shape[0]):\n",
    "    mean_spam_sims.append(np.mean(X_tfidf_svd_allcos[ix, spam_ix]))\n",
    "\n",
    "X_tfidf_svd_spamcos = sp.hstack((csr_matrix(mean_spam_sims).T, X_tfidf_svd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 801)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_svd_spamcos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1672x801 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1339272 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_svd_spamcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-5.80042398e-04,  2.23569820e-02, -8.40956775e-03, ...,\n",
       "          2.01629513e-02,  1.30223037e-02,  1.68701983e-02],\n",
       "        [-2.14483682e-04,  2.38006692e-02, -1.45188925e-02, ...,\n",
       "         -2.65676338e-02,  2.42388434e-02,  5.53829668e-03],\n",
       "        [-7.11830359e-04,  5.21375410e-02, -1.36876042e-02, ...,\n",
       "          4.96163833e-02,  1.41231183e-04, -2.17165222e-03],\n",
       "        ...,\n",
       "        [-2.28196181e-04,  3.00118353e-02, -7.88647507e-03, ...,\n",
       "          1.74274889e-02, -1.95212903e-02, -1.43835931e-03],\n",
       "        [ 8.01057158e-03,  2.27854786e-02,  2.94052142e-02, ...,\n",
       "          2.39558645e-02,  3.57047719e-02, -4.26120041e-03],\n",
       "        [ 4.97053695e-04,  2.13937945e-02,  9.51652762e-05, ...,\n",
       "          9.07231013e-03, -1.31506711e-02,  4.29393375e-02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_svd_spamcos.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
