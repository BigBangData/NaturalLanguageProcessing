{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Pipeline 2\n",
    "\n",
    "This notebook is for developing the cleanup pipeline, not implementing it. Implementation is done via the `cleanup_module.py` which is imported into a notebook or script. \n",
    "\n",
    "*Purpose*\n",
    "- Preprocessing is performed for a TF-IDF representation. Text to DFM representations are explained in more detail in this [Document Term Matrices notebook.](10.extra_Document_Term_Matrices.ipynb)\n",
    "\n",
    "*Results*\n",
    "- In a hybrid approach, I create a pipeline that repurposes the custom **DocumentToWordCounterTransformer** class from the previous notebook and includes sklearn's **TfidfVectorizer**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import cleanup_module as Cmod\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['You love me', \n",
    "          'You do not love me',\n",
    "          'You really really love food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', Cmod.DocumentToWordCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=7)),\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_counter = pipe['counter'].fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bow = pipe['bow'].fit(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 1, 'love': 2, 'me': 3, 'really': 4, 'do': 5, 'not': 6, 'food': 7}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pipe_bow.transform(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 2, 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray() # first col is \"words missing from vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.288, 1.693, 1.693, 1.693, 1.693]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # IDF for the pipe_bow.vocabulary_\n",
    "[np.around(x,3) for x in pipe['tfidf'].fit(bow).idf_[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipe['tfidf'].fit_transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.   , 0.523, 0.523, 0.673, 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.   , 0.326, 0.326, 0.42 , 0.   , 0.552, 0.552, 0.   ]),\n",
       " array([0.   , 0.247, 0.247, 0.   , 0.838, 0.   , 0.   , 0.419])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,3) for x in tfidf.toarray()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.5228, 0.5228, 0.6733, 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.3263, 0.3263, 0.4202, 0.    , 0.5525, 0.5525, 0.    ]),\n",
       " array([0.    , 0.2474, 0.2474, 0.    , 0.8379, 0.    , 0.    , 0.4189])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entire pipeline produces same result but does't save IDF or vocab\n",
    "end_res = pipe.fit_transform(corpus)\n",
    "[np.around(x,4) for x in end_res.toarray()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using small POC sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample for dev\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.999, random_state=42)\n",
    "\n",
    "# create array\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1197,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipe\n",
    "X_end = pipe.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1197x501 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9799 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.605, 0.   , 0.127, 0.786, 0.   , 0.   ]),\n",
       " array([0., 0., 0., 0., 0., 0.]),\n",
       " array([0.302, 0.21 , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.144, 0.25 , 0.121, 0.   , 0.   , 0.   ]),\n",
       " array([0.438, 0.   , 0.056, 0.   , 0.   , 0.   ]),\n",
       " array([0.232, 0.   , 0.   , 0.   , 0.289, 0.   ]),\n",
       " array([0.319, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.272, 0.315, 0.   , 0.   , 0.225, 0.   ]),\n",
       " array([0.087, 0.302, 0.146, 0.   , 0.   , 0.   ]),\n",
       " array([0.116, 0.   , 0.   , 0.   , 0.   , 0.   ])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,3) for x in X_end.toarray()[:10,:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step by step for vocab and idf\n",
    "pipe_counter = pipe['counter'].fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow = pipe['bow'].fit(pipe_counter) \n",
    "pipe_bow.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 1)\n",
      "('USERNAME', 2)\n",
      "('NUMBER', 3)\n",
      "('my', 4)\n",
      "('you', 5)\n",
      "('not', 6)\n",
      "('have', 7)\n",
      "('am', 8)\n",
      "('me', 9)\n",
      "('just', 10)\n"
     ]
    }
   ],
   "source": [
    "for ix, w in enumerate(pipe_bow.vocabulary_.items()):\n",
    "    if ix < 10:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pipe_bow.transform(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [5, 2, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [3, 2, 0, ..., 0, 0, 0],\n",
       "       [1, 2, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()[:10] # misses too many words of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1111,\n",
       " 1.9314,\n",
       " 1.8619,\n",
       " 2.8899,\n",
       " 2.7605,\n",
       " 2.9294,\n",
       " 2.9123,\n",
       " 3.1684,\n",
       " 3.3093,\n",
       " 3.4537]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,4) for x in pipe['tfidf'].fit(bow).idf_[:10]] # IDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipe['tfidf'].fit_transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1197x501 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9799 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.605, 0.   , 0.127, 0.786, 0.   , 0.   ]),\n",
       " array([0., 0., 0., 0., 0., 0.]),\n",
       " array([0.302, 0.21 , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.144, 0.25 , 0.121, 0.   , 0.   , 0.   ]),\n",
       " array([0.438, 0.   , 0.056, 0.   , 0.   , 0.   ]),\n",
       " array([0.232, 0.   , 0.   , 0.   , 0.289, 0.   ]),\n",
       " array([0.319, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.272, 0.315, 0.   , 0.   , 0.225, 0.   ]),\n",
       " array([0.087, 0.302, 0.146, 0.   , 0.   , 0.   ]),\n",
       " array([0.116, 0.   , 0.   , 0.   , 0.   , 0.   ])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,3) for x in tfidf.toarray()[:10,:6]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate couple quick models to test `fit_perform` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', Cmod.DocumentToWordCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=1000)), # default vocab\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pipe.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1197x1001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10881 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.6984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(round(score.mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "score = cross_val_score(NB_clf, X_train_transformed, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(round(score.mean(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
