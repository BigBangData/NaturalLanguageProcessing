{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup & Preprocess - Bigrams\n",
    "\n",
    "---\n",
    "\n",
    "*Features* \n",
    "\n",
    "- The **DocumentToWordCounterTransformer** class is extended to a **DocumentToBigramCounterTransformer** class.\n",
    "- This implementation keeps unigrams. \n",
    "- Tfidf is performed on this Bag-of-(upto)-Bigrams representation.\n",
    "\n",
    "\n",
    "*Results*\n",
    "\n",
    "- using the new **DocumentToBigramCounterTransformer** class yields the following results with a vocabulary of 500 terms:\n",
    "\n",
    "| Model | Representation | Accuracy | Variance |\n",
    "|:---|:---|:---|:---|\n",
    "|Naive Bayes |Bag-of-upto-Bigrams | 0.9767 |(+/- 0.0062)|\n",
    "|Naive Bayes |BoB + TF-IDF| 0.9672 |(+/- 0.0073)|\n",
    "|Logistic Regr. |Bag-of-upto-Bigrams  | 0.9846 |(+/- 0.0062)|\n",
    "|Logistic Regr. |BoB + TF-IDF| 0.9649 |(+/- 0.0101)|\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2020-12-07\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time()\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    raw_path = os.path.join(\"..\",\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "X_train = load_data(\"X_train\")\n",
    "y_train = load_data(\"y_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanup_module as Cmod\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# full pipe\n",
    "pipe = Pipeline([('counter', Cmod.DocumentToBigramCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=500)),\n",
    "                 ('tfidf', TfidfTransformer())])\n",
    "\n",
    "X_counter = pipe['counter'].fit_transform(X_train)\n",
    "X_bow = pipe['bow'].fit_transform(X_counter)\n",
    "X_tfidf = pipe['tfidf'].fit_transform(X_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a couple baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9767 (+/- 0.0062)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "# Bag-of-bigrams\n",
    "score = cross_val_score(NB_clf, X_bow, y_train, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9672 (+/- 0.0073)\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-bigrams + Tfidf\n",
    "score = cross_val_score(NB_clf, X_tfidf, y_train, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9846 (+/- 0.0062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.5s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "# Bag-of-bigrams\n",
    "score = cross_val_score(log_clf, X_bow, y_train, cv=10, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649 (+/- 0.0101)\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-bigrams + Tfidf\n",
    "score = cross_val_score(log_clf, X_tfidf, y_train, cv=10, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0 m 12 s\n"
     ]
    }
   ],
   "source": [
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Time elapsed: {mins:0.0f} m {secs:0.0f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
