{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Cuves\n",
    "\n",
    "---\n",
    "\n",
    "__This Notebook__\n",
    "\n",
    "- plot learning curves to assess how models are training and whether they're overfitting\n",
    "\n",
    "__Results__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2021-02-13\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import joblib \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "    ShuffleSplit, StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import make_scorer, accuracy_score, \\\n",
    "    recall_score, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load target \n",
    "raw_path = os.path.join(\"data\",\"1_raw\")\n",
    "y_df = pd.read_csv(os.path.join(raw_path, 'y_train.csv'))\n",
    "y_array = np.array(y_df.iloc[:,0].ravel())\n",
    "y = y_array.copy()\n",
    "y[y=='ham'] = 0\n",
    "y[y=='spam'] = 1\n",
    "y = y.astype('int')\n",
    "\n",
    "# load matrix\n",
    "proc_dir = os.path.join(\"data\", \"2_processed\")\n",
    "X = sp.load_npz(os.path.join(proc_dir, \n",
    "                             'X_tfidf_svd800_spamcos.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot(clf, X, y, cv, verbose, train_sizes, n_jobs, \n",
    "               scorer_, metric, axes, axis):\n",
    "    \"\"\"\n",
    "    Trains and plots learning_curves, given:\n",
    "        scorer_: a make_scorer object\n",
    "        metric: str, the name of the metric\n",
    "    ...and all other args passed to learning_curve in the \n",
    "    plot_learning_curve function.\n",
    "    \"\"\"\n",
    "    # train\n",
    "    train_sizes, train_scores, test_scores = \\\n",
    "        learning_curve(clf, X, y, cv=cv, verbose=verbose,\n",
    "                       train_sizes=train_sizes, n_jobs=n_jobs,\n",
    "                       scoring=scorer_)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)    \n",
    "    \n",
    "    # plot\n",
    "    axes[axis].grid()\n",
    "    axes[axis].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[axis].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[axis].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[axis].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[axis].legend(loc=\"lower right\")\n",
    "    axes[axis].set_ylabel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(clf, title, X, y, axes=None, ylim=(.95, 1.01), \n",
    "                        cv=5, train_sizes=np.linspace(.1, 1.0, 5),\n",
    "                        verbose=0, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Adapted from the Plot Learning Curves example in Scikit-Learn \n",
    "    to show only the performance of a scorer, using accuracy, \n",
    "    sensitivity, and specificity as metrics.\n",
    "    \"\"\"\n",
    "    # set axes, title, ylims, xlabel\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 12))\n",
    "    \n",
    "    axes[0].set_title(title)\n",
    "    axes[2].set_xlabel(\"Training Examples\")\n",
    "    \n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "        axes[1].set_ylim(*ylim)\n",
    "        axes[2].set_ylim(*ylim)\n",
    "\n",
    "    # setup scorers\n",
    "    scorers = {\n",
    "        'acc': make_scorer(accuracy_score),\n",
    "        'tpr': make_scorer(recall_score, pos_label=1), # sensitivity\n",
    "        'tnr': make_scorer(recall_score, pos_label=0) # specificity\n",
    "    }\n",
    "\n",
    "    # plots\n",
    "    train_plot(clf, X, y, cv=cv, verbose=verbose, \n",
    "               train_sizes=train_sizes, n_jobs=n_jobs, \n",
    "               scorer_=scorers['acc'], metric='Accuracy', \n",
    "               axes=axes, axis=0)\n",
    "    train_plot(clf, X, y, cv=cv, verbose=verbose, \n",
    "               train_sizes=train_sizes, n_jobs=n_jobs, \n",
    "               scorer_=scorers['tpr'], metric='Sensitivity', \n",
    "               axes=axes, axis=1)\n",
    "    train_plot(clf, X, y, cv=cv, verbose=verbose, \n",
    "               train_sizes=train_sizes, n_jobs=n_jobs, \n",
    "               scorer_=scorers['tnr'], metric='Specificity', \n",
    "               axes=axes, axis=2)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_classifiers(clf1, clf2, title1, title2):\n",
    "    \"\"\"\n",
    "    Compares two classifier's learning curves.\n",
    "    \"\"\"\n",
    "    t = time.time()\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "    plot_learning_curve(clf1, title1, X, y, axes=axes[:, 0], cv=cv)\n",
    "    plot_learning_curve(clf2, title2, X, y, axes=axes[:, 1], cv=cv)\n",
    "    m, s = divmod(time.time() - t, 60)\n",
    "    print(f'Elapsed: {m:0.0f} m {s:0.0f} s')\n",
    "    plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "title1 = r\"Learning Curves (Gradient Boosting Nbk.2)\"\n",
    "title2 = r\"Learning Curves (Gradient Boosting Nbk.3)\"\n",
    "\n",
    "clf1 = GradientBoostingClassifier(\n",
    "    random_state=42,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=1,\n",
    "    n_estimators=50\n",
    ")\n",
    "\n",
    "clf2 = GradientBoostingClassifier(\n",
    "    random_state=42, \n",
    "    max_depth=8, \n",
    "    max_features=300, \n",
    "    min_samples_split=5, \n",
    "    n_estimators=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare_two_classifiers(clf1, clf2, title1, title2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
