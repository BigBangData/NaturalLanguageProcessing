{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "*Primary*\n",
    "\n",
    "- create a portfolio-worthy data science project\n",
    "- start a portfolio of projects online\n",
    "\n",
    "*Secondary*\n",
    "\n",
    "- learn NLP\n",
    "- practice ML, build classifiers for unstructured, text data\n",
    "\n",
    "*Main Steps*\n",
    "\n",
    "- recreate the 'Intro To Text Analytics With R' project in Python\n",
    "- repurpose and generalize the NLP-ML workflow with Twitter data\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General ML Project Structure\n",
    "\n",
    "1. define the problem, set expectations and evaluation criteria\n",
    "2. preliminary and minimal EDA and pre-splitting cleanup\n",
    "3. split dataset into trainining and test subsets; set the test subset aside\n",
    "4. create a cleanup and preprocessing pipeline for the training data that can be re-applied to the test data\n",
    "5. train a couple baseline models to ensure process is smooth and pre-processing is dialed in\n",
    "6. using cross-validation, evaluate a variety models without hyperparameter tuning to establish some baselines\n",
    "7. short-list promising models for further hyperparemeter tuning\n",
    "8. iterate on any phase of the project as needed\n",
    "9. consider feature selection and feature engineering (this can be done earlier)\n",
    "10. decide on a final cleanup and processing pipeline\n",
    "11. settle on a final model\n",
    "12. re-apply all cleanup and processing steps to the test set and evaluate final model - once\n",
    "13. create a final presentation of the solution for technical and non-technical audiences\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Project Definition\n",
    "\n",
    "There is a lot more unstructured, text data than structured data. To leverage this unstructured text data one needs to apply text analytic techniques, or 'natural language processing', to structure the data and get value from it. \n",
    "\n",
    "Value can be defined in many ways. \n",
    "\n",
    "**Example 1.** \n",
    "\n",
    "A common need in businesses that capture text data is to be able to extract keywords from this text data. Say a business has an online app that historically has captured information in a text field, but nobody has had the time to read that input. Going forward, product managers decide this field should be turned into a drop-down menu. To integrate the past and future states of this field, there's now a need to bucket the unstructured text information into categories in the new drop-down menu. To solve this, a data scientist can apply entity extraction techniques: \n",
    "\n",
    "> \"Named-entity recognition (NER) [...] is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories...\" [Wikipedia, accessed Dec 19, 2020](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "\n",
    "\n",
    "**Example 2.** \n",
    "\n",
    "```\n",
    "[TODO: needs revision]\n",
    "```\n",
    "\n",
    "Given the goal of binary classification, say, to separate spam from ham (legitimate) messages or emails, or *positive* (happier) from *negative* (unhappier) Tweets, one can build classifiers that learn from a corpus and is able to predict, given a new instance (new email), wether it is spam or not. The spam/ham problem is old and mostly solved, and the positive/negative Tweet is also well known and studied. It is less clear to me whether classifiers build for one would also perform well in the other realm. Being able to generalize a workflow with quick deployment and testing of various classifiers is still challenging - many \"out-of-the-box\" analytics tools fail to deliver. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Project: Introduction to Text Analytics With R\n",
    "\n",
    "This is a Python offshoot of the original YouTube series tutorial by David Langer from Data Science Dojo: [DSD's Introduction to Text Analytics with R](https://www.youtube.com/playlist?list=PLTJTBoU5HOCR5Vkah2Z-AU76ZYsZjGFK6)\n",
    "\n",
    "\n",
    "#### What is the problem this project is trying to solve?\n",
    "\n",
    "\n",
    "```\n",
    "TODO....\n",
    "```\n",
    "\n",
    "\n",
    "#### What are the expectations?\n",
    "\n",
    "\n",
    "```\n",
    "TODO....\n",
    "```\n",
    "\n",
    "\n",
    "#### What does *success** look like for this project?\n",
    "\n",
    "\n",
    "```\n",
    "TODO....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Management\n",
    "\n",
    "#### Status History\n",
    "\n",
    "*Notebook 3: Tfidf*\n",
    "\n",
    "- $97.5\\%$ accuracy with a Tfidf matrix of 450 unigram terms on sklearn's baseline logistic classifier.\n",
    "\n",
    "*Notebook 4: Bigrams*\n",
    "\n",
    "- $98.46\\%$ accuracy with a Bag-of-(upto)-Bigrams of 500 terms on sklearn's baseline logistic classifier.\n",
    "\n",
    "*Notebook 5: Ngrams*\n",
    "    \n",
    "- $98.59\\%$ accuracy with a Bag-of-(upto)-Trigrams with 2,000 terms on sklearn's baseline logistic classifier.\n",
    "    \n",
    "*Notebook 6: Dimensionality Reduction*\n",
    "\n",
    "- BoT with 2,000 terms has best accuracy and sensitivity (0.9069), great specificity (0.9979)\n",
    "- SVD with 300 components on Tfidf has second highest accuracy (0.9840) and high sensitivity (0.8798) and specificity (0.9993)\n",
    "\n",
    "*Notebook 7: Feature Engineering*\n",
    "\n",
    "- After visualizations, the first feature (raw document length) appears to be the most useful in separating the target\n",
    "- After a quick modeling stage, all features but the RSR have the best accuracy (0.8823), yet low sensitivity (0.3270) and high specificity (0.9672)\n",
    "\n",
    "*Notebook 8: Cosine Similarity*\n",
    "\n",
    "With the baseline logistic classifier:\n",
    "\n",
    "- Adding cosine similarity to BoT or Tfidf representations does not improve them\n",
    "- This needs to be tested with more complex models and in conjunction with new features\n",
    "    \n",
    "*Notebook 9: Comparing Representations*\n",
    "\n",
    "Features:\n",
    "- Considered the entire problem space and defined 12 possible representations\n",
    "\n",
    "After the preliminary modeling phase with a baseline logistic classifier:\n",
    "\n",
    "- Overall accuracy and specificity are high enough, sensitivity is the metric that needs improvement\n",
    "- BoT alone seems to perform quite well, but SVD without Tfidf degrades it\n",
    "- The more features we add to Tfidf the better it gets, perhaps with the exception of cosine similarities\n",
    "- Performance with more complex models may vary; further testing is warranted\n",
    "\n",
    "*Notebook 10: Random Forests 1*\n",
    "\n",
    "Two strategies identified for optimizing sensitivity:\n",
    "1. grid search using scorers and refit to optimize for sensitivity\n",
    "2. move decision threshold and observe effects in precision-recall curve and specificity\n",
    "\n",
    "Grid searching on every representation is too expensive, only used the BoT.\n",
    "\n",
    "*Notebook 11: Random Forests 2*\n",
    "\n",
    "- Grid search all 12 representations using a shallower test param grid\n",
    "- Compare run times with a py script, which is 1.7x faster (25m for notebook, 15m from command line)\n",
    "\n",
    "Prepared py script to run from command line.\n",
    "\n",
    "*Notebook 12: Random Forests 3*\n",
    "\n",
    "Evaluates gridsearch run.\n",
    "\n",
    "\n",
    "\n",
    "#### House cleaning\n",
    "\n",
    "- make sure all notebooks have attributions, nice intro, etc.\n",
    "- make sure notebooks make sense to someone unfamiliar with project\n",
    "    - needs intermediate notebooks explaining text cleanup with demos\n",
    "- update README\n",
    "\n",
    "\n",
    "#### Current Status\n",
    "\n",
    "- run py script to grid search 12 representations.\n",
    "\n",
    "\n",
    "#### Future Steps\n",
    "\n",
    "- Settle on a cleanup-preprocessing pipeline\n",
    "    - Ex. if using Tfidf, keep idf, etc. (SVD, cosine similarity)\n",
    "- Create a script for the pipeline \n",
    "    - Make sure it works with the test dataset\n",
    "    \n",
    "- Modeling\n",
    "- Evaluation\n",
    "- Presentation\n",
    "\n",
    "#### Topic Modeling\n",
    "\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- lda2Vec (word2vec)\n",
    "\n",
    "#### Statistical Modeling\n",
    "\n",
    "- SGD\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Boosting\n",
    "- SVC\n",
    "- Ensembles\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "- Random Forest + VarImp plot \n",
    "- LASSO\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "- MCC\n",
    "- confusion matrix\n",
    "- specific predictions\n",
    "\n",
    "#### Thoughts\n",
    "\n",
    "- what about using TextBlob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
