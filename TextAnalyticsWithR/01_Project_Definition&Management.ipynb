{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General: Goals\n",
    "\n",
    "- create portfolio-worthy data science projects\n",
    "- learn NLP and practice ML by building classifiers that use open text fields as input\n",
    "- create a Python framework that generalizes the 'Intro To Text Analytics With R' workflow\n",
    "- try the framework with Twitter data instead of SMS data \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General: ML Project Structure\n",
    "\n",
    "These steps do not need to be sequential. For example, feature engineering can be done sooner. They are also cyclic: in practice, one might need to continuously re-define the problem, and iterate on EDA and modeling steps.\n",
    "\n",
    "1. define the problem, set expectations and evaluation criteria\n",
    "2. preliminary and minimal EDA and pre-splitting cleanup\n",
    "3. split dataset into trainining and test subsets; set the test subset aside\n",
    "4. create a cleanup and preprocessing pipeline for the training data that can be re-applied to the test data\n",
    "5. train a couple baseline models to ensure process is smooth and pre-processing is dialed in\n",
    "6. using cross-validation, evaluate a variety models without hyperparameter tuning to establish some baselines\n",
    "7. short-list promising models for further hyperparemeter tuning\n",
    "8. iterate on any phase of the project as needed\n",
    "9. consider feature selection and feature engineering\n",
    "10. decide on a final cleanup and processing pipeline\n",
    "11. settle on a final model\n",
    "12. re-apply all cleanup and processing steps to the test set and evaluate final model - once\n",
    "13. create a final presentation of the solution for technical and non-technical audiences\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General: Project Definition\n",
    "\n",
    "There is a lot more unstructured, text data than structured data. To leverage this unstructured text data one needs to apply text analytic techniques to structure the data before getting value from it. \n",
    "\n",
    "Value can be defined in many ways, here are a couple:\n",
    "\n",
    "**Framework for Binary Classification** \n",
    "\n",
    "\n",
    "Given the goal of binary classification, say, to separate *spam* from *ham* (legitimate) messages, or *positive* (happier) from *negative* (unhappier) comments on social media, one can build classifiers that learn from a corpus and is able to predict, given a new instance (new message or comment), wether it is spam or not, or positive or negative. \n",
    "\n",
    "Both cases are well-known and mostly solved. It is less clear to me whether classifiers built for spam detection would also perform well, say, predicting negative Tweets. Having a framework that quickly deploys and assesses the accuracy of classifiers for binary predictions given open text fields seems a valuable pursuit.\n",
    "\n",
    "\n",
    "**Entity Extraction** \n",
    "\n",
    "A common need in businesses that capture text data is to be able to extract keywords from this text data. Say a business has an online app that historically has captured information in a text field, but nobody has had the time to read that input. Going forward, product managers decide this field should be turned into a drop-down menu. To integrate the past and future versions of this field, there's a need to bucket the unstructured text into categories for the new drop-down. To solve this, a data scientist can apply entity extraction techniques:\n",
    "\n",
    "> \"Named-entity recognition (NER) [...] is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories...\" [Wikipedia, accessed Dec 19, 2020](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Project: Introduction to Text Analytics With R\n",
    "\n",
    "This is a Python offshoot of the original YouTube series tutorial by David Langer from Data Science Dojo: [DSD's Introduction to Text Analytics with R](https://www.youtube.com/playlist?list=PLTJTBoU5HOCR5Vkah2Z-AU76ZYsZjGFK6)\n",
    "\n",
    "\n",
    "#### What is the problem this project is trying to solve?\n",
    "\n",
    "Separate SPAM from HAM (legitimate) SMS messages.\n",
    "\n",
    "\n",
    "#### What are the expectations?\n",
    "\n",
    "That the classifier built for this purpose can do this task quickly and accurately enough to avoid frustrating users.\n",
    "\n",
    "\n",
    "#### What does success look like for this project?\n",
    "\n",
    "\n",
    "A classifier that achieves high sensitivity, thus avoiding sending legitimate SMS to the spam folder, which would be a lot more frustrating than the occasional spam not being filtered. This classifier also has somewhat high specificicity, it filters most spam messages as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Project: The Data\n",
    "\n",
    "The dataset is now spread across the internet, perhaps a good source is the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection#). The dataset is a collage of sources; the data collection process is explained by the original authors Tiago A. Almeida and José María Gómez Hidalgo in this [University of Campinas website.](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Project: Management\n",
    "\n",
    "#### Status History\n",
    "\n",
    "*Notebook 3: Tfidf*\n",
    "\n",
    "- Unigram Tfidf with 450 terms, logistic classifier: 0.975 accuracy\n",
    "\n",
    "*Notebook 4: Bigrams*\n",
    "\n",
    "- Bag-of-upto-Bigrams with 500 terms, logistic classifier: 0.985 accuracy\n",
    "\n",
    "*Notebook 5: Ngrams*\n",
    "    \n",
    "- Bag-of-upto-Trigrams (BoT) with 2,000 terms, logistic classifier: 0.986 accuracy\n",
    "    \n",
    "*Notebook 6a-b: Dimensionality Reduction*\n",
    "\n",
    "- SVD with 300 components on Tfidf: 0.9864 accuracy, 0.9089 sensitivity, 0.9982 specificity\n",
    "- SVD not clearly advantageous with a logistic classifier, but that will change with more complex models\n",
    "- Notebook 6b experiments with scaling vs not: similar results \n",
    "\n",
    "*Notebook 7: Feature Engineering*\n",
    "\n",
    "- After data viz, the first feature (raw document length) is the most useful in separating the target\n",
    "- After modeling, all features but the RSR achieve best accuracy (0.8823), yet low sensitivity (0.3270)\n",
    "\n",
    "*Notebook 8: Cosine Similarity*\n",
    "\n",
    "- Benefit of spam cosine similarity unclear with logistic classifier; this changes with random forests\n",
    "    \n",
    "*Notebook 9: Comparing Representations*\n",
    "\n",
    "- Use logistic classifier to compare twelve possible representations\n",
    "- BoT performs well, also Tfidf with more features (except cosine similarity)\n",
    "- Sensitivity needs most improvement, cannot break 0.9069 without random forestts\n",
    "\n",
    "*Notebook 10: Random Forests 1*\n",
    "\n",
    "- Study hyperparameters, conduct grid search on BoT\n",
    "- Consider moving the decision threshold (plots precision-recall curves) to gain sensitivity\n",
    "    - a strategy that should be reserved for the final stages, since it doesn't improve the classifier itself\n",
    "    \n",
    "```\n",
    "threshold: 0.5\n",
    "accuracy: 0.9887\n",
    "sensitivity: 0.9225\n",
    "specificity: 0.9988\n",
    "\n",
    "threshold: 0.2\n",
    "accuracy: 0.9785\n",
    "sensitivity: 0.9767\n",
    "specificity: 0.9787\n",
    "```\n",
    "\n",
    "\n",
    "*Notebook 11: Random Forests 2*\n",
    "\n",
    "- Grid search all 12 representations using a shallower test param grid\n",
    "- Compare run times with a py script (25min notebook, 15min command line)\n",
    "\n",
    "*Notebooks 12a-c: Random Forests 3*\n",
    "\n",
    "- Run grid searches on notebooks since py script fails too often\n",
    "\n",
    "*Notebook 13: Random Forests 4*\n",
    "\n",
    "- Evaluate results of previous grid searches, finds issue with low sensitivity and scaling of SVD\n",
    "- Go back and study SVD scaling (Notebook 6)\n",
    "- Unscaled SVD speeds up training and improves sensitivity to 0.95 with 0.5 threshold\n",
    "\n",
    "*Notebook 14: Random Forests 5*\n",
    "\n",
    "- More grid searches, best results on unscaled SVD w/ 500 components on Tfidf, w/ spam cosine similarities\n",
    "- Noticing that quick one-time predictions are too variable, using 10-fold CV and studying variation is needed\n",
    "\n",
    "```\n",
    "threshold: 0.5\n",
    "accurady: 0.9925\n",
    "sensitivity: 0.9639\n",
    "specificity: 0.9968\n",
    "```\n",
    "\n",
    "*Notebook 15: Random Forests 6*\n",
    "\n",
    "- Upping the number of components in SVD is the most helpful tactic\n",
    "- The final model uses an 800-component SVD\n",
    "- The final params are: ```{'max_depth': 8, 'max_features': 150, 'min_samples_split': 3, 'n_estimators': 100}```\n",
    "- The best decision threshold is $0.3$\n",
    "- Accuracy, specificity, and most importantly sensitivity balance out at $\\approx{99.2\\%}$ with little variation\n",
    "- Mean validation sensitivity was 0.9742 with a mean fit time of $\\approx{45 sec}$ \n",
    "\n",
    "*Notebook 15: Voting Classifier*\n",
    "\n",
    "- Wisdom of the crows doesn't uphold when there's an expert\n",
    "- Random forest vastly outperforms the simpler models and combination thereof (includes LR, SVC)\n",
    "\n",
    "*Notebook 16: SVM 1*\n",
    "\n",
    "- Study SVC class, stumbles around\n",
    "\n",
    "*Notebook 17: SVM 2*\n",
    "\n",
    "- Grid search SVC varying C and gamma yields 86% sensitivity\n",
    "- Increase voting classifier's sensitivity to 93%, still well below random forest alone\n",
    "\n",
    "\n",
    "#### House cleaning\n",
    "\n",
    "- make sure all notebooks have attributions, nice intro, etc.\n",
    "- make sure notebooks make sense to someone unfamiliar with project\n",
    "    - needs intermediate notebooks explaining text cleanup with demos\n",
    "- update README\n",
    "\n",
    "\n",
    "#### Current Status\n",
    "\n",
    "\n",
    "\n",
    "#### Future Steps\n",
    "\n",
    "- Settle on a cleanup-preprocessing pipeline\n",
    "    - Ex. if using Tfidf, keep idf, etc. (SVD, cosine similarity)\n",
    "- Create a script for the pipeline \n",
    "    - Make sure it works with the test dataset\n",
    "- Modeling:\n",
    "    - SGD\n",
    "    - Boosting\n",
    "    - Ensembles\n",
    "    - Select a final model, select two other promising ones\n",
    "- Evaluation: evaluate the model chosen on the test, and evaluate two others to see whether decision was correct?\n",
    "    - Question: is this a scientific way to evaluate final model selection?\n",
    "- Presentation: ooph\n",
    "\n",
    "#### Topic Modeling\n",
    "\n",
    "- Latent Dirichlet Allocation (LDA): study, read\n",
    "- lda2Vec (word2vec): study, read\n",
    "- NER: study, read\n",
    "\n",
    "#### Statistical Modeling\n",
    "\n",
    "- SGD\n",
    "- Boosting\n",
    "- Ensembles\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "- Random Forest + VarImp plot \n",
    "- LASSO\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "- MCC\n",
    "- confusion matrix\n",
    "- specific predictions\n",
    "\n",
    "#### Thoughts\n",
    "\n",
    "- what about using TextBlob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
