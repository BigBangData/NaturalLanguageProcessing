{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "\n",
    "## Structure\n",
    "\n",
    "1. set expectations and define the problem, including evaluation criteria\n",
    "2. preliminary and minimal EDA and pre-splitting cleanup\n",
    "3. split dataset into trainining and test subsets; set the test subset aside\n",
    "4. create a cleanup pipeline for the training data that can be re-applied to the test data\n",
    "    * a POC is done by sampling down the sentiment140 dataset\n",
    "5. create an ML pre-processing pipeline that can be re-applied as well\n",
    "6. train a couple baseline models to ensure process is smooth and pre-processing is dialed in\n",
    "7. using cross-validation, evaluate a variety models without hyperparameter tuning to establish some baselines\n",
    "8. short-list promising models for further hyperparemeter tuning\n",
    "9. iterate on any phase of the project as needed\n",
    "10. consider feature selection and feature engineering\n",
    "11. decide on a final cleanup and processing pipeline\n",
    "    * apply to the entire training subset\n",
    "12. settle on a final model\n",
    "    * cross-validate results using the entire training subset\n",
    "13. re-apply all cleanup and processing steps to the test set and evaluate final model - once\n",
    "14. create a final presentation of the solution for technical and non-technical audiences\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "First Goal:\n",
    "\n",
    "- learn NLP and machine learning\n",
    "- train classifiers to predict positive/negative sentiment of a Tweet using the sentiment140 data\n",
    "- compare with TextBlob sentiment polarity\n",
    "- evaluate models based on accuracy at first, then precision-recall, or MCC\n",
    "\n",
    "The application or an \"app idea\" is beyond the scope of the project - this could be any app that uses short text input from users and needs to, as part of its process, evaluate the \"sentiment\" of the text. An example would be Twitter users who download their Tweets for the last year and want to know how positive or negative their posts were, in a timeline for example, or as aggregate in comparison with a cohort of friends. Another example would be an app that tracks positive/negative sentiments around a topic, by using hashtags and whatnot, in conjunction with this sentiment evaluation.\n",
    "\n",
    "The scope of this project is to provide a model that most accurately predicts the sentiment of a Tweet - whether negative or positive, and does so within a reasonable amount of time, but not as fast as possible (not real time). The evaluation criteria of what constitutes \"success\" and \"most accurate prediction\" is TBD. I might use ROC/AUC curves and whatnot. Accuracy alone isn't enough without considering recall.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Management\n",
    "\n",
    "#### Former Stati\n",
    "\n",
    "- TextBlob revealed issues with predictions:\n",
    "    * they are less accurate for the sentiment140 data\n",
    "    * testing some very clear new cases showed TextBlob to be a lot more stable\n",
    "    * *why are predictions unstable with the classifiers (NB, LR)?*\n",
    "        - hypothesis: too many variables\n",
    "        \n",
    "- Tested the POC process so far with the twitterbot data:\n",
    "    * so far accuracies are lower\n",
    "    * this just introduces more complexity - stop\n",
    "    \n",
    "- SVD did not improve accuracy, speed, or size\n",
    "\n",
    "- Added Ngrams:\n",
    "    * Bigrams: 80% accuracy, m=120k, n=50k, Tfidf, LR, 18s train time\n",
    "    * Trigrams: 80% accuracy, m=120km n=100k, Tfidf, LR, 3s train time\n",
    "    \n",
    "#### Previous Status\n",
    "\n",
    "- Feature Eng:\n",
    "    * got 59% accuracy with 7 features\n",
    "    * no improvement when joined with Trigrams\n",
    "    * no improvement when joined with SVD\n",
    "    \n",
    "#### Current Status\n",
    "\n",
    "- Cosine Similarity \n",
    "    * revisit R Analytics\n",
    "\n",
    "\n",
    "#### Future Steps\n",
    "\n",
    "- Finalize cleanup pipeline: from dev versions to implementation\n",
    "    * maybe use a .py script\n",
    "    \n",
    "- Start modeling again\n",
    "\n",
    "#### Topic Modeling\n",
    "\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- lda2Vec (word2vec)\n",
    "\n",
    "#### Statistical Modeling\n",
    "\n",
    "- SGD\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Boosting\n",
    "- SVC\n",
    "- Ensembles\n",
    "- Evaluate: MCC, look at confusion matrix\n",
    "\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "- Random Forest + VarImp plot \n",
    "- LASSO\n",
    "\n",
    "#### Thoughts\n",
    "\n",
    "- run with full data only as last resort\n",
    "- run with better data, whether curated dataset or other data\n",
    "\n",
    "#### Goal\n",
    "\n",
    "- what's the use case?\n",
    "    * Ex. app idea where user inputs text to get polarity, but TextBlob does that well already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
