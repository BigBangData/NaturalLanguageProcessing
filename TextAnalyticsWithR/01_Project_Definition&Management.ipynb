{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "*Primary*\n",
    "\n",
    "- create a portfolio-worthy data science project\n",
    "- start a portfolio of projects online\n",
    "\n",
    "*Secondary*\n",
    "\n",
    "- learn NLP\n",
    "- practice ML: build classifiers for unstructured, text data\n",
    "\n",
    "*Main Steps*\n",
    "\n",
    "- recreate the 'Intro To Text Analytics With R' project in Python\n",
    "- repurpose and generalize the NLP-ML workflow with Twitter data\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "1. define the problem, set expectations and evaluation criteria\n",
    "2. preliminary and minimal EDA and pre-splitting cleanup\n",
    "3. split dataset into trainining and test subsets; set the test subset aside\n",
    "4. create a cleanup and preprocessing pipeline for the training data that can be re-applied to the test data\n",
    "5. train a couple baseline models to ensure process is smooth and pre-processing is dialed in\n",
    "6. using cross-validation, evaluate a variety models without hyperparameter tuning to establish some baselines\n",
    "7. short-list promising models for further hyperparemeter tuning\n",
    "8. iterate on any phase of the project as needed\n",
    "9. consider feature selection and feature engineering (this can be done earlier)\n",
    "10. decide on a final cleanup and processing pipeline\n",
    "11. settle on a final model\n",
    "12. re-apply all cleanup and processing steps to the test set and evaluate final model - once\n",
    "13. create a final presentation of the solution for technical and non-technical audiences\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Definition\n",
    "\n",
    "There is a lot more unstructured, text data than structured data. To leverage this unstructured text data one needs to apply text analytic techniques, or 'natural language processing', to structure the data and get value from it. \n",
    "\n",
    "Value can be defined in many ways. \n",
    "\n",
    "**Example 1.** \n",
    "\n",
    "A common need in businesses that capture text data is to be able to extract keywords from this text data. Say a business has an online app that historically has captured information in a text field, but nobody has had the time to read that input. Going forward, product managers decide this field should be turned into a drop-down menu. To integrate the past and future states of this field, there's now a need to bucket the unstructured text information into categories in the new drop-down menu. To solve this, a data scientist can apply entity extraction techniques: \n",
    "\n",
    "> \"Named-entity recognition (NER) [...] is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories...\" [Wikipedia, accessed Dec 19, 2020](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "\n",
    "\n",
    "**Example 2.** \n",
    "\n",
    "```\n",
    "[TODO: needs revision]\n",
    "```\n",
    "\n",
    "Given the goal of binary classification, say, to separate spam from ham (legitimate) messages or emails, or *positive* (happier) from *negative* (unhappier) Tweets, one can build classifiers that learn from a corpus and is able to predict, given a new instance (new email), wether it is spam or not. The spam/ham problem is old and mostly solved, and the positive/negative Tweet is also well known and studied. It is less clear to me whether classifiers build for one would also perform well in the other realm. Being able to generalize a workflow with quick deployment and testing of various classifiers is still challenging - many \"out-of-the-box\" analytics tools fail to deliver. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Project: Text Analytics With R\n",
    "\n",
    "This is a Python offshoot of the original YouTube series tutorial by David Langer from Data Science Dojo: [DSD's Introduction to Text Analytics with R](https://www.youtube.com/playlist?list=PLTJTBoU5HOCR5Vkah2Z-AU76ZYsZjGFK6)\n",
    "\n",
    "\n",
    "#### What is the problem this project is trying to solve?\n",
    "\n",
    "\n",
    "\n",
    "#### What are the expectations?\n",
    "\n",
    "\n",
    "```\n",
    "TODO....\n",
    "```\n",
    "\n",
    "\n",
    "#### What does *success** look like for this project?\n",
    "\n",
    "\n",
    "```\n",
    "TODO....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Management\n",
    "\n",
    "#### Status History\n",
    "\n",
    "*Notebook 3: Tfidf*\n",
    "\n",
    "- $97.5\\%$ accuracy with a Tfidf matrix of 450 unigram terms on sklearn's baseline logistic classifier.\n",
    "\n",
    "*Notebook 4: Bigrams*\n",
    "\n",
    "- $98.46\\%$ accuracy with a Bag-of-(upto)-Bigrams of 500 terms on sklearn's baseline logistic classifier.\n",
    "\n",
    "*Notebook 5: Ngrams*\n",
    "    \n",
    "- $98.59\\%$ accuracy with a Bag-of-(upto)-Trigrams with 2,000 terms on sklearn's baseline logistic classifier.\n",
    "    \n",
    "*Notebook 6: Dimensionality Reduction*\n",
    "\n",
    "- BoT with 2,000 terms has best accuracy and sensitivity (0.9069), great specificity (0.9979)\n",
    "- SVD with 300 components on Tfidf has second highest accuracy (0.9840) and high sensitivity (0.8798) and specificity (0.9993)\n",
    "\n",
    "*Notebook 7: Feature Engineering*\n",
    "\n",
    "- After visualizations, the first feature (raw document length) appears to be the most useful in separating the target\n",
    "- After a quick modeling stage, all features but the RSR have the best accuracy (0.8823), yet low sensitivity (0.3270) and high specificity (0.9672)\n",
    "\n",
    "*Notebook 8: Comparing Representations*\n",
    "\n",
    "- Given the baseline logistic classifier:\n",
    "    - Overall accuracy and specificity are high\n",
    "    - Sensitivity, which is desired, is the metric that needs improvement\n",
    "\n",
    "- Best representations:\n",
    "    - BoT alone, acc:0.9859, tpr:0.9069, tnr:0.9979\n",
    "    - BoT with all features but RSR, acc:0.9849, tpr:0.9031, tnr:0.9973\n",
    "    - SVD with all features but RSR (to test out with a more complex model), acc:0.9828, tpr:0.8973, tnr:0.9959\n",
    "    \n",
    "*Notebook 9: Cosine Similarity*\n",
    "\n",
    "- Given the basline logistic classifier:\n",
    "    - Cosine similarities (over spam SMS) appear helpful upon visualization (esp. Tfidf)\n",
    "    - Only predict the ham base rate with the simple LR model (BoT = Tfidf)\n",
    "    \n",
    "- This needs testing with a more complex model\n",
    "\n",
    "#### House cleaning\n",
    "\n",
    "- add some intros to notebooks; add source info\n",
    "- move data folder to project\n",
    "- add clean data subfolder and redo loading in notebooks\n",
    "- update README\n",
    "\n",
    "\n",
    "#### Current Status\n",
    "\n",
    "Questions:\n",
    "\n",
    "- original added cosine similarity to SVD on Tfidf of BoT\n",
    "- use stratified train-test split for first split?\n",
    "\n",
    "- Random Forests 2: evaluate first gridsearch\n",
    "\n",
    "\n",
    "#### Future Steps\n",
    "\n",
    "- Settle on a cleanup-preprocessing pipeline\n",
    "    - Ex. if using Tfidf, keep idf, etc. (SVD, cosine similarity)\n",
    "- Create a script for the pipeline \n",
    "    - Make sure it works with the test dataset\n",
    "    \n",
    "- Modeling\n",
    "- Evaluation\n",
    "- Presentation\n",
    "\n",
    "#### Topic Modeling\n",
    "\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- lda2Vec (word2vec)\n",
    "\n",
    "#### Statistical Modeling\n",
    "\n",
    "- SGD\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Boosting\n",
    "- SVC\n",
    "- Ensembles\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "- Random Forest + VarImp plot \n",
    "- LASSO\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "- MCC\n",
    "- confusion matrix\n",
    "- specific predictions\n",
    "\n",
    "#### Thoughts\n",
    "\n",
    "- what about using TextBlob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
