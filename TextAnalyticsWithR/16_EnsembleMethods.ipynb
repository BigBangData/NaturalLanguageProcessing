{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2021-01-21\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import joblib \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def load_data(data):\n",
    "    raw_path = os.path.join(\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "X_train = load_data(\"X_train\")\n",
    "y_train = load_data(\"y_train\")\n",
    "\n",
    "y = y_train.copy()\n",
    "\n",
    "# transform y_array into int type\n",
    "y[y=='ham'] = 0\n",
    "y[y=='spam'] = 1\n",
    "y = y.astype('int')\n",
    "\n",
    "# load contractions map for custom cleanup\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom.clean_preprocess as cp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe = Pipeline([('counter', cp.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bot', cp.WordCounterToVectorTransformer(vocabulary_size=2000)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))])\n",
    "\n",
    "X_tfidf = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "def perform_SVD(X, n_components=300): \n",
    "    \n",
    "    X_array = X.asfptype()\n",
    "    U, Sigma, VT = svds(X_array.T, # term-document matrix\n",
    "                        k=n_components)\n",
    "    # reverse outputs\n",
    "    Sigma = Sigma[::-1]\n",
    "    U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "    \n",
    "    # return V \n",
    "    V = VT.T\n",
    "    scaler = MaxAbsScaler()\n",
    "    V_scaled = scaler.fit_transform(V)\n",
    "    return V_scaled # scaled for Logistic Regression\n",
    "\n",
    "X_tfidf_svd = perform_SVD(X_tfidf, n_components=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X_tfidf_svd_allcos = cosine_similarity(X_tfidf_svd)\n",
    "\n",
    "train_df = pd.DataFrame({'sms':X_train, 'target':y_train})\n",
    "\n",
    "# get spam indexes\n",
    "spam_ix = train_df.loc[train_df['target']=='spam'].index\n",
    "\n",
    "# calculate average spam similarity on SVD\n",
    "mean_spam_sims = []\n",
    "\n",
    "for ix in range(X_tfidf_svd_allcos.shape[0]):\n",
    "    mean_spam_sims.append(np.mean(X_tfidf_svd_allcos[ix, spam_ix]))\n",
    "\n",
    "X_tfidf_svd_spamcos = sp.hstack((csr_matrix(mean_spam_sims).T, X_tfidf_svd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_tfidf_svd_spamcos,\n",
    "                                                  y, \n",
    "                                                  stratify=y,\n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate estimators\n",
    "log_clf = LogisticRegression(\n",
    "    solver=\"liblinear\"\n",
    "    , random_state=42\n",
    ")\n",
    "\n",
    "rnd_clf = RandomForestClassifier(\n",
    "    n_jobs=-1\n",
    "    , random_state=42\n",
    "    , max_depth=8\n",
    "    , max_features=150\n",
    "    , min_samples_split=3\n",
    "    , n_estimators=100\n",
    ")\n",
    "\n",
    "svm_clf = SVC(\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_clf_prob = SVC(\n",
    "    random_state=42\n",
    "    , probability=True\n",
    ")\n",
    "\n",
    "vot_clf_hard = VotingClassifier(\n",
    "    estimators=[('log', log_clf), ('rnd', rnd_clf), ('svm', svm_clf)]\n",
    "    , voting='hard'\n",
    ")\n",
    "\n",
    "vot_clf_soft = VotingClassifier(\n",
    "    estimators=[('log', log_clf), ('rnd', rnd_clf), ('svm', svm_clf_prob)]\n",
    "    , voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_eval(classifiers):\n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        print(clf.__class__, __name__, 'acc', round(accuracy_score(y_val, y_pred), 4))\n",
    "        print(clf.__class__, __name__, 'tpr', round(recall_score(y_val, y_pred, pos_label=1) , 4))\n",
    "        print(clf.__class__, __name__, 'tnr', round(recall_score(y_val, y_pred, pos_label=0), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._logistic.LogisticRegression'> __main__ acc 0.9723\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'> __main__ tpr 0.8062\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'> __main__ tnr 0.9976\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> __main__ acc 0.9887\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> __main__ tpr 0.938\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> __main__ tnr 0.9965\n",
      "<class 'sklearn.svm._classes.SVC'> __main__ acc 0.959\n",
      "<class 'sklearn.svm._classes.SVC'> __main__ tpr 0.6899\n",
      "<class 'sklearn.svm._classes.SVC'> __main__ tnr 1.0\n",
      "<class 'sklearn.ensemble._voting.VotingClassifier'> __main__ acc 0.9744\n",
      "<class 'sklearn.ensemble._voting.VotingClassifier'> __main__ tpr 0.8062\n",
      "<class 'sklearn.ensemble._voting.VotingClassifier'> __main__ tnr 1.0\n"
     ]
    }
   ],
   "source": [
    "quick_eval([log_clf, rnd_clf, svm_clf, vot_clf_hard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._logistic.LogisticRegression'> __main__ acc 0.9723\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'> __main__ tpr 0.8062\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'> __main__ tnr 0.9976\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> __main__ acc 0.9887\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> __main__ tpr 0.938\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> __main__ tnr 0.9965\n",
      "<class 'sklearn.svm._classes.SVC'> __main__ acc 0.959\n",
      "<class 'sklearn.svm._classes.SVC'> __main__ tpr 0.6899\n",
      "<class 'sklearn.svm._classes.SVC'> __main__ tnr 1.0\n",
      "<class 'sklearn.ensemble._voting.VotingClassifier'> __main__ acc 0.9836\n",
      "<class 'sklearn.ensemble._voting.VotingClassifier'> __main__ tpr 0.8837\n",
      "<class 'sklearn.ensemble._voting.VotingClassifier'> __main__ tnr 0.9988\n"
     ]
    }
   ],
   "source": [
    "quick_eval([log_clf, rnd_clf, svm_clf_prob, vot_clf_soft])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after scaling to help out the logistic classifier, this ensemble still does more poorly than the highly optimized random forest model, which isn't performing its best because of the scaled SVD. A second attempt without the simplistic SVM classifier doesn't improve results.\n",
    "\n",
    "It appears as if the *wisdom of the crowds* only works when everyone in the crowd is more or less clueless and makes mistakes. When we have an \"expert\" in the crowd, we should probably follow that expert, and so an ensemble will not necessarily perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True, random_state=42)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf_prob.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = svm_clf_prob.predict_proba(X_val)\n",
    "y_pred = svm_clf_prob.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.989, 0.011],\n",
       "        [0.989, 0.011],\n",
       "        [0.998, 0.002],\n",
       "        [0.   , 1.   ],\n",
       "        [0.991, 0.009],\n",
       "        [0.992, 0.008],\n",
       "        [0.   , 1.   ],\n",
       "        [0.993, 0.007],\n",
       "        [0.989, 0.011],\n",
       "        [0.   , 1.   ]]),\n",
       " array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True) \n",
    "y_prob[0:10], y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = []\n",
    "for n,p in y_prob:\n",
    "    if n > .5:\n",
    "        y_pred2.append(0)\n",
    "    else:\n",
    "        y_pred2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 117)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred), sum(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.68992\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy_score(y_pred, y_pred), 5))\n",
    "print(round(recall_score(y_val, y_pred, pos_label=1) , 5))\n",
    "print(round(recall_score(y_val, y_pred, pos_label=0), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97128\n",
      "0.86822\n",
      "0.99409\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy_score(y_pred2, y_pred), 5))\n",
    "print(round(recall_score(y_val, y_pred2, pos_label=1) , 5))\n",
    "print(round(recall_score(y_val, y_pred2, pos_label=0), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Setting C__: `C=1.0`. If you have a lot of noisy observations you should decrease it: decreasing C corresponds to more regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
