{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "---\n",
    "\n",
    "*Features*\n",
    "\n",
    "- Use SVD for dimensionality reduction. \n",
    "\n",
    "- Point of departure: [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/). \n",
    "\n",
    "- Consulted Prof. Steve Brunton's [YouTube lecture series](https://www.youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv) and [Data-Driven Science and Engineering book](https://www.amazon.com/Data-Driven-Science-Engineering-Learning-Dynamical/dp/1108422098) - see notes from first few lectures [here](Extra_SteveBrunton_SVD_lecture.pdf).\n",
    "\n",
    "*Results*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2020-12-21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time()\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    raw_path = os.path.join(\"..\",\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "X_train = load_data(\"X_train\")\n",
    "y_train = load_data(\"y_train\")\n",
    "\n",
    "# transform y_array into int type\n",
    "y_train[y_train=='ham'] = 0\n",
    "y_train[y_train=='spam'] = 1\n",
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW and Tfidf\n",
    "\n",
    "Here I clean and preprocess the data in two formats, a Bag-of-upto-Trigrams with 2,000 terms, and a Tfidf representation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import custom.clean_preprocess as cp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe = Pipeline([('counter', cp.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bow', cp.WordCounterToVectorTransformer(vocabulary_size=2000)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))                  \n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "X_trans_counter = pipe['counter'].fit_transform(X_train)\n",
    "X_trans_bot = pipe['bow'].fit_transform(X_trans_counter) \n",
    "X_trans_bot = X_trans_bot.asfptype() # for SVD\n",
    "\n",
    "# Tfidf\n",
    "X_trans_tfidf = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "Borroming from sklearn's **TruncatedSVD** class, \"arpack\" algorithm (the \"randomized\" algorithm takes longer and arrives at the same result), here are the relevant code bits:\n",
    "\n",
    "[(source)](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/decomposition/_truncated_svd.py#L24)\n",
    "```\n",
    "149    def fit_transform(self, X, y=None):\n",
    "[...]\n",
    "168        if self.algorithm == \"arpack\":\n",
    "169             U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n",
    "170             # svds doesn't abide by scipy.linalg.svd/randomized_svd\n",
    "171             # conventions, so reverse its outputs.\n",
    "172            Sigma = Sigma[::-1]\n",
    "173            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "```                  \n",
    "\n",
    "### SVD on Bag-of-Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "U, Sigma, VT = svds(X_trans_bot.T, # transposed to a term-document matrix\n",
    "                    k=300) # k = number of components / \"topics\"\n",
    "# reverse outputs\n",
    "Sigma = Sigma[::-1]\n",
    "U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "# scale V (transpose of VT)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_svd_scaled = scaler.fit_transform(VT.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2001, 300), (300,), (300, 3900))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, Sigma.shape, VT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3900, 300), (3900,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT.T.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U contains the eigenvectors of the term correlations: $XX^T$\n",
    "- V contains the eigenvectors of the document correlations: $X^TX$\n",
    "- $\\Sigma$ contains the singular values of the factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9662 (+/- 0.0073)\n",
      "sensitivity: 0.7602 (+/- 0.0560)\n",
      "specificity: 0.9976 (+/- 0.0029)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "my_scorer = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'sensitivity': make_scorer(recall_score, pos_label=1),\n",
    "    'specificity': make_scorer(recall_score, pos_label=0)\n",
    "}\n",
    "\n",
    "acc = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['accuracy'], n_jobs=-1)\n",
    "tpr = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['sensitivity'], n_jobs=-1)\n",
    "tnr = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['specificity'], n_jobs=-1)\n",
    "\n",
    "print(f'accuracy: {acc.mean():0.4f} (+/- {np.std(acc):0.4f})')\n",
    "print(f'sensitivity: {tpr.mean():0.4f} (+/- {np.std(tpr):0.4f})')\n",
    "print(f'specificity: {tnr.mean():0.4f} (+/- {np.std(tnr):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_scores(clf, X, y, random_state, test_size):\n",
    "    \"\"\"Get accuracy, sensitivity, and specificity\n",
    "       for a given classifier.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state)\n",
    "    # fit and predict\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    # condition positive/negative\n",
    "    cond_pos = tp + fn\n",
    "    cond_neg = fp + tn\n",
    "    \n",
    "    # accuracy, sensitivity, specificity\n",
    "    acc = (tp + tn) / (cond_pos + cond_neg)\n",
    "    tpr = tp / cond_pos\n",
    "    tnr = tn / cond_neg\n",
    "\n",
    "    return acc, tpr, tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9513 (+/- 0.0088)\n",
      "sensitivity: 0.6552 (+/- 0.0529)\n",
      "specificity: 0.9978 (+/- 0.0014)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "random.seed(2343)\n",
    "random_states = [random.randint(1, 9999) for i in range(0, 10)]\n",
    "\n",
    "accs, tprs, tnrs = [], [], []\n",
    "for state in random_states:\n",
    "    acc, tpr, tnr = get_scores(log_clf, X_train_svd_scaled, y_train, \n",
    "                               random_state=state, test_size=.33)\n",
    "    accs.append(acc)\n",
    "    tprs.append(tpr)\n",
    "    tnrs.append(tnr)\n",
    "    \n",
    "print(f'accuracy: {np.mean(accs):0.4f} (+/- {np.std(accs):0.4f})')\n",
    "print(f'sensitivity: {np.mean(tprs):0.4f} (+/- {np.std(tprs):0.4f})')\n",
    "print(f'specificity: {np.mean(tnrs):0.4f} (+/- {np.std(tnrs):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Bag-of-Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9859 (+/- 0.0053)\n",
      "sensitivity: 0.9069 (+/- 0.0412)\n",
      "specificity: 0.9979 (+/- 0.0019)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "acc = cross_val_score(log_clf, X_trans_bot, y_train, cv=10, verbose=0, scoring=my_scorer['accuracy'], n_jobs=-1)\n",
    "tpr = cross_val_score(log_clf, X_trans_bot, y_train, cv=10, verbose=0, scoring=my_scorer['sensitivity'], n_jobs=-1)\n",
    "tnr = cross_val_score(log_clf, X_trans_bot, y_train, cv=10, verbose=0, scoring=my_scorer['specificity'], n_jobs=-1)\n",
    "\n",
    "print(f'accuracy: {acc.mean():0.4f} (+/- {np.std(acc):0.4f})')\n",
    "print(f'sensitivity: {tpr.mean():0.4f} (+/- {np.std(tpr):0.4f})')\n",
    "print(f'specificity: {tnr.mean():0.4f} (+/- {np.std(tnr):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9829 (+/- 0.0021)\n",
      "sensitivity: 0.8923 (+/- 0.0186)\n",
      "specificity: 0.9971 (+/- 0.0019)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "random.seed(2343)\n",
    "random_states = [random.randint(1, 9999) for i in range(0, 10)]\n",
    "\n",
    "accs, tprs, tnrs = [], [], []\n",
    "for state in random_states:\n",
    "    acc, tpr, tnr = get_scores(log_clf, X_trans_bot, y_train, \n",
    "                               random_state=state, test_size=.33)\n",
    "    accs.append(acc)\n",
    "    tprs.append(tpr)\n",
    "    tnrs.append(tnr)\n",
    "    \n",
    "print(f'accuracy: {np.mean(accs):0.4f} (+/- {np.std(accs):0.4f})')\n",
    "print(f'sensitivity: {np.mean(tprs):0.4f} (+/- {np.std(tprs):0.4f})')\n",
    "print(f'specificity: {np.mean(tnrs):0.4f} (+/- {np.std(tnrs):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2343)\n",
    "random_states = [random.randint(1, 9999) for i in range(0, 10)]\n",
    "\n",
    "accs, tprs, tnrs = [], [], []\n",
    "for state in random_states:\n",
    "    acc, tpr, tnr = get_scores(log_clf, X_trans_, y_train, \n",
    "                               random_state=state, test_size=.33)\n",
    "    accs.append(acc)\n",
    "    tprs.append(tpr)\n",
    "    tnrs.append(tnr)\n",
    "    \n",
    "print(f'accuracy: {np.mean(accs):0.4f} (+/- {np.std(accs):0.4f})')\n",
    "print(f'sensitivity: {np.mean(tprs):0.4f} (+/- {np.std(tprs):0.4f})')\n",
    "print(f'specificity: {np.mean(tnrs):0.4f} (+/- {np.std(tnrs):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD on Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, VT = svds(X_trans_tfidf.T, # transposed to a term-document matrix\n",
    "                    k=300) # k = number of components / \"topics\"\n",
    "# reverse outputs\n",
    "Sigma = Sigma[::-1]\n",
    "U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "# scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train_svd_scaled = scaler.fit_transform(VT.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9836 (+/- 0.0048)\n",
      "sensitivity: 0.8857 (+/- 0.0358)\n",
      "specificity: 0.9985 (+/- 0.0020)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "acc = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['accuracy'], n_jobs=-1)\n",
    "tpr = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['sensitivity'], n_jobs=-1)\n",
    "tnr = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['specificity'], n_jobs=-1)\n",
    "\n",
    "print(f'accuracy: {acc.mean():0.4f} (+/- {np.std(acc):0.4f})')\n",
    "print(f'sensitivity: {tpr.mean():0.4f} (+/- {np.std(tpr):0.4f})')\n",
    "print(f'specificity: {tnr.mean():0.4f} (+/- {np.std(tnr):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9849 (+/- 0.0042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_trans_bow, y_train, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes in the [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)**\n",
    "\n",
    "Avoid Naive Bayes on SVD since it implies strong independence between variables.\n",
    "\n",
    "\"*Apart from LSA, there are other advanced and efficient topic modeling techniques such as Latent Dirichlet Allocation (LDA) and lda2Vec. We have a wonderful article on LDA which you can check out [here](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/). lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0 m 28 s\n"
     ]
    }
   ],
   "source": [
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Time elapsed: {mins:0.0f} m {secs:0.0f} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
