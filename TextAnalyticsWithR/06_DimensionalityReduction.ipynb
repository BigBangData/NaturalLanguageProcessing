{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Vector Decomposition\n",
    "\n",
    "---\n",
    "\n",
    "*Features*\n",
    "\n",
    "- Use SVD for dimensionality reduction. \n",
    "\n",
    "- Point of departure: [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/). \n",
    "\n",
    "- Consulted Prof. Steve Brunton's [YouTube lecture series](https://www.youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv) and [Data-Driven Science and Engineering book](https://www.amazon.com/Data-Driven-Science-Engineering-Learning-Dynamical/dp/1108422098) - see notes from first few lectures [here](Extra_SteveBrunton_SVD_lecture.pdf).\n",
    "\n",
    "*Results*\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2020-12-22\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time()\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    raw_path = os.path.join(\"..\",\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "X_train = load_data(\"X_train\")\n",
    "y_train = load_data(\"y_train\")\n",
    "\n",
    "# transform y_array into int type\n",
    "y_train[y_train=='ham'] = 0\n",
    "y_train[y_train=='spam'] = 1\n",
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW and Tfidf\n",
    "\n",
    "Here I clean and preprocess the data in two formats, a Bag-of-upto-Trigrams with 2,000 terms, and a Tfidf representation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import custom.clean_preprocess as cp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe = Pipeline([('counter', cp.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bow', cp.WordCounterToVectorTransformer(vocabulary_size=2000)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))                  \n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "X_trans_counter = pipe['counter'].fit_transform(X_train)\n",
    "X_trans_bot = pipe['bow'].fit_transform(X_trans_counter) \n",
    "X_trans_bot = X_trans_bot.asfptype() # for SVD\n",
    "\n",
    "# Tfidf\n",
    "X_trans_tfidf = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "Borroming from sklearn's **TruncatedSVD** class, \"arpack\" algorithm (the \"randomized\" algorithm takes longer and arrives at the same result), here are the relevant code bits:\n",
    "\n",
    "[(source)](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/decomposition/_truncated_svd.py#L24)\n",
    "```\n",
    "149    def fit_transform(self, X, y=None):\n",
    "[...]\n",
    "168        if self.algorithm == \"arpack\":\n",
    "169             U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n",
    "170             # svds doesn't abide by scipy.linalg.svd/randomized_svd\n",
    "171             # conventions, so reverse its outputs.\n",
    "172            Sigma = Sigma[::-1]\n",
    "173            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "```                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U contains the eigenvectors of the term correlations: $XX^T$\n",
    "- V contains the eigenvectors of the document correlations: $X^TX$\n",
    "- $\\Sigma$ contains the singular values of the factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def perform_SVD(X, k=300):\n",
    "    U, Sigma, VT = svds(X.T, # transposed to a term-document matrix\n",
    "                    k=k) # k = number of components\n",
    "    # reverse outputs\n",
    "    Sigma = Sigma[::-1]\n",
    "    U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "    # scale\n",
    "    V = VT.T\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(V)\n",
    "\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform SVD for Bag-of-Trigrams and Tfidf\n",
    "X_svd_bot = perform_SVD(X_trans_bot)\n",
    "X_svd_tfidf = perform_SVD(X_trans_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "### Leveraging sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score\n",
    "\n",
    "def scikitlearn_cv(clf, X, y, seed_, cv=10, test_size=.25):\n",
    "    scorer_ = {\n",
    "        'acc': make_scorer(accuracy_score),\n",
    "        'tpr': make_scorer(recall_score, pos_label=1),\n",
    "        'tnr': make_scorer(recall_score, pos_label=0)\n",
    "    }\n",
    "    acc = cross_val_score(clf, X, y, cv=cv, verbose=0, scoring=scorer_['acc'], n_jobs=-1)\n",
    "    tpr = cross_val_score(clf, X, y, cv=cv, verbose=0, scoring=scorer_['tpr'], n_jobs=-1)\n",
    "    tnr = cross_val_score(clf, X, y, cv=cv, verbose=0, scoring=scorer_['tnr'], n_jobs=-1)\n",
    "    \n",
    "    return acc.mean(), tpr.mean(), tnr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-rolled CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def hand_rolled_cv(clf, X, y, seed_, cv=10, test_size=.25):\n",
    "                  \n",
    "    def get_scores(clf, X, y, random_state, test_size):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, \n",
    "                                                            random_state=random_state)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        acc = (tp + tn) / (tp + fn + fp + tn)\n",
    "        tpr = tp / (tp + fn)\n",
    "        tnr = tn / (fp + tn)\n",
    "        return acc, tpr, tnr\n",
    "\n",
    "    random.seed(seed_)\n",
    "    random_states = [random.randint(1, 9999) for i in range(0, cv)]\n",
    "\n",
    "    accs, tprs, tnrs = [], [], []\n",
    "    for state in random_states:\n",
    "        acc, tpr, tnr = get_scores(clf, X, y, \n",
    "                                   random_state=state, test_size=test_size)\n",
    "        accs.append(acc)\n",
    "        tprs.append(tpr)\n",
    "        tnrs.append(tnr)\n",
    "    \n",
    "    return np.mean(accs), np.mean(tprs), np.mean(tnrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper for multiple CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_cvs(clf, Xs, Xnames, y, seed_, cv=10, test_size=.25):\n",
    "\n",
    "    h_accs, h_tprs, h_tnrs = [], [], []\n",
    "    s_accs, s_tprs, s_tnrs = [], [], []\n",
    "    for X in Xs:\n",
    "        h_acc, h_tpr, h_tnr = hand_rolled_cv(clf, X, y, seed_=seed_, cv=cv, test_size=test_size)\n",
    "        s_acc, s_tpr, s_tnr = scikitlearn_cv(clf, X, y, seed_=seed_, cv=cv, test_size=test_size)\n",
    "        h_accs.append(round(h_acc, 4))\n",
    "        h_tprs.append(round(h_tpr, 4))\n",
    "        h_tnrs.append(round(h_tnr, 4))\n",
    "        s_accs.append(round(s_acc, 4))\n",
    "        s_tprs.append(round(s_tpr, 4))\n",
    "        s_tnrs.append(round(s_tnr, 4))\n",
    "    \n",
    "    data = {'Representation': Xnames,\n",
    "            'HR_mean_accuracy': h_accs,\n",
    "            'HR_mean_sensitivity': h_tprs, \n",
    "            'HR_mean_specificity': h_tnrs, \n",
    "            'SK_mean_accuracy': s_accs, \n",
    "            'SK_mean_sensitivity': s_tprs, \n",
    "            'SK_mean_specificity': s_tnrs}\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "Xs = [X_svd_bot, X_trans_bot, X_svd_tfidf, X_trans_tfidf]\n",
    "Xnames = ['SVD on BoT', 'Original BoT', 'SVD on Tfidf', 'Original Tfidf']\n",
    "\n",
    "data = collect_cvs(log_clf, Xs, Xnames, y_train, seed_=1209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Representation</th>\n",
       "      <th>HR_mean_accuracy</th>\n",
       "      <th>HR_mean_sensitivity</th>\n",
       "      <th>HR_mean_specificity</th>\n",
       "      <th>SK_mean_accuracy</th>\n",
       "      <th>SK_mean_sensitivity</th>\n",
       "      <th>SK_mean_specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVD on BoT</td>\n",
       "      <td>0.9609</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9662</td>\n",
       "      <td>0.7602</td>\n",
       "      <td>0.9976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Original BoT</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.8969</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9069</td>\n",
       "      <td>0.9979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVD on Tfidf</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.8798</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.8857</td>\n",
       "      <td>0.9985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Original Tfidf</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.8450</td>\n",
       "      <td>0.9982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Representation  HR_mean_accuracy  HR_mean_sensitivity  HR_mean_specificity  \\\n",
       "0      SVD on BoT            0.9609               0.7128               0.9975   \n",
       "1    Original BoT            0.9837               0.8969               0.9965   \n",
       "2    SVD on Tfidf            0.9840               0.8798               0.9993   \n",
       "3  Original Tfidf            0.9772               0.8321               0.9986   \n",
       "\n",
       "   SK_mean_accuracy  SK_mean_sensitivity  SK_mean_specificity  \n",
       "0            0.9662               0.7602               0.9976  \n",
       "1            0.9859               0.9069               0.9979  \n",
       "2            0.9836               0.8857               0.9985  \n",
       "3            0.9779               0.8450               0.9982  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes in the [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)**\n",
    "\n",
    "Avoid Naive Bayes on SVD since it implies strong independence between variables.\n",
    "\n",
    "\"*Apart from LSA, there are other advanced and efficient topic modeling techniques such as Latent Dirichlet Allocation (LDA) and lda2Vec. We have a wonderful article on LDA which you can check out [here](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/). lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0 m 17 s\n"
     ]
    }
   ],
   "source": [
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Time elapsed: {mins:0.0f} m {secs:0.0f} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
