{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "---\n",
    "\n",
    "*Features*\n",
    "\n",
    "- Use SVD for dimensionality reduction. \n",
    "\n",
    "- Point of departure: [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/). \n",
    "\n",
    "- Consulted Prof. Steve Brunton's [YouTube lecture series](https://www.youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv) and [Data-Driven Science and Engineering book](https://www.amazon.com/Data-Driven-Science-Engineering-Learning-Dynamical/dp/1108422098) - see notes from first few lectures [here](Extra_SteveBrunton_SVD_lecture.pdf).\n",
    "\n",
    "*Results*\n",
    "\n",
    "- SVD is just predicting the base rate (0.8674, or 1 - spam prop): something is wrong...\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2020-12-20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time()\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    raw_path = os.path.join(\"..\",\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "X_train = load_data(\"X_train\")\n",
    "y_train = load_data(\"y_train\")\n",
    "\n",
    "# transform y_array into int type\n",
    "y_train[y_train=='ham'] = 0\n",
    "y_train[y_train=='spam'] = 1\n",
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW and Tfidf\n",
    "\n",
    "Here I clean and preprocess the data in two formats, a Bag-of-(upto)-Bigrams with 2,000 terms, and a Tfidf representation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import custom.clean_preprocess as cp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe = Pipeline([('counter', cp.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bow', cp.WordCounterToVectorTransformer(vocabulary_size=2000)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))                  \n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "X_trans_counter = pipe['counter'].fit_transform(X_train)\n",
    "X_trans_bow = pipe['bow'].fit_transform(X_trans_counter) \n",
    "\n",
    "# Tfidf\n",
    "X_trans_tfidf = pipe.fit_transform(X_train)\n",
    "\n",
    "# convert to fptype for SVD\n",
    "X_trans_bow = X_trans_bow.asfptype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "Borroming from sklearn's **TruncatedSVD** class, \"arpack\" algorithm (the \"randomized\" algorithm takes longer and arrives at the same result), here are the relevant code bits:\n",
    "\n",
    "[(source)](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/decomposition/_truncated_svd.py#L24)\n",
    "```\n",
    "149    def fit_transform(self, X, y=None):\n",
    "[...]\n",
    "168        if self.algorithm == \"arpack\":\n",
    "169             U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n",
    "170             # svds doesn't abide by scipy.linalg.svd/randomized_svd\n",
    "171             # conventions, so reverse its outputs.\n",
    "172            Sigma = Sigma[::-1]\n",
    "173            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "```                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "U, Sigma, VT = svds(X_trans_tfidf.T, # transposed to a term-document matrix\n",
    "                    k=300) # k = number of components / \"topics\"\n",
    "# reverse outputs\n",
    "Sigma = Sigma[::-1]\n",
    "U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "# scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train_svd_scaled = scaler.fit_transform(VT.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2001, 300), (300,), (300, 3900))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, Sigma.shape, VT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3900, 300), (3900,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = VT.T\n",
    "V.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U contains the eigenvectors of the term correlations: $XX^T$\n",
    "- V contains the eigenvectors of the document correlations: $X^TX$\n",
    "- $\\Sigma$ contains the singular values of the factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9662 (+/- 0.0073)\n",
      "sensitivity: 0.7602 (+/- 0.0560)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "acc = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "tpr = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring='recall', n_jobs=-1)\n",
    "\n",
    "print(f'accuracy: {acc.mean():0.4f} (+/- {np.std(acc):0.4f})')\n",
    "print(f'sensitivity: {tpr.mean():0.4f} (+/- {np.std(tpr):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_scores(X, y, random_state):\n",
    "    log_clf = LogisticRegression(solver=\"liblinear\", \n",
    "                                 random_state=random_state)\n",
    "    \n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.33, \n",
    "                                          random_state=random_state)\n",
    "    log_clf.fit(Xtr, ytr)\n",
    "    ypred = log_clf.predict(Xte)\n",
    "\n",
    "    [[TP, FP], \n",
    "     [FN, TN]] = confusion_matrix(yte, ypred)\n",
    "    cond_pos = TP + FN\n",
    "    cond_neg = FP + TN\n",
    "    \n",
    "    accuracy = (TP + TN) / (cond_pos + cond_neg)\n",
    "    sensitivity = TP / cond_pos\n",
    "    specificity = TN / cond_neg\n",
    "    \n",
    "    print(f'accuracy: {accuracy:0.4f}')\n",
    "    print(f'sensitivity: {sensitivity:0.4f}')\n",
    "    print(f'specificity: {specificity:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9479\n",
      "sensitivity: 0.9461\n",
      "specificity: 0.9661\n"
     ]
    }
   ],
   "source": [
    "print_scores(X_train_svd_scaled, y_train, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9510\n",
      "sensitivity: 0.9475\n",
      "specificity: 0.9839\n"
     ]
    }
   ],
   "source": [
    "print_scores(X_train_svd_scaled, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9503\n",
      "sensitivity: 0.9463\n",
      "specificity: 0.9912\n"
     ]
    }
   ],
   "source": [
    "print_scores(X_train_svd_scaled, y_train, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9777 (+/- 0.0037)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_trans_tfidf, y_train, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, VT = svds(X_trans_bow.T, # transposed to a term-document matrix\n",
    "                    k=300) # k = number of components / \"topics\"\n",
    "# reverse outputs\n",
    "Sigma = Sigma[::-1]\n",
    "U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "# scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train_svd_scaled = scaler.fit_transform(VT.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9623 (+/- 0.0061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9849 (+/- 0.0042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_trans_bow, y_train, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes in the [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)**\n",
    "\n",
    "Avoid Naive Bayes on SVD since it implies strong independence between variables.\n",
    "\n",
    "\"*Apart from LSA, there are other advanced and efficient topic modeling techniques such as Latent Dirichlet Allocation (LDA) and lda2Vec. We have a wonderful article on LDA which you can check out [here](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/). lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0 m 28 s\n"
     ]
    }
   ],
   "source": [
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Time elapsed: {mins:0.0f} m {secs:0.0f} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
