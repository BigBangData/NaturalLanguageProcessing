{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Vector Decomposition\n",
    "\n",
    "---\n",
    "\n",
    "*Features*\n",
    "\n",
    "- Use SVD for dimensionality reduction. \n",
    "\n",
    "- Point of departure: [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/). \n",
    "\n",
    "- Consulted Prof. Steve Brunton's [YouTube lecture series](https://www.youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv) and [Data-Driven Science and Engineering book](https://www.amazon.com/Data-Driven-Science-Engineering-Learning-Dynamical/dp/1108422098) - see notes from first few lectures [here](Extra_SteveBrunton_SVD_lecture.pdf).\n",
    "\n",
    "*Results*\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2020-12-21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time()\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    raw_path = os.path.join(\"..\",\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "X_train = load_data(\"X_train\")\n",
    "y_train = load_data(\"y_train\")\n",
    "\n",
    "# transform y_array into int type\n",
    "y_train[y_train=='ham'] = 0\n",
    "y_train[y_train=='spam'] = 1\n",
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW and Tfidf\n",
    "\n",
    "Here I clean and preprocess the data in two formats, a Bag-of-upto-Trigrams with 2,000 terms, and a Tfidf representation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import custom.clean_preprocess as cp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe = Pipeline([('counter', cp.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bow', cp.WordCounterToVectorTransformer(vocabulary_size=2000)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))                  \n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "X_trans_counter = pipe['counter'].fit_transform(X_train)\n",
    "X_trans_bot = pipe['bow'].fit_transform(X_trans_counter) \n",
    "X_trans_bot = X_trans_bot.asfptype() # for SVD\n",
    "\n",
    "# Tfidf\n",
    "X_trans_tfidf = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "Borroming from sklearn's **TruncatedSVD** class, \"arpack\" algorithm (the \"randomized\" algorithm takes longer and arrives at the same result), here are the relevant code bits:\n",
    "\n",
    "[(source)](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/decomposition/_truncated_svd.py#L24)\n",
    "```\n",
    "149    def fit_transform(self, X, y=None):\n",
    "[...]\n",
    "168        if self.algorithm == \"arpack\":\n",
    "169             U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n",
    "170             # svds doesn't abide by scipy.linalg.svd/randomized_svd\n",
    "171             # conventions, so reverse its outputs.\n",
    "172            Sigma = Sigma[::-1]\n",
    "173            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "```                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U contains the eigenvectors of the term correlations: $XX^T$\n",
    "- V contains the eigenvectors of the document correlations: $X^TX$\n",
    "- $\\Sigma$ contains the singular values of the factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD on Bag-of-Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "U, Sigma, VT = svds(X_trans_bot.T, # transposed to a term-document matrix\n",
    "                    k=300) # k = number of components / \"topics\"\n",
    "# reverse outputs\n",
    "Sigma = Sigma[::-1]\n",
    "U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "# scale V (transpose of VT)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_svd_scaled = scaler.fit_transform(VT.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2001, 300), (300,), (300, 3900))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, Sigma.shape, VT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3900, 300), (3900,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT.T.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "my_scorer = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'sensitivity': make_scorer(recall_score, pos_label=1),\n",
    "    'specificity': make_scorer(recall_score, pos_label=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9662 (+/- 0.0073)\n",
      "sensitivity: 0.7602 (+/- 0.0560)\n",
      "specificity: 0.9976 (+/- 0.0029)\n"
     ]
    }
   ],
   "source": [
    "acc = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['accuracy'], n_jobs=-1)\n",
    "tpr = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['sensitivity'], n_jobs=-1)\n",
    "tnr = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['specificity'], n_jobs=-1)\n",
    "\n",
    "print(f'accuracy: {acc.mean():0.4f} (+/- {np.std(acc):0.4f})')\n",
    "print(f'sensitivity: {tpr.mean():0.4f} (+/- {np.std(tpr):0.4f})')\n",
    "print(f'specificity: {tnr.mean():0.4f} (+/- {np.std(tnr):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-rolled CV for SVD on Bag-of-Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def handrolled_cv(clf, X, y, seed_, cv=10, test_size=.25):\n",
    "                  \n",
    "    def get_scores(clf, X, y, random_state, test_size):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, \n",
    "                                                            random_state=random_state)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        acc = (tp + tn) / (tp + fn + fp + tn)\n",
    "        tpr = tp / (tp + fn)\n",
    "        tnr = tn / (fp + tn)\n",
    "        return acc, tpr, tnr\n",
    "\n",
    "    random.seed(seed_)\n",
    "    random_states = [random.randint(1, 9999) for i in range(0, cv)]\n",
    "\n",
    "    accs, tprs, tnrs = [], [], []\n",
    "    for state in random_states:\n",
    "        acc, tpr, tnr = get_scores(clf, X, y, \n",
    "                                   random_state=state, test_size=test_size)\n",
    "        accs.append(acc)\n",
    "        tprs.append(tpr)\n",
    "        tnrs.append(tnr)\n",
    "\n",
    "    print(f'accuracy: {np.mean(accs):0.4f} (+/- {np.std(accs):0.4f})')\n",
    "    print(f'sensitivity: {np.mean(tprs):0.4f} (+/- {np.std(tprs):0.4f})')\n",
    "    print(f'specificity: {np.mean(tnrs):0.4f} (+/- {np.std(tnrs):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9599 (+/- 0.0075)\n",
      "sensitivity: 0.7111 (+/- 0.0372)\n",
      "specificity: 0.9978 (+/- 0.0016)\n"
     ]
    }
   ],
   "source": [
    "handrolled_cv(log_clf, X_train_svd_scaled, y_train, seed_=2340981)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Bag-of-Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9859 (+/- 0.0053)\n",
      "sensitivity: 0.9069 (+/- 0.0412)\n",
      "specificity: 0.9979 (+/- 0.0019)\n"
     ]
    }
   ],
   "source": [
    "acc = cross_val_score(log_clf, X_trans_bot, y_train, cv=10, verbose=0, scoring=my_scorer['accuracy'], n_jobs=-1)\n",
    "tpr = cross_val_score(log_clf, X_trans_bot, y_train, cv=10, verbose=0, scoring=my_scorer['sensitivity'], n_jobs=-1)\n",
    "tnr = cross_val_score(log_clf, X_trans_bot, y_train, cv=10, verbose=0, scoring=my_scorer['specificity'], n_jobs=-1)\n",
    "\n",
    "print(f'accuracy: {acc.mean():0.4f} (+/- {np.std(acc):0.4f})')\n",
    "print(f'sensitivity: {tpr.mean():0.4f} (+/- {np.std(tpr):0.4f})')\n",
    "print(f'specificity: {tnr.mean():0.4f} (+/- {np.std(tnr):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-rolled CV for Original Bag-of-Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9845 (+/- 0.0045)\n",
      "sensitivity: 0.9087 (+/- 0.0212)\n",
      "specificity: 0.9960 (+/- 0.0032)\n"
     ]
    }
   ],
   "source": [
    "handrolled_cv(log_clf, X_trans_bot, y_train, seed_=2340981)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD on Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, VT = svds(X_trans_tfidf.T, # transposed to a term-document matrix\n",
    "                    k=300) # k = number of components / \"topics\"\n",
    "# reverse outputs\n",
    "Sigma = Sigma[::-1]\n",
    "U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "# scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train_svd_scaled = scaler.fit_transform(VT.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9836 (+/- 0.0048)\n",
      "sensitivity: 0.8857 (+/- 0.0358)\n",
      "specificity: 0.9985 (+/- 0.0020)\n"
     ]
    }
   ],
   "source": [
    "acc = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['accuracy'], n_jobs=-1)\n",
    "tpr = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['sensitivity'], n_jobs=-1)\n",
    "tnr = cross_val_score(log_clf, X_train_svd_scaled, y_train, cv=10, verbose=0, scoring=my_scorer['specificity'], n_jobs=-1)\n",
    "\n",
    "print(f'accuracy: {acc.mean():0.4f} (+/- {np.std(acc):0.4f})')\n",
    "print(f'sensitivity: {tpr.mean():0.4f} (+/- {np.std(tpr):0.4f})')\n",
    "print(f'specificity: {tnr.mean():0.4f} (+/- {np.std(tnr):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-rolled CV for SVD on Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9848 (+/- 0.0031)\n",
      "sensitivity: 0.8881 (+/- 0.0210)\n",
      "specificity: 0.9994 (+/- 0.0009)\n"
     ]
    }
   ],
   "source": [
    "handrolled_cv(log_clf, X_train_svd_scaled, y_train, seed_=2340981)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9779 (+/- 0.0088)\n",
      "sensitivity: 0.8450 (+/- 0.0685)\n",
      "specificity: 0.9982 (+/- 0.0024)\n"
     ]
    }
   ],
   "source": [
    "acc = cross_val_score(log_clf, X_trans_tfidf, y_train, cv=10, verbose=0, scoring=my_scorer['accuracy'], n_jobs=-1)\n",
    "tpr = cross_val_score(log_clf, X_trans_tfidf, y_train, cv=10, verbose=0, scoring=my_scorer['sensitivity'], n_jobs=-1)\n",
    "tnr = cross_val_score(log_clf, X_trans_tfidf, y_train, cv=10, verbose=0, scoring=my_scorer['specificity'], n_jobs=-1)\n",
    "\n",
    "print(f'accuracy: {acc.mean():0.4f} (+/- {np.std(acc):0.4f})')\n",
    "print(f'sensitivity: {tpr.mean():0.4f} (+/- {np.std(tpr):0.4f})')\n",
    "print(f'specificity: {tnr.mean():0.4f} (+/- {np.std(tnr):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-rolled CV for Original Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9782 (+/- 0.0042)\n",
      "sensitivity: 0.8441 (+/- 0.0280)\n",
      "specificity: 0.9985 (+/- 0.0015)\n"
     ]
    }
   ],
   "source": [
    "handrolled_cv(log_clf, X_trans_tfidf, y_train, seed_=2340981)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes in the [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)**\n",
    "\n",
    "Avoid Naive Bayes on SVD since it implies strong independence between variables.\n",
    "\n",
    "\"*Apart from LSA, there are other advanced and efficient topic modeling techniques such as Latent Dirichlet Allocation (LDA) and lda2Vec. We have a wonderful article on LDA which you can check out [here](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/). lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0 m 17 s\n"
     ]
    }
   ],
   "source": [
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Time elapsed: {mins:0.0f} m {secs:0.0f} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
