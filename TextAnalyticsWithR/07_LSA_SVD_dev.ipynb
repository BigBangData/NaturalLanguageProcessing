{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "This notebook is for developing, not implementing a solution.\n",
    "\n",
    "*Current State*\n",
    "- Preprocessing was performed for a Bag-of-upto-Ngrams with TF-IDF representation\n",
    "- This type of representation still suffers from the curse of dimensionality, best accuracies achieved with a Bag-of-upto-Trigram TF-IDF training subset of 120k documents (Tweets) and 100k terms, using a baseline Logistic Regression model, were approx. 80%\n",
    "- LSA, in particular, matrix decomposition using SVD, can be used to reduce a document-term matrix to an approximate document-component (or topic) matrix that captures more signal per component, if not overall\n",
    "\n",
    "*Results*\n",
    "- With 10% of our training data, properly transformed as described above, and evaluated with the same baseline LR model, the projection into a latent semantic space of 300 components seems to have reduced the overall signal, achieving only 75% accuracy (1,000 components bump it up to 77%)\n",
    "- It remains to be seen whether with addition of feature engineering, this reduced semantic space will perform better than adding those engineered features into the much larger sparse matrix (which yielded no improvement)\n",
    "- It also remains to be seen whether more complex models like Random Forests will perform better in this reduced semantic space\n",
    "- Lastly, perhaps by plotting learning curves we'll see whether SVD helps given more data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cleanup_module as Cmod\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 10% of the training data for POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample for dev\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# create array\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119747,), (119747,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape, y_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Bag-of-upto-Trigrams with 100,000 terms\n",
    "\n",
    "Using `sublinear_tf=True, use_idf=True` as recommended in [docs](https://scikit-learn.org/stable/modules/decomposition.html#lsa): \n",
    "\n",
    "*In particular, sublinear scaling and inverse document frequency should be turned on (`sublinear_tf=True, use_idf=True`) to bring the feature values closer to a Gaussian distribution, compensating for LSAâ€™s erroneous assumptions about textual data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', Cmod.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=100000)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True, use_idf=True))]) # performs similar to default (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 2 min 41 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X_train_transformed = pipe.fit_transform(X_array)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x100001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2687589 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed # document term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "Point of departure: [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/). \n",
    "\n",
    "Consulted Prof. Steve Brunton's [YouTube lecture series](https://www.youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv) and [Data-Driven Science and Engineering book](https://www.amazon.com/Data-Driven-Science-Engineering-Learning-Dynamical/dp/1108422098) - see notes from first few lectures [here](Extra_SteveBrunton_SVD_lecture.pdf).\n",
    "\n",
    "\n",
    "Borroming from sklearn's **TruncatedSVD** class, \"arpack\" algorithm (the \"randomized\" algorithm takes longer and arrives at the same result), here are the relevant code bits:\n",
    "\n",
    "[(source)](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/decomposition/_truncated_svd.py#L24)\n",
    "```\n",
    "149    def fit_transform(self, X, y=None):\n",
    "[...]\n",
    "168        if self.algorithm == \"arpack\":\n",
    "169             U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n",
    "170             # svds doesn't abide by scipy.linalg.svd/randomized_svd\n",
    "171             # conventions, so reverse its outputs.\n",
    "172            Sigma = Sigma[::-1]\n",
    "173            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "```                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 2 min 0 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "U, Sigma, VT = svds(X_train_transformed.T, # transposed to a term-document matrix\n",
    "                    k=1000) # k = number of components / \"topics\"\n",
    "    \n",
    "# reverse outputs\n",
    "Sigma = Sigma[::-1]\n",
    "U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100001, 300), (300,), (300, 119747))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, Sigma.shape, VT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119747, 300), (119747,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = VT.T\n",
    "V.shape, y_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U contains the eigenvectors of the term correlations: $XX^T$\n",
    "- V contains the eigenvectors of the document correlations: $X^TX$\n",
    "- $\\Sigma$ contains the singular values of the factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    6.3s remaining:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7520 (+/- 0.0017)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, V, y_array, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    3.3s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8013 (+/- 0.0015)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_array, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 112)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(X_train_transformed), sys.getsizeof(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "V_sparse = csr_matrix(V, shape=(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(V_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    5.2s remaining:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7520 (+/- 0.0017)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, V_sparse, y_array, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting singular values\n",
    "\n",
    "The idea is to help see whether the full 1000 components are needed or we can get away with using a smaller subset - the results below show that despite a slightly faster training time (compared to the full 1000 components not the original data), the hit in accuracy isn't worth dumping those components that have \"low energy\" or seem to explain less of the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(Sigma)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: avoid Naive Bayes on SVD since it implies strong independence between variables.\n",
    "\n",
    "Quoting the [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/) ~\n",
    "\n",
    "\"*Apart from LSA, there are other advanced and efficient topic modeling techniques such as Latent Dirichlet Allocation (LDA) and lda2Vec. We have a wonderful article on LDA which you can check out [here](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/). lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings.*\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
