{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Pipeline 3\n",
    "\n",
    "*Purpose*\n",
    "- adding bigrams to custom **DocumentToWordCounterTransformer** class. Note that this implementation keeps unigrams. TF-IDF is then performed on top of this Bag-of-upto-Bigrams representation.\n",
    "\n",
    "*Issues*\n",
    "- unable to remove stopwords from bigrams.\n",
    "- unable to select bigrams only option.\n",
    "\n",
    "*Results*\n",
    "- using the new **DocumentToBigramCounterTransformer** class on 10% of the training data (120k instances) and a vocabulary of 50k tokens (so 50k features) yields the following accuracies with 10-fold cross-validation:\n",
    "\n",
    "| Model | Representation | Accuracy | Variance |\n",
    "|:---|:---|:---|:---|\n",
    "|Naive Bayes |Bag-of-upto-Bigrams | 0.7890 |(+/- 0.0022)|\n",
    "|Naive Bayes |BoB + TF-IDF| 0.7908 |(+/- 0.0024)|\n",
    "|Logistic Regr. |Bag-of-upto-Bigrams  | 0.7895 |(+/- 0.0021)|\n",
    "|Logistic Regr. |BoB + TF-IDF| 0.8019 |(+/- 0.0024)|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urlextract\n",
    "from nltk import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import cleanup_module as Cmod\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def expand_contractions(text, contractions_map):\n",
    "    \n",
    "    pattern = re.compile('({})'.format('|'.join(contractions_map.keys())), \n",
    "                        flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_map.get(match)\\\n",
    "                                if contractions_map.get(match)\\\n",
    "                                else contractions_map.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentToBigramCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, expand_contractions=True, lower_case=True, \n",
    "                 replace_usernames=True, unescape_html=True, \n",
    "                 replace_urls=True, replace_numbers=True, \n",
    "                 remove_junk=True, remove_punctuation=True, \n",
    "                 replace_emojis=True, replace_nonascii=True, \n",
    "                 remove_stopwords=True, lemmatization=True,\n",
    "                 bigrams=True):\n",
    "        self.expand_contractions = expand_contractions\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "        self.bigrams = bigrams\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.expand_contractions and contractions_map is not None:\n",
    "                doc = expand_contractions(doc, contractions_map)\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'^@([^\\s]+)',' USERNAME ', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '\n",
    "            # tokenize\n",
    "            tokens = doc.split()\n",
    "            if self.remove_stopwords:\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with']\n",
    "                tokens = [t for t in tokens if t not in stop_words]\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            if self.bigrams:\n",
    "                bigrams = ngrams(word_tokenize(doc), 2)\n",
    "                bigrams = ['_'.join(grams) for grams in bigrams]\n",
    "                tokens = [*tokens, *bigrams]\n",
    "            # include counts\n",
    "            tokens_counts = Counter(tokens)\n",
    "            # append to list\n",
    "            X_transformed.append(tokens_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['You the love me', \n",
    "          'You do not love me',\n",
    "          'You really really love food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = DocumentToBigramCounterTransformer()\n",
    "X_trans = wordvec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'you': 1, 'love': 1, 'me': 1, 'you_the': 1, 'the_love': 1, 'love_me': 1}),\n",
       "       Counter({'you': 1, 'do': 1, 'not': 1, 'love': 1, 'me': 1, 'you_do': 1, 'do_not': 1, 'not_love': 1, 'love_me': 1}),\n",
       "       Counter({'really': 2, 'you': 1, 'love': 1, 'food': 1, 'you_really': 1, 'really_really': 1, 'really_love': 1, 'love_food': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', DocumentToBigramCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=20)),\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_counter = pipe['counter'].fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bow = pipe['bow'].fit(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 1,\n",
       " 'love': 2,\n",
       " 'me': 3,\n",
       " 'love_me': 4,\n",
       " 'really': 5,\n",
       " 'you_the': 6,\n",
       " 'the_love': 7,\n",
       " 'do': 8,\n",
       " 'not': 9,\n",
       " 'you_do': 10,\n",
       " 'do_not': 11,\n",
       " 'not_love': 12,\n",
       " 'food': 13,\n",
       " 'you_really': 14,\n",
       " 'really_really': 15,\n",
       " 'really_love': 16,\n",
       " 'love_food': 17}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pipe_bow.transform(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray() # first col is \"words missing from vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.288,\n",
       " 1.288,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 2.386,\n",
       " 2.386,\n",
       " 2.386]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # IDF for the pipe_bow.vocabulary_\n",
    "[np.around(x,3) for x in pipe['tfidf'].fit(bow).idf_[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipe['tfidf'].fit_transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.3008, 0.3008, 0.3874, 0.3874, 0.    , 0.5094, 0.5094,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.2256, 0.2256, 0.2905, 0.2905, 0.    , 0.    , 0.    ,\n",
       "        0.382 , 0.382 , 0.382 , 0.382 , 0.382 , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.1897, 0.1897, 0.    , 0.    , 0.6422, 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.3211, 0.3211, 0.3211,\n",
       "        0.3211, 0.3211, 0.    , 0.    , 0.    ])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,4) for x in tfidf.toarray()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.3008, 0.3008, 0.3874, 0.3874, 0.    , 0.5094, 0.5094,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.2256, 0.2256, 0.2905, 0.2905, 0.    , 0.    , 0.    ,\n",
       "        0.382 , 0.382 , 0.382 , 0.382 , 0.382 , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.1897, 0.1897, 0.    , 0.    , 0.6422, 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.3211, 0.3211, 0.3211,\n",
       "        0.3211, 0.3211, 0.    , 0.    , 0.    ])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entire pipeline produces same result but does't save IDF or vocab\n",
    "end_res = pipe.fit_transform(corpus)\n",
    "[np.around(x,4) for x in end_res.toarray()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC: sample 10% of the training data\n",
    "\n",
    "About 120,000 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample 10%\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# create array\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119747,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipe\n",
    "pipe = Pipeline([('counter', DocumentToBigramCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=1000)),\n",
    "                 ('tfidf', TfidfTransformer())])\n",
    "\n",
    "X_end = pipe.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x1001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1299526 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.549, 0.113, 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.736, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.674, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.706, 0.   , 0.085, 0.   , 0.   , 0.   ]),\n",
       " array([0.754, 0.099, 0.049, 0.078, 0.   , 0.   ]),\n",
       " array([0.319, 0.118, 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.571, 0.088, 0.086, 0.138, 0.   , 0.   ]),\n",
       " array([0.573, 0.   , 0.052, 0.   , 0.   , 0.   ]),\n",
       " array([0.86 , 0.053, 0.   , 0.   , 0.078, 0.   ]),\n",
       " array([0.6  , 0.069, 0.   , 0.109, 0.   , 0.   ])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,3) for x in X_end.toarray()[:10,:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step by step for vocab and idf\n",
    "X_counter = pipe['counter'].fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counter_fit = pipe['bow'].fit(X_counter) \n",
    "X_counter_fit.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 1)\n",
      "('USERNAME', 2)\n",
      "('NUMBER', 3)\n",
      "('my', 4)\n",
      "('you', 5)\n",
      "('not', 6)\n",
      "('am', 7)\n",
      "('have', 8)\n",
      "('i_am', 9)\n",
      "('me', 10)\n",
      "('to_NUMBER', 992)\n",
      "('watching_the', 993)\n",
      "('worst', 994)\n",
      "('make_me', 995)\n",
      "('oh_no', 996)\n",
      "('message', 997)\n",
      "('my_friends', 998)\n",
      "('running', 999)\n",
      "('you_too', 1000)\n"
     ]
    }
   ],
   "source": [
    "for ix, tuple_ in enumerate(X_counter_fit.vocabulary_.items()):\n",
    "    if ix < 10 or ix > X_counter_fit.vocabulary_size-10:\n",
    "        print(tuple_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = pipe_bow.transform(X_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  0,  0,  0,  0,  0],\n",
       "       [35,  0,  0,  0,  0,  0],\n",
       "       [41,  0,  0,  0,  0,  1],\n",
       "       [33,  0,  0,  0,  0,  0],\n",
       "       [12,  2,  0,  0,  0,  0],\n",
       "       [11,  0,  0,  0,  0,  0],\n",
       "       [35,  2,  0,  0,  0,  0],\n",
       "       [22,  0,  0,  0,  0,  0],\n",
       "       [10,  2,  0,  0,  0,  0],\n",
       "       [ 9,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow.toarray()[6:16,:6] # first row is how many words the dictionary missed: turns out a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.866, 4.2015, 3.3384, 8.2987, 4.5142, 8.3891, 8.6858, 3.6736, 2.9798]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,4) for x in pipe['tfidf'].fit(X_bow).idf_[:10]] # IDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipe['tfidf'].fit_transform(X_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x21 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 188390 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.993, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.874, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.994, 0.   , 0.   , 0.   , 0.   , 0.109]),\n",
       " array([1., 0., 0., 0., 0., 0.]),\n",
       " array([0.902, 0.431, 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([1., 0., 0., 0., 0., 0.]),\n",
       " array([0.982, 0.161, 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([1., 0., 0., 0., 0., 0.]),\n",
       " array([0.868, 0.497, 0.   , 0.   , 0.   , 0.   ])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,3) for x in tfidf.toarray()[6:15,:6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate couple quick models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', DocumentToBigramCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=50000)), # more realistic vocabulary size\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counter = pipe['counter'].fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counter_fit = pipe['bow'].fit(X_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 1)\n",
      "('USERNAME', 2)\n",
      "('NUMBER', 3)\n",
      "('my', 4)\n",
      "('you', 5)\n",
      "('not', 6)\n",
      "('am', 7)\n",
      "('have', 8)\n",
      "('i_am', 9)\n",
      "('me', 10)\n",
      "('and_nearly', 49992)\n",
      "('wishing_everyone', 49993)\n",
      "('in_twitterland', 49994)\n",
      "('in_december', 49995)\n",
      "('mouthed', 49996)\n",
      "('claw', 49997)\n",
      "('ass_day', 49998)\n",
      "('bagus', 49999)\n",
      "('follow_gw', 50000)\n"
     ]
    }
   ],
   "source": [
    "for ix, tuple_ in enumerate(X_counter_fit.vocabulary_.items()):\n",
    "    if ix < 10 or ix > X_counter_fit.vocabulary_size-10:\n",
    "        print(tuple_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x50001 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 2232287 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow = pipe['bow'].fit_transform(X_counter)\n",
    "X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x50001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2232287 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf = pipe['tfidf'].fit_transform(X_bow)\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7890 (+/- 0.0022)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(NB_clf, X_bow, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7908 (+/- 0.0024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Tfidf with bigrams\n",
    "score = cross_val_score(NB_clf, X_tfidf, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   20.1s remaining:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   29.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7895 (+/- 0.0021)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(log_clf, X_bow, y_array, cv=10, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    3.9s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8019 (+/- 0.0024)\n"
     ]
    }
   ],
   "source": [
    "# Tfidf with bigrams\n",
    "score = cross_val_score(log_clf, X_tfidf, y_array, cv=10, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
