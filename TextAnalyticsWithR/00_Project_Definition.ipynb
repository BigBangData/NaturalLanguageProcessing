{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General: Goals\n",
    "\n",
    "- create portfolio-worthy data science projects\n",
    "- learn NLP and practice ML by building classifiers that use open text fields as input\n",
    "- create a Python framework that generalizes a workflow in R for detecting SMS spam messages\n",
    "- evaluate the effectiveness of the framework with Twitter data to predict some binary outcome\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General: ML Project Structure\n",
    "\n",
    "These steps do not need to be sequential. For example, feature engineering can be done sooner. They are also cyclic: in practice, one might need to continuously re-define the problem, and iterate on EDA and modeling steps.\n",
    "\n",
    "1. define the problem, set expectations and evaluation criteria\n",
    "2. preliminary and minimal EDA and pre-splitting cleanup\n",
    "3. split dataset into trainining and test subsets; set the test subset aside\n",
    "4. create a cleanup and preprocessing pipeline for the training data that can be re-applied to the test data\n",
    "5. train a couple baseline models to ensure process is smooth and pre-processing is dialed in\n",
    "6. using cross-validation, evaluate a variety models without hyperparameter tuning to establish some baselines\n",
    "7. short-list promising models for further hyperparemeter tuning\n",
    "8. iterate on any phase of the project as needed\n",
    "9. consider feature selection and feature engineering\n",
    "10. decide on a final cleanup and processing pipeline\n",
    "11. settle on a final model\n",
    "12. re-apply all cleanup and processing steps to the test set and evaluate final model - once\n",
    "13. create a final presentation of the solution for technical and non-technical audiences\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General: Project Definition\n",
    "\n",
    "There is a lot more unstructured, text data than structured data. To leverage this unstructured text data one needs to apply text analytic techniques to structure the data before getting value from it. \n",
    "\n",
    "Value can be defined in many ways, here are a couple:\n",
    "\n",
    "**Framework for Binary Classification** \n",
    "\n",
    "\n",
    "Given the goal of binary classification, say, to separate *spam* from *ham* (legitimate) messages, or *positive* (happier) from *negative* (unhappier) comments on social media, one can build classifiers that learn from a corpus and is able to predict, given a new instance (new message or comment), wether it is spam or not, or positive or negative. \n",
    "\n",
    "Both cases are well-known and mostly solved. It is less clear to me whether classifiers built for spam detection would also perform well, say, predicting negative Tweets. Having a framework that quickly deploys and assesses the accuracy of classifiers for binary predictions given open text fields seems a valuable pursuit.\n",
    "\n",
    "\n",
    "**Entity Extraction** \n",
    "\n",
    "A common need in businesses that capture text data is to be able to extract keywords from this text data. Say a business has an online app that historically has captured information in a text field, but nobody has had the time to read that input. Going forward, product managers decide this field should be turned into a drop-down menu. To integrate the past and future versions of this field, there's a need to bucket the unstructured text into categories for the new drop-down. To solve this, a data scientist can apply entity extraction techniques:\n",
    "\n",
    "> \"Named-entity recognition (NER) [...] is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories...\" [Wikipedia, accessed Dec 19, 2020](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Project: Introduction to Text Analytics With R\n",
    "\n",
    "This is a Python offshoot of the original YouTube series tutorial by David Langer from Data Science Dojo: [DSD's Introduction to Text Analytics with R](https://www.youtube.com/playlist?list=PLTJTBoU5HOCR5Vkah2Z-AU76ZYsZjGFK6)\n",
    "\n",
    "\n",
    "#### What is the problem this project is trying to solve?\n",
    "\n",
    "Separate SPAM from HAM (legitimate) SMS messages.\n",
    "\n",
    "\n",
    "#### What are the expectations?\n",
    "\n",
    "That the classifier built for this purpose can do this task quickly and accurately enough to avoid frustrating users.\n",
    "\n",
    "\n",
    "#### What does success look like for this project?\n",
    "\n",
    "\n",
    "A classifier that achieves high sensitivity, thus avoiding sending legitimate SMS to the spam folder, which would be a lot more frustrating than the occasional spam not being filtered. This classifier also has somewhat high specificicity, it filters most spam messages as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Project: The Data\n",
    "\n",
    "The dataset is now spread across the internet, perhaps a good source is the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection#). The dataset is a collage of sources; the data collection process is explained by the original authors Tiago A. Almeida and José María Gómez Hidalgo in this [University of Campinas website.](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
