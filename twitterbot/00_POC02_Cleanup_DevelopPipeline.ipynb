{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - POC\n",
    "---\n",
    "\n",
    "## 4. Cleaning Pipeline\n",
    "\n",
    "So far the steps have been the same as they'd have been for the main project. Settting up a cleaning pipeline is an involved process and the formal beginning of the POC. For the final project, I will not be sampling the data down and all the processing steps will be encapsulated in a custom module.\n",
    "\n",
    "\n",
    "## POC Only - Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urlextract\n",
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "deduped_path = os.path.join(\"..\",\"data\",\"1.2_deduped\",\"tweets\",\"prepared\")\n",
    "X_train = pd.read_csv(os.path.join(deduped_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(deduped_path, \"y_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample down considerably to X, y sample subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.999, random_state=159)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is to forget about the `_rest` datasets and focus on the X, y small subsets, as if they were the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 779\n",
      "Target distribution: 0.529\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset size: {len(X):0.0f}')\n",
    "print(f'Target distribution: {sum(y[\"target\"])/len(y):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401292</th>\n",
       "      <td>1307540808128700417</td>\n",
       "      <td>D35NUD4T3</td>\n",
       "      <td>Bionic themed game. Let‚Äôs play Fighters! üòâüòà ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414709</th>\n",
       "      <td>1314591505466626049</td>\n",
       "      <td>halfmoonbean</td>\n",
       "      <td>the amt of pictures mj posted today... \\nwe lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444919</th>\n",
       "      <td>1308438100700524546</td>\n",
       "      <td>devilsvalentine</td>\n",
       "      <td>@ThrowAwayPogue i‚Äôm so proud ü•∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722403</th>\n",
       "      <td>1308088080658608131</td>\n",
       "      <td>ONLY4TREASURE</td>\n",
       "      <td>@hwankyuthree what‚Äôs happening ü•∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334176</th>\n",
       "      <td>1310026458740465671</td>\n",
       "      <td>mentalninja33</td>\n",
       "      <td>@TheEdumicator @unearthvii It really is a mast...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID         username  \\\n",
       "401292  1307540808128700417        D35NUD4T3   \n",
       "414709  1314591505466626049     halfmoonbean   \n",
       "444919  1308438100700524546  devilsvalentine   \n",
       "722403  1308088080658608131    ONLY4TREASURE   \n",
       "334176  1310026458740465671    mentalninja33   \n",
       "\n",
       "                                                    tweet  \n",
       "401292  Bionic themed game. Let‚Äôs play Fighters! üòâüòà ht...  \n",
       "414709  the amt of pictures mj posted today... \\nwe lo...  \n",
       "444919                     @ThrowAwayPogue i‚Äôm so proud ü•∫  \n",
       "722403                   @hwankyuthree what‚Äôs happening ü•∫  \n",
       "334176  @TheEdumicator @unearthvii It really is a mast...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an array out of the tweet column, and that array will not contain the shuffled indices in a randomly sampled training dataset so we capture that in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1307540808128700417</td>\n",
       "      <td>D35NUD4T3</td>\n",
       "      <td>Bionic themed game. Let‚Äôs play Fighters! üòâüòà ht...</td>\n",
       "      <td>401292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314591505466626049</td>\n",
       "      <td>halfmoonbean</td>\n",
       "      <td>the amt of pictures mj posted today... \\nwe lo...</td>\n",
       "      <td>414709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1308438100700524546</td>\n",
       "      <td>devilsvalentine</td>\n",
       "      <td>@ThrowAwayPogue i‚Äôm so proud ü•∫</td>\n",
       "      <td>444919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1308088080658608131</td>\n",
       "      <td>ONLY4TREASURE</td>\n",
       "      <td>@hwankyuthree what‚Äôs happening ü•∫</td>\n",
       "      <td>722403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1310026458740465671</td>\n",
       "      <td>mentalninja33</td>\n",
       "      <td>@TheEdumicator @unearthvii It really is a mast...</td>\n",
       "      <td>334176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID         username  \\\n",
       "0  1307540808128700417        D35NUD4T3   \n",
       "1  1314591505466626049     halfmoonbean   \n",
       "2  1308438100700524546  devilsvalentine   \n",
       "3  1308088080658608131    ONLY4TREASURE   \n",
       "4  1310026458740465671    mentalninja33   \n",
       "\n",
       "                                               tweet   index  \n",
       "0  Bionic themed game. Let‚Äôs play Fighters! üòâüòà ht...  401292  \n",
       "1  the amt of pictures mj posted today... \\nwe lo...  414709  \n",
       "2                     @ThrowAwayPogue i‚Äôm so proud ü•∫  444919  \n",
       "3                   @hwankyuthree what‚Äôs happening ü•∫  722403  \n",
       "4  @TheEdumicator @unearthvii It really is a mast...  334176  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.insert(3, 'index', X.index)\n",
    "X.index = range(len(X))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bionic themed game. Let‚Äôs play Fighters! üòâüòà https://t.co/wNGLFlmz4C',\n",
       "       'the amt of pictures mj posted today... \\nwe love kim myungjunü•∫‚ù§Ô∏è'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an array of Tweets\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "X_array[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401292</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414709</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target\n",
       "401292       1\n",
       "414709       1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "The contraction map and `expand_contractions` function were adapted from this [KDnuggests article.](https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html)\n",
    "\n",
    "The **DocumentToWordCounterTransformer** class was inspired by Aurelien Geron's **EmailToWordCounterTransformer** class from his famous [classification notebook](https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb), and the **WordCounterToVectorTransformer** is a straight copy of Geron's same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contractions_map):\n",
    "    \n",
    "    pattern = re.compile('({})'.format('|'.join(contractions_map.keys())), \n",
    "                        flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_map.get(match)\\\n",
    "                                if contractions_map.get(match)\\\n",
    "                                else contractions_map.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "           \n",
    "class DocumentToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, expand_contractions=True, lower_case=True, \n",
    "                 replace_usernames=True, unescape_html=True, \n",
    "                 replace_urls=True, replace_numbers=True, \n",
    "                 remove_junk=True, remove_punctuation=True, \n",
    "                 replace_emojis=True, replace_nonascii=True, \n",
    "                 remove_stopwords=True, lemmatization=True):\n",
    "        self.expand_contractions = expand_contractions\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.expand_contractions and contractions_map is not None:\n",
    "                doc = expand_contractions(doc, contractions_map)\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'^@([^\\s]+)',' USERNAME ', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¬•|\\√¢|\\¬´|\\¬ª|\\√ë|\\√ê|\\¬º|\\¬Ω|\\¬æ|\\!|\\?|\\¬ø|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\¬∞|\\¬µ|\\¬¥|\\¬∫|\\¬π|\\¬≥'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '\n",
    "            word_counts = Counter(doc.split())\n",
    "            if self.remove_stopwords:\n",
    "                # 25 semantically non-selective words from the Reuters-RCV1 dataset\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with']\n",
    "                for word in stop_words:\n",
    "                    try:\n",
    "                        word_counts.pop(word)\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            if self.expand_contractions:\n",
    "                    leftovers = ['t','s','d','m','ve','re','ll','']\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                lemmatized_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "                    lemmatized_word_counts[lemmatized_word] += count\n",
    "                word_counts = lemmatized_word_counts      \n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wordcounts = DocumentToWordCounterTransformer().fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'USERNAME': 1, 'i': 1, 'm': 1, 'so': 1, 'proud': 1})\n",
      "{'USERNAME': 1, 'what': 1, 's': 1, 'happening': 1})\n",
      "{'USERNAME': 1, 'unearthvii': 1, 'really': 1, 'masterpiece': 1, 'go': 1, 'fridge': 1, 'even': 1, 'kid': 1, 'like': 1, 'dang': 1, 's': 1, 'pretty': 1, 'good': 1})\n"
     ]
    }
   ],
   "source": [
    "for counter in X_wordcounts[2:5]:\n",
    "    counter = str(counter).split(\"(\")[1]\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<779x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 6476 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer()\n",
    "X_vectors = vocab_transformer.fit_transform(X_wordcounts)\n",
    "X_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1312949693404438528</td>\n",
       "      <td>defikigai</td>\n",
       "      <td>finding money üòÉü§≤üèª</td>\n",
       "      <td>516760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID   username              tweet   index\n",
       "20  1312949693404438528  defikigai  finding money üòÉü§≤üèª  516760"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from original data\n",
    "X.loc[20:20, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"document_to_wordcount\", DocumentToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<779x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 6476 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.564, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.571, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.609, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.532, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.600, total=   0.0s\n",
      "Mean accuracy: 0.5751282051282052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_array, cv=5, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.603, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.538, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.603, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.571, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.574, total=   0.0s\n",
      "Mean accuracy: 0.5776592224979321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "score = cross_val_score(NB_clf, X_train_transformed, y_array, cv=5, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
