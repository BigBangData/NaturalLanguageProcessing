{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Setiment Analysis \n",
    "\n",
    "### Part 2: sentiment140 dataset pre-processing for ML\n",
    "\n",
    "\n",
    "The data comes from Marios Michailidis' sentiment140 dataset hosted in [Kaggle.](https://www.kaggle.com/kazanova/sentiment140/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 9.64 second(s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import load_data as ld\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "df = ld.run_processes()\n",
    "\n",
    "finish = time.perf_counter()\n",
    "print(f'Finished in {round(finish-start, 2)} second(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>filtered</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>upset cant update his facebook texting might c...</td>\n",
       "      <td>upset cant updat hi facebook text might cri re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>i dived many times ball managed save 50 rest g...</td>\n",
       "      <td>i dive mani time ball manag save 50 rest go ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy like fire</td>\n",
       "      <td>my whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "      <td>no not behaving all im mad why am i here becau...</td>\n",
       "      <td>no not behav all im mad whi am i here becaus i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>not whole crew</td>\n",
       "      <td>not whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       0  is upset that he can't update his Facebook by ...   \n",
       "1       0  @Kenichan I dived many times for the ball. Man...   \n",
       "2       0    my whole body feels itchy and like its on fire    \n",
       "3       0  @nationwideclass no, it's not behaving at all....   \n",
       "4       0                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  is upset that he cant update his facebook by t...   \n",
       "1  i dived many times for the ball managed to sav...   \n",
       "2     my whole body feels itchy and like its on fire   \n",
       "3  no its not behaving at all im mad why am i her...   \n",
       "4                                 not the whole crew   \n",
       "\n",
       "                                            filtered  \\\n",
       "0  upset cant update his facebook texting might c...   \n",
       "1  i dived many times ball managed save 50 rest g...   \n",
       "2                my whole body feels itchy like fire   \n",
       "3  no not behaving all im mad why am i here becau...   \n",
       "4                                     not whole crew   \n",
       "\n",
       "                                             stemmed  \n",
       "0  upset cant updat hi facebook text might cri re...  \n",
       "1  i dive mani time ball manag save 50 rest go ou...  \n",
       "2                 my whole bodi feel itchi like fire  \n",
       "3  no not behav all im mad whi am i here becaus i...  \n",
       "4                                     not whole crew  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ML pre-processing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute NAs created during cleanup\n",
    "\n",
    "We do not want to drop since the fact they ended up as empty strings is possibly informative - for ex. possibly more positive Tweets?\n",
    "\n",
    "TODO: examine them to see what pattern could be used to impute NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target          0\n",
       "text            0\n",
       "tokenized    3696\n",
       "filtered     3819\n",
       "stemmed      3819\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = df.dropna() # drop for now\n",
    "dfm.index = range(1,len(dfm) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((len(df)-len(dfm))/len(df), 4) # prop is low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with BoW and TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "y = np.array(dfm.iloc[:, 0]).ravel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "X_text = np.array(dfm.iloc[:, 1]).ravel() # original text\n",
    "X_tokn = np.array(dfm.iloc[:, 2]).ravel() # tokenized\n",
    "X_filt = np.array(dfm.iloc[:, 3]).ravel() # filtered\n",
    "X_stem = np.array(dfm.iloc[:, 4]).ravel() # stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate vectorizers\n",
    "bow_vectorizer = CountVectorizer() # simple BoW \n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=False) # default Tfidf\n",
    "log_tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True) # log(tf) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_bow = bow_vectorizer.fit_transform(X_text)\n",
    "X_tokn_bow = bow_vectorizer.fit_transform(X_tokn)\n",
    "X_filt_bow = bow_vectorizer.fit_transform(X_filt)\n",
    "X_stem_bow = bow_vectorizer.fit_transform(X_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_tfidf = tfidf_vectorizer.fit_transform(X_text)\n",
    "X_tokn_tfidf = tfidf_vectorizer.fit_transform(X_tokn)\n",
    "X_filt_tfidf = tfidf_vectorizer.fit_transform(X_filt)\n",
    "X_stem_tfidf = tfidf_vectorizer.fit_transform(X_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_log_tfidf = log_tfidf_vectorizer.fit_transform(X_text)\n",
    "X_tokn_log_tfidf = log_tfidf_vectorizer.fit_transform(X_tokn)\n",
    "X_filt_log_tfidf = log_tfidf_vectorizer.fit_transform(X_filt)\n",
    "X_stem_log_tfidf = log_tfidf_vectorizer.fit_transform(X_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: cleanup problem: hashtags and mentions (@) are usually useful in Tweets\n",
    "\n",
    "TODO: examine NAs and use some kind of imputation method\n",
    "\n",
    "TODO: split JN into Pre-Processing and Modeling as originally intended\n",
    "\n",
    "TODO: save sparse matrices in pre-processing JN\n",
    "\n",
    "TODO: Modeling - def function that feeds data to see learning curves and assess fitting (ageron)\n",
    "\n",
    "TODO: use CV folds? hyperparameter tuning? SVC, etc...\n",
    "\n",
    "TODO: feature extraction (text length? do some visualizations)\n",
    "\n",
    "TODO: feature selection?\n",
    "\n",
    "TODO: N-grams?\n",
    "\n",
    "TODO: redo get_accuracy(), has many issues - look at sklearn examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(vectorizer, model, X, y):\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # this is bad, fit every time? Just fit once!\n",
    "    X_fit = vectorizer.fit_transform(X)\n",
    "    \n",
    "    # move to Modeling phase\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_fit, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # just... nah. We want to use a pipeline, save model params\n",
    "    if model == 'NB':\n",
    "        NB = MultinomialNB()\n",
    "        NB.fit(X_train, y_train)\n",
    "        y_preds = NB.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_preds)\n",
    "        print('Accuracy')\n",
    "        print(''.join(['NB: ' , str(round(accuracy, 4))]))\n",
    "\n",
    "    elif model == 'LR':\n",
    "        LR = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        LR.fit(X_train, y_train)\n",
    "        y_preds = LR.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_preds)\n",
    "        print(''.join(['LR: ' , str(round(accuracy, 4))]))\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    finish = time.perf_counter()\n",
    "    print(f'Finished in {round(finish-start, 2)} secs')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW with original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "NB: 0.7825\n",
      "Finished in 53.67 secs\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "get_accuracy(count_vectorizer, 'NB', X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanch\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.8022\n",
      "Finished in 586.5 secs\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "get_accuracy(count_vectorizer, 'LR', X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "NB: 0.7755\n",
      "LR: 0.8006\n",
      "Finished in 243.62 secs\n"
     ]
    }
   ],
   "source": [
    "X = np.array(dfm.iloc[:, 2]).ravel()\n",
    "get_accuracy(vectorizer, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "NB: 0.7748\n",
      "LR: 0.7997\n",
      "Finished in 220.1 secs\n"
     ]
    }
   ],
   "source": [
    "X = np.array(dfm.iloc[:, 3]).ravel()\n",
    "get_accuracy(vectorizer, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "NB: 0.7701\n",
      "LR: 0.7946\n",
      "Finished in 247.27 secs\n"
     ]
    }
   ],
   "source": [
    "X = np.array(dfm.iloc[:, 4]).ravel()\n",
    "get_accuracy(vectorizer, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
