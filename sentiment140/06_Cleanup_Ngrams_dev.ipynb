{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Pipeline 3\n",
    "\n",
    "Adding ngrams to custom **DocumentToWordCounterTransformer** class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urlextract\n",
    "from html import unescape\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import cleanup_module as Cmod\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contractions_map):\n",
    "    \n",
    "    pattern = re.compile('({})'.format('|'.join(contractions_map.keys())), \n",
    "                        flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_map.get(match)\\\n",
    "                                if contractions_map.get(match)\\\n",
    "                                else contractions_map.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentToNgramCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, expand_contractions=True, lower_case=True, \n",
    "                 replace_usernames=True, unescape_html=True, \n",
    "                 replace_urls=True, replace_numbers=True, \n",
    "                 remove_junk=True, remove_punctuation=True, \n",
    "                 replace_emojis=True, replace_nonascii=True, \n",
    "                 remove_stopwords=True, lemmatization=True,\n",
    "                 n_grams=2 # defaults to bigram\n",
    "                ): \n",
    "        self.expand_contractions = expand_contractions\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "        self.n_grams = n_grams\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.expand_contractions and contractions_map is not None:\n",
    "                doc = expand_contractions(doc, contractions_map)\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'^@([^\\s]+)',' USERNAME ', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '\n",
    "            # tokenize\n",
    "            tokens = doc.split()\n",
    "            if self.remove_stopwords:\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with']\n",
    "                tokens = [t for t in tokens if t not in stop_words]\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                unigrams = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            if self.n_grams:\n",
    "                for i in range(self.n_grams+1):\n",
    "                    grams = ngrams(word_tokenize(doc), i)\n",
    "                    grams = ['_'.join(gram) for gram in grams]\n",
    "                    tokens = [*tokens, *grams]\n",
    "            # include counts\n",
    "            tokens_counts = Counter(tokens)\n",
    "            # append to list\n",
    "            X_transformed.append(tokens_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['You love me', \n",
    "          'You do not love me',\n",
    "          'You really really love food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = DocumentToNgramCounterTransformer(n_grams=3)\n",
    "X_trans = wordvec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'you': 3, 'love': 3, 'me': 3, 'you_love': 1, 'love_me': 1, 'you_love_me': 1}),\n",
       "       Counter({'you': 3, 'do': 3, 'not': 3, 'love': 3, 'me': 3, 'you_do': 1, 'do_not': 1, 'not_love': 1, 'love_me': 1, 'you_do_not': 1, 'do_not_love': 1, 'not_love_me': 1}),\n",
       "       Counter({'really': 6, 'you': 3, 'love': 3, 'food': 3, 'you_really': 1, 'really_really': 1, 'really_love': 1, 'love_food': 1, 'you_really_really': 1, 'really_really_love': 1, 'really_love_food': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=20)),\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_counter = pipe['counter'].fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bow = pipe['bow'].fit(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 1,\n",
       " 'love': 2,\n",
       " 'me': 3,\n",
       " 'really': 4,\n",
       " 'do': 5,\n",
       " 'not': 6,\n",
       " 'food': 7,\n",
       " 'love_me': 8,\n",
       " 'you_love': 9,\n",
       " 'you_love_me': 10,\n",
       " 'you_do': 11,\n",
       " 'do_not': 12,\n",
       " 'not_love': 13,\n",
       " 'you_do_not': 14,\n",
       " 'do_not_love': 15,\n",
       " 'not_love_me': 16,\n",
       " 'you_really': 17,\n",
       " 'really_really': 18,\n",
       " 'really_love': 19,\n",
       " 'love_food': 20}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pipe_bow.transform(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 3, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 3, 3, 3, 0, 3, 3, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [3, 3, 3, 0, 6, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray() # first col is \"words missing from vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.288,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.288,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # IDF for the pipe_bow.vocabulary_\n",
    "[np.around(x,3) for x in pipe['tfidf'].fit(bow).idf_[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipe['tfidf'].fit_transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.4725, 0.4725, 0.6084, 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.2028, 0.2667, 0.2667, 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.2951, 0.2951, 0.3799, 0.    , 0.4996, 0.4996, 0.    ,\n",
       "        0.1266, 0.    , 0.    , 0.1665, 0.1665, 0.1665, 0.1665, 0.1665,\n",
       "        0.1665, 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.3742, 0.221 , 0.221 , 0.    , 0.7484, 0.    , 0.    , 0.3742,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.1247, 0.1247, 0.1247, 0.1247])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,4) for x in tfidf.toarray()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.4725, 0.4725, 0.6084, 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.2028, 0.2667, 0.2667, 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.2951, 0.2951, 0.3799, 0.    , 0.4996, 0.4996, 0.    ,\n",
       "        0.1266, 0.    , 0.    , 0.1665, 0.1665, 0.1665, 0.1665, 0.1665,\n",
       "        0.1665, 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.3742, 0.221 , 0.221 , 0.    , 0.7484, 0.    , 0.    , 0.3742,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.1247, 0.1247, 0.1247, 0.1247])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entire pipeline produces same result but does't save IDF or vocab\n",
    "end_res = pipe.fit_transform(corpus)\n",
    "[np.around(x,4) for x in end_res.toarray()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC: sample 10% of the training data\n",
    "\n",
    "About 120,000 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample 10%\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# create array\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119747,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipe\n",
    "pipe = Pipeline([('counter', DocumentToNgramCounterTransformer()), # bigrams\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=1000)),\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 2 min 55 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X_end = pipe.fit_transform(X_array)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x1001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1539899 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.487, 0.169, 0.   , 0.137, 0.   , 0.   ]),\n",
       " array([0.501, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.653, 0.   , 0.   , 0.   , 0.177, 0.   ]),\n",
       " array([0.503, 0.   , 0.114, 0.094, 0.   , 0.   ]),\n",
       " array([0.481, 0.141, 0.069, 0.171, 0.   , 0.111]),\n",
       " array([0.098, 0.136, 0.   , 0.11 , 0.   , 0.   ]),\n",
       " array([0.311, 0.115, 0.113, 0.   , 0.   , 0.181]),\n",
       " array([0.332, 0.   , 0.065, 0.053, 0.   , 0.   ]),\n",
       " array([0.61 , 0.077, 0.   , 0.125, 0.   , 0.   ]),\n",
       " array([0.398, 0.092, 0.   , 0.149, 0.   , 0.145])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,3) for x in X_end.toarray()[:10,:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 2 min 48 sec\n"
     ]
    }
   ],
   "source": [
    "# step by step for vocab and idf\n",
    "start_time = time.time()\n",
    "\n",
    "pipe_counter = pipe['counter'].fit_transform(X_array)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0 min 4 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pipe_bow = pipe['bow'].fit(pipe_counter) \n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 1)\n",
      "('USERNAME', 2)\n",
      "('to', 3)\n",
      "('the', 4)\n",
      "('NUMBER', 5)\n",
      "('my', 6)\n",
      "('you', 7)\n",
      "('not', 8)\n",
      "('a', 9)\n",
      "('is', 10)\n"
     ]
    }
   ],
   "source": [
    "for ix, w in enumerate(pipe_bow.vocabulary_.items()):\n",
    "    if ix < 10:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0 min 4 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bow = pipe_bow.transform(pipe_counter)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  3,  0, ...,  0,  0,  0],\n",
       "       [13,  0,  0, ...,  0,  0,  0],\n",
       "       [17,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [28,  0,  3, ...,  0,  0,  0],\n",
       "       [44,  3,  0, ...,  0,  0,  0],\n",
       "       [24,  3,  0, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()[:10] # misses too many words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0062, 1.8634, 1.8303, 2.2596, 2.32, 2.935, 2.7413, 2.866, 2.8091, 2.5682]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,4) for x in pipe['tfidf'].fit(bow).idf_[:10]] # IDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0 min 0 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tfidf = pipe['tfidf'].fit_transform(bow)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x1001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1539899 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.487, 0.169, 0.   , 0.137, 0.   , 0.   ]),\n",
       " array([0.501, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.653, 0.   , 0.   , 0.   , 0.177, 0.   ]),\n",
       " array([0.503, 0.   , 0.114, 0.094, 0.   , 0.   ]),\n",
       " array([0.481, 0.141, 0.069, 0.171, 0.   , 0.111]),\n",
       " array([0.098, 0.136, 0.   , 0.11 , 0.   , 0.   ]),\n",
       " array([0.311, 0.115, 0.113, 0.   , 0.   , 0.181]),\n",
       " array([0.332, 0.   , 0.065, 0.053, 0.   , 0.   ]),\n",
       " array([0.61 , 0.077, 0.   , 0.125, 0.   , 0.   ]),\n",
       " array([0.398, 0.092, 0.   , 0.149, 0.   , 0.145])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,3) for x in tfidf.toarray()[:10,:6]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate couple quick models on bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', DocumentToNgramCounterTransformer()), # bigrams\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=50000)), # more realistic vocab size\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 2 min 49 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X1 = pipe['counter'].fit_transform(X_array)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pipe['bow'].fit(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49981 money_but\n",
      "49982 brut\n",
      "49983 kats\n",
      "49984 wuzzup\n",
      "49985 cutiest\n",
      "49986 thers\n",
      "49987 scarecrow\n",
      "49988 cleanliness\n",
      "49989 twitterwall\n",
      "49990 shuttles\n",
      "49991 rolla\n",
      "49992 gardner\n",
      "49993 surgury\n",
      "49994 nozzle\n",
      "49995 printers\n",
      "49996 entertainer\n",
      "49997 stephy\n",
      "49998 selfridges\n",
      "49999 peppermint\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(X2.vocabulary_):\n",
    "    if i > 49980:\n",
    "        print(i, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = pipe['bow'].fit_transform(X1) # BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = pipe['tfidf'].fit_transform(X3) # Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7812 (+/- 0.0032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(NB_clf, X3, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7835 (+/- 0.0035)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Tfidf with bigrams\n",
    "score = cross_val_score(NB_clf, X4, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  1.6min remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7786 (+/- 0.0033)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(log_clf, X3, y_array, cv=10, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    5.2s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7973 (+/- 0.0032)\n"
     ]
    }
   ],
   "source": [
    "# Tfidf with bigrams\n",
    "score = cross_val_score(log_clf, X4, y_array, cv=10, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
