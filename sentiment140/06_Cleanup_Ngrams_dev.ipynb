{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Pipeline 4\n",
    "\n",
    "REDO Ngram counter with learning curves.\n",
    "\n",
    "*Purpose*\n",
    "- generalize the **DocumentToBigramCounterTransformer** class to a **DocumentToNgramCounterTransformer** class. \n",
    "\n",
    "*Concerns*\n",
    "- has same issues as Bigram class (stop words in Ngrams, no Ngram-only option)\n",
    "- also noticed other transformations might not have worked, like expand_contractions (Ex. \"havent_done\"); why?\n",
    "- is potentially slower because of the for loop in `self.n_grams`\n",
    "\n",
    "*Results*\n",
    "- using the same dataset $(m=120k,n=50k)$ on the new **DocumentToNgramCounterTransformer** class set to the default (Bigram) yields the same exact results, which is a great sign it's working the same as the **DocumentToBigramCounterTransformer** class\n",
    "- using the same dataset with **Trigrams** yields slightly lower accuracies\n",
    "- doubling the vocabulary to $n=100k$ yields similar accuracies\n",
    "- with higher vocabularies (250, 500k) we get no improvement, but surprisingly, the training time is similar\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urlextract\n",
    "from html import unescape\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import cleanup_module as Cmod\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contractions_map):\n",
    "    \n",
    "    pattern = re.compile('({})'.format('|'.join(contractions_map.keys())), \n",
    "                        flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_map.get(match)\\\n",
    "                                if contractions_map.get(match)\\\n",
    "                                else contractions_map.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentToNgramCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, expand_contractions=True, lower_case=True, \n",
    "                 replace_usernames=True, unescape_html=True, \n",
    "                 replace_urls=True, replace_numbers=True, \n",
    "                 remove_junk=True, remove_punctuation=True, \n",
    "                 replace_emojis=True, replace_nonascii=True, \n",
    "                 remove_stopwords=True, lemmatization=True,\n",
    "                 n_grams=2 # defaults to bigram\n",
    "                ): \n",
    "        self.expand_contractions = expand_contractions\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "        self.n_grams = n_grams\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.expand_contractions and contractions_map is not None:\n",
    "                doc = expand_contractions(doc, contractions_map)\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'^@([^\\s]+)',' USERNAME ', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '\n",
    "            # tokenize\n",
    "            tokens = doc.split()\n",
    "            if self.remove_stopwords:\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with']\n",
    "                tokens = [t for t in tokens if t not in stop_words]\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            if self.n_grams:\n",
    "                for i in range(2, self.n_grams+1): # fix doubling of unigrams\n",
    "                    grams = ngrams(word_tokenize(doc), i)\n",
    "                    grams = ['_'.join(gram) for gram in grams]\n",
    "                    tokens = [*tokens, *grams]\n",
    "            # include counts\n",
    "            tokens_counts = Counter(tokens)\n",
    "            # append to list\n",
    "            X_transformed.append(tokens_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['You The love me', \n",
    "          'You do not love me',\n",
    "          'You really really love food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = DocumentToNgramCounterTransformer(n_grams=3)\n",
    "X_trans = wordvec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'you': 1, 'love': 1, 'me': 1, 'you_the': 1, 'the_love': 1, 'love_me': 1, 'you_the_love': 1, 'the_love_me': 1}),\n",
       "       Counter({'you': 1, 'do': 1, 'not': 1, 'love': 1, 'me': 1, 'you_do': 1, 'do_not': 1, 'not_love': 1, 'love_me': 1, 'you_do_not': 1, 'do_not_love': 1, 'not_love_me': 1}),\n",
       "       Counter({'really': 2, 'you': 1, 'love': 1, 'food': 1, 'you_really': 1, 'really_really': 1, 'really_love': 1, 'love_food': 1, 'you_really_really': 1, 'really_really_love': 1, 'really_love_food': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=20)),\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_counter = pipe['counter'].fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bow = pipe['bow'].fit(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 1,\n",
       " 'love': 2,\n",
       " 'me': 3,\n",
       " 'love_me': 4,\n",
       " 'really': 5,\n",
       " 'you_the': 6,\n",
       " 'the_love': 7,\n",
       " 'you_the_love': 8,\n",
       " 'the_love_me': 9,\n",
       " 'do': 10,\n",
       " 'not': 11,\n",
       " 'you_do': 12,\n",
       " 'do_not': 13,\n",
       " 'not_love': 14,\n",
       " 'you_do_not': 15,\n",
       " 'do_not_love': 16,\n",
       " 'not_love_me': 17,\n",
       " 'food': 18,\n",
       " 'you_really': 19,\n",
       " 'really_really': 20}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pipe_bow.transform(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [5, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray() # first col is \"words missing from vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.288,\n",
       " 1.288,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # IDF for the pipe_bow.vocabulary_\n",
    "[np.around(x,3) for x in pipe['tfidf'].fit(bow).idf_[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipe['tfidf'].fit_transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.2441, 0.2441, 0.3143, 0.3143, 0.    , 0.4133, 0.4133,\n",
       "        0.4133, 0.4133, 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.1881, 0.1881, 0.2423, 0.2423, 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.3186, 0.3186, 0.3186, 0.3186, 0.3186, 0.3186,\n",
       "        0.3186, 0.3186, 0.    , 0.    , 0.    ]),\n",
       " array([0.8744, 0.1033, 0.1033, 0.    , 0.    , 0.3498, 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.1749, 0.1749, 0.1749])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,4) for x in tfidf.toarray()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.2441, 0.2441, 0.3143, 0.3143, 0.    , 0.4133, 0.4133,\n",
       "        0.4133, 0.4133, 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.1881, 0.1881, 0.2423, 0.2423, 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.3186, 0.3186, 0.3186, 0.3186, 0.3186, 0.3186,\n",
       "        0.3186, 0.3186, 0.    , 0.    , 0.    ]),\n",
       " array([0.8744, 0.1033, 0.1033, 0.    , 0.    , 0.3498, 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.1749, 0.1749, 0.1749])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entire pipeline produces same result but does't save IDF or vocab\n",
    "end_res = pipe.fit_transform(corpus)\n",
    "[np.around(x,4) for x in end_res.toarray()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC: sample 10% of the training data\n",
    "\n",
    "About 120,000 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample 10%\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# create arrays\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams\n",
    "\n",
    "Here I just confirm that the new Ngram class works just as the previous Bigram class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipe\n",
    "pipe = Pipeline([('counter', DocumentToNgramCounterTransformer()), # should be the same as Bigram transformer\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=50000)),\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1 min 44 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X_counter = pipe['counter'].fit_transform(X_array)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counter_fit = pipe['bow'].fit(X_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 1)\n",
      "('USERNAME', 2)\n",
      "('NUMBER', 3)\n",
      "('my', 4)\n",
      "('you', 5)\n",
      "('not', 6)\n",
      "('am', 7)\n",
      "('have', 8)\n",
      "('i_am', 9)\n",
      "('me', 10)\n",
      "('and_nearly', 49992)\n",
      "('wishing_everyone', 49993)\n",
      "('in_twitterland', 49994)\n",
      "('in_december', 49995)\n",
      "('mouthed', 49996)\n",
      "('claw', 49997)\n",
      "('ass_day', 49998)\n",
      "('bagus', 49999)\n",
      "('follow_gw', 50000)\n"
     ]
    }
   ],
   "source": [
    "for ix, tuple_ in enumerate(X_counter_fit.vocabulary_.items()):\n",
    "    if ix < 10 or ix > X_counter_fit.vocabulary_size-10:\n",
    "        print(tuple_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x50001 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 2232287 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow = pipe['bow'].fit_transform(X_counter)\n",
    "X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x50001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2232287 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf = pipe['tfidf'].fit_transform(X_bow)\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7890 (+/- 0.0022)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(NB_clf, X_bow, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7908 (+/- 0.0024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Tfidf with bigrams\n",
    "score = cross_val_score(NB_clf, X_tfidf, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   18.7s remaining:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   28.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7895 (+/- 0.0021)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(log_clf, X_bow, y_array, cv=10, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    4.0s remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8019 (+/- 0.0024)\n"
     ]
    }
   ],
   "source": [
    "# Tfidf with bigrams\n",
    "score = cross_val_score(log_clf, X_tfidf, y_array, cv=10, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams & Quadrigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_ngrams(X_array, y_array, n_grams, vocab_sizes):\n",
    "    T1 = time.time()\n",
    "    fit_times, NB_means, NB_stds, LR_means, LR_stds = [], [], [], [], []\n",
    "\n",
    "    for vocab_size in vocab_sizes:\n",
    "        start = time.time()\n",
    "        pipe = Pipeline([('counter', DocumentToNgramCounterTransformer(n_grams=n_grams)),\n",
    "                         ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=vocab_size)),\n",
    "                         ('tfidf', TfidfTransformer())])\n",
    "        \n",
    "        # fit_transform\n",
    "        X_transformed = pipe.fit_transform(X_array)\n",
    "        fit_times.append(round(time.time() - start, 1))\n",
    "        \n",
    "        # instantiate models\n",
    "        NB_clf = MultinomialNB()\n",
    "        log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "        \n",
    "        # NB\n",
    "        scores = cross_val_score(NB_clf, X_transformed, y_array, cv=5, \n",
    "                                 verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "        NB_means.append(round(scores.mean(), 4))\n",
    "        NB_stds.append(round(np.std(scores), 4))\n",
    "        \n",
    "        # LR\n",
    "        scores = cross_val_score(log_clf, X_transformed, y_array, cv=5, \n",
    "                                verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "        LR_means.append(round(scores.mean(), 4))\n",
    "        LR_stds.append(round(np.std(scores), 4))\n",
    "\n",
    "    df = pd.DataFrame({\"ngram\":n_grams,\n",
    "                       \"vocab_size\":vocab_sizes,\n",
    "                       \"fit_time\":fit_times,\n",
    "                       \"NB_mean\":NB_means,\n",
    "                       \"NB_std\":NB_stds,\n",
    "                       \"LR_mean\":LR_means,\n",
    "                       \"LR_std\":LR_stds\n",
    "    })\n",
    "    mins, secs = divmod(time.time() - T1, 60)\n",
    "    print(f'Elapsed: {mins:0.0f} m {secs:0.0f} s')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 7 m 24 s\n"
     ]
    }
   ],
   "source": [
    "df = gridsearch_ngrams(X_array, y_array, 2, list(range(50, 5000, 500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>NB_mean</th>\n",
       "      <th>NB_std</th>\n",
       "      <th>LR_mean</th>\n",
       "      <th>LR_std</th>\n",
       "      <th>ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>42.2</td>\n",
       "      <td>0.6465</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.6541</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550</td>\n",
       "      <td>41.4</td>\n",
       "      <td>0.7324</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.7478</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1050</td>\n",
       "      <td>42.7</td>\n",
       "      <td>0.7470</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.7624</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1550</td>\n",
       "      <td>42.4</td>\n",
       "      <td>0.7548</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2050</td>\n",
       "      <td>42.1</td>\n",
       "      <td>0.7603</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.7773</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2550</td>\n",
       "      <td>43.5</td>\n",
       "      <td>0.7659</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3050</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.7683</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3550</td>\n",
       "      <td>41.8</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.7866</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4050</td>\n",
       "      <td>43.1</td>\n",
       "      <td>0.7722</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.7881</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4550</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.7742</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.7893</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vocab_size  fit_time  NB_mean  NB_std  LR_mean  LR_std  ngram\n",
       "0          50      42.2   0.6465  0.0019   0.6541  0.0022      2\n",
       "1         550      41.4   0.7324  0.0021   0.7478  0.0017      2\n",
       "2        1050      42.7   0.7470  0.0011   0.7624  0.0014      2\n",
       "3        1550      42.4   0.7548  0.0010   0.7711  0.0014      2\n",
       "4        2050      42.1   0.7603  0.0008   0.7773  0.0006      2\n",
       "5        2550      43.5   0.7659  0.0018   0.7820  0.0010      2\n",
       "6        3050      42.9   0.7683  0.0021   0.7846  0.0013      2\n",
       "7        3550      41.8   0.7711  0.0015   0.7866  0.0006      2\n",
       "8        4050      43.1   0.7722  0.0021   0.7881  0.0014      2\n",
       "9        4550      42.9   0.7742  0.0021   0.7893  0.0015      2"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 9 m 51 s\n"
     ]
    }
   ],
   "source": [
    "df = gridsearch_ngrams(X_array, y_array, 3, list(range(50, 5000, 500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>NB_mean</th>\n",
       "      <th>NB_std</th>\n",
       "      <th>LR_mean</th>\n",
       "      <th>LR_std</th>\n",
       "      <th>ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>56.8</td>\n",
       "      <td>0.6456</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550</td>\n",
       "      <td>56.6</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.7466</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1050</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.7442</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.7604</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1550</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.7523</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.7687</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2050</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.7571</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.7740</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2550</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.7627</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.7794</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3050</td>\n",
       "      <td>58.2</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3550</td>\n",
       "      <td>57.3</td>\n",
       "      <td>0.7666</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4050</td>\n",
       "      <td>58.7</td>\n",
       "      <td>0.7692</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4550</td>\n",
       "      <td>57.4</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vocab_size  fit_time  NB_mean  NB_std  LR_mean  LR_std  ngram\n",
       "0          50      56.8   0.6456  0.0018   0.6533  0.0024      3\n",
       "1         550      56.6   0.7310  0.0020   0.7466  0.0014      3\n",
       "2        1050      57.0   0.7442  0.0010   0.7604  0.0017      3\n",
       "3        1550      56.7   0.7523  0.0018   0.7687  0.0019      3\n",
       "4        2050      57.4   0.7571  0.0009   0.7740  0.0011      3\n",
       "5        2550      56.7   0.7627  0.0013   0.7794  0.0012      3\n",
       "6        3050      58.2   0.7650  0.0018   0.7818  0.0010      3\n",
       "7        3550      57.3   0.7666  0.0024   0.7833  0.0010      3\n",
       "8        4050      58.7   0.7692  0.0016   0.7857  0.0010      3\n",
       "9        4550      57.4   0.7710  0.0023   0.7869  0.0011      3"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = list(range(5000, 50000, 5000)) \\\n",
    "            + list(range(50000, 500000, 50000)) \\\n",
    "            + list(range(500000, 5050000, 500000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = gridsearch_ngrams(X_array[:1000], y_array[:1000], 2, vocab_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
