{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "*Pupose*\n",
    "\n",
    "- Develop Feature Engineering, such as:\n",
    "    * TextLength, etc.\n",
    "    * Number of terms!\n",
    "    * Scale\n",
    "    * Cosine Similarity?\n",
    "    * count of swear words, or negative words, or positive words, etc. (need lists)\n",
    "    * semantic analysis research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urlextract\n",
    "from html import unescape\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load contractions map\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "# functions\n",
    "def expand_contractions(text, contractions_map):\n",
    "    \n",
    "    pattern = re.compile('({})'.format('|'.join(contractions_map.keys())), \n",
    "                        flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_map.get(match)\\\n",
    "                                if contractions_map.get(match)\\\n",
    "                                else contractions_map.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rsr(txt):\n",
    "    \"\"\"Calculates the ratio of characters in \n",
    "    the right-side of the QWERTY keyboard, also\n",
    "    known as RSR (Right-Side Ratio), given a \n",
    "    lower-case text object.\n",
    "    \"\"\"\n",
    "    lside = ['q','w','e','r','t',\n",
    "             'a','s','d','f','g',\n",
    "             'z','x','c','v','b']\n",
    "    rside = ['y','u','i','o','p',\n",
    "             'h','j','k','l',\n",
    "             'n','m']\n",
    "    txt = str(txt)\n",
    "    sub_string = [x for x in txt]\n",
    "    lcount = rcount = 0\n",
    "    for i in sub_string:\n",
    "        if i in lside:\n",
    "            lcount += 1\n",
    "        elif i in rside:\n",
    "            rcount += 1\n",
    "        else:\n",
    "            pass\n",
    "    den = rcount+lcount\n",
    "    if den != 0:\n",
    "        return round(rcount / den, 4)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC: sample $10\\%$ of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample 0.1%\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# create arrays\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119747,), (119747,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape, y_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate url extractor and lemmatizer\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentToFeaturesCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, expand_contractions=True, lower_case=True, \n",
    "                 replace_usernames=True, unescape_html=True, \n",
    "                 replace_urls=True, replace_numbers=True, \n",
    "                 remove_junk=True, remove_punctuation=True, \n",
    "                 replace_emojis=True, replace_nonascii=True, \n",
    "                 remove_stopwords=True, lemmatization=True):\n",
    "        self.expand_contractions = expand_contractions\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # raw len\n",
    "        doclen_raw = [len(doc) for doc in X]\n",
    "        doclen_clean = []\n",
    "        wordlen_max = []\n",
    "        wordlen_mean = []\n",
    "        wordlen_std = []\n",
    "        rsr_ = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.expand_contractions and contractions_map is not None:\n",
    "                doc = expand_contractions(doc, contractions_map)\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'^@([^\\s]+)','usr', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, 'url')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'nur', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern, 'jek', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', 'emj', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = 'nas'\n",
    "            # clean len\n",
    "            doclen_clean.append(len(doc.strip()))\n",
    "            rsr_.append(calc_rsr(doc.strip()))\n",
    "            # tokenize\n",
    "            tokens = doc.split()\n",
    "            lengths = [len(t) for t in tokens]\n",
    "            # token len stats\n",
    "            try:\n",
    "                wordlen_max.append(max(lengths))\n",
    "            except ValueError:\n",
    "                wordlen_max.append(0)\n",
    "            try:\n",
    "                wordlen_mean.append(round(np.mean(lengths),4))\n",
    "            except ValueError:\n",
    "                wordlen_mean.append(0)  \n",
    "            try:\n",
    "                wordlen_std.append(round(np.std(lengths),4))\n",
    "            except ValueError:\n",
    "                wordlen_std.append(0)\n",
    "        # list of lists\n",
    "        X_transformed = np.array([\n",
    "                                 doclen_raw,\n",
    "                                 doclen_clean,\n",
    "                                 wordlen_max,\n",
    "                                 wordlen_mean,\n",
    "                                 wordlen_std,\n",
    "                                 rsr_     \n",
    "                                ])\n",
    "        return X_transformed.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['is going to be M.I.A. for awhile... eff finals ',\n",
       "       'Bewildered by #photography post processing ',\n",
       "       'Plays the guitar.! Superstar real quick ',\n",
       "       '@lassi Cheers mate! Had to check this hyped service out! Looking pretty nice ',\n",
       "       '@drerperiod i tried to tell @chr0nic but she was soo sure lolol.... mann i wanted to come but had to get this studying done  how was it??',\n",
       "       'I love going to bed in a clean room ',\n",
       "       \"@NBear927 I'm rooting for him also!!! But he didn't make it on E3. \",\n",
       "       \"@CheslaMaree don't worry some people don't know how to show there true feelings in retrun, Blaine says don't worry be happy \",\n",
       "       'Sharing my feelings with Benu. Thx nu, I could use a frined to talk to. Untitled - Simple Plan is really good for both of us ',\n",
       "       'Eleven hour shift to look forward to tmoro. Start at 7am. This is what i call bad times '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = DocumentToFeaturesCounterTransformer().fit_transform(X_array[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 47.    ,  42.    ,   6.    ,   2.9091,   1.8318,   0.4375],\n",
       "       [ 43.    ,  41.    ,  11.    ,   7.4   ,   3.6661,   0.4324],\n",
       "       [ 40.    ,  41.    ,   9.    ,   5.    ,   1.9272,   0.4   ],\n",
       "       [ 77.    ,  77.    ,   7.    ,   5.    ,   1.6172,   0.4462],\n",
       "       [137.    , 129.    ,   9.    ,   3.8148,   1.9444,   0.4563],\n",
       "       [ 36.    ,  35.    ,   5.    ,   3.    ,   1.4907,   0.5185],\n",
       "       [ 67.    ,  69.    ,  13.    ,   3.6667,   2.8206,   0.5636],\n",
       "       [124.    , 116.    ,   8.    ,   3.875 ,   1.5894,   0.4839],\n",
       "       [125.    , 119.    ,   8.    ,   3.8   ,   2.0199,   0.5053],\n",
       "       [ 88.    ,  87.    ,   7.    ,   3.8889,   1.5595,   0.4143]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_transformed = DocumentToFeaturesCounterTransformer().fit_transform(X_array)\n",
    "except RuntimeWarning:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119747, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NA issues\n",
    "\n",
    "Some Tweets only had a username, so I changed the code to substitute username with usr. In a similar vein, I added 3-letter substitutes for urls, emojis, etc., and tried to keep them rsr balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_transformed)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0, 1, 2, 3, 4, 5]\n",
       "Index: []"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.loc[:,3].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torrents.ru ?????  ? ??? ???????? ???????? ????????? ? &quot;????????!&quot;'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[50264] # yikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www.quality-rx.com/?fid=3498  '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[102286] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 76.    , 143.    ,  27.    ,  15.    ,  10.0995,   0.6667])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed[50264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.    ,  3.    ,  3.    ,  3.    ,  0.    ,  0.6667])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed[102286]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5298 (+/- 0.0047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(NB_clf, X_transformed, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    3.5s remaining:    2.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5897 (+/- 0.0049)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(log_clf, X_transformed, y_array, cv=10, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
