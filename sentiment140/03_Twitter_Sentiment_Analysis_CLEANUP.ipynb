{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis \n",
    "\n",
    "### Part 1: sentiment140 dataset cleanup\n",
    "\n",
    "The code was originally inspired Gaurav Singhal's guide: [Building a Twitter Setiment Analysis in Python.](https://www.pluralsight.com/guides/building-a-twitter-sentiment-analysis-in-python)\n",
    "\n",
    "The data comes from Marios Michailidis' sentiment140 dataset hosted in [Kaggle.](https://www.kaggle.com/kazanova/sentiment140/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All details of cleanup steps can be found in the custom python script `cleanup_module.py`. \n",
    "\n",
    "Michailidis' dataset consists of 1.6 M rows evenly split into negative and positive Tweets. The labels were created automatically using emoticons - the specific steps of how this was accomplished have not been disclosed.\n",
    "\n",
    "Since my cleanup function entails heavy CPU-bound processes I use multiprocessing, splitting the data into 32 50k-row chunks which are processed 8 at a time (my laptop has 8 logical processors). The order of processing is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cleaned up train dataset: 3\n",
      "Saving cleaned up train dataset: 1\n",
      "Saving cleaned up train dataset: 6\n",
      "Saving cleaned up train dataset: 8\n",
      "Saving cleaned up train dataset: 4\n",
      "Saving cleaned up train dataset: 5\n",
      "Saving cleaned up train dataset: 7\n",
      "Saving cleaned up train dataset: 2\n",
      "Saving cleaned up train dataset: 9\n",
      "Saving cleaned up train dataset: 12\n",
      "Saving cleaned up train dataset: 11\n",
      "Saving cleaned up train dataset: 14\n",
      "Saving cleaned up train dataset: 16\n",
      "Saving cleaned up train dataset: 13\n",
      "Saving cleaned up train dataset: 10\n",
      "Saving cleaned up train dataset: 15\n",
      "Saving cleaned up train dataset: 18\n",
      "Saving cleaned up train dataset: 19\n",
      "Saving cleaned up train dataset: 17\n",
      "Saving cleaned up train dataset: 23\n",
      "Saving cleaned up train dataset: 20\n",
      "Saving cleaned up train dataset: 21\n",
      "Saving cleaned up train dataset: 24\n",
      "Saving cleaned up train dataset: 22\n",
      "Saving cleaned up train dataset: 26\n",
      "Saving cleaned up train dataset: 25\n",
      "Saving cleaned up train dataset: 27\n",
      "Saving cleaned up train dataset: 30\n",
      "Saving cleaned up train dataset: 28\n",
      "Saving cleaned up train dataset: 29\n",
      "Saving cleaned up train dataset: 31\n",
      "Saving cleaned up train dataset: 32\n",
      "Finished in 407.63 second(s)\n"
     ]
    }
   ],
   "source": [
    "cmd = 'python cleanup_module.py'\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverting back to the original data (which includes Tweet IDs, etc.) from the cleaned data:\n",
    "\n",
    "- the key is the list of parameters passed to the multiprocessing executor; i.e. - this last set of parameters indicates that the cleaned dataset 32 contains the range from 1550000 to 1600000:\n",
    "\n",
    "```\n",
    "(range(1550000, 1600001), 32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dirpath = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "filename = \"training.1600000.processed.noemoticon.csv\"\n",
    "filepath = os.path.join(dirpath, filename)\n",
    "\n",
    "df = pd.read_csv(filepath,\n",
    "                 encoding='latin-1',\n",
    "                 usecols=[0,5])\n",
    "\n",
    "df.columns = ['target','text']\n",
    "              \n",
    "dirpath = os.path.join(\"..\",\"data\",\"2_clean\",\"sentiment140\")\n",
    "filename = \"train_32.csv\"\n",
    "filepath = os.path.join(dirpath, filename)\n",
    "\n",
    "df_clean =  pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1550401</th>\n",
       "      <td>4</td>\n",
       "      <td>Going to see Ghosts of Girlfriends Past with @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550402</th>\n",
       "      <td>4</td>\n",
       "      <td>@sarah_cawood can't wait to see the movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550403</th>\n",
       "      <td>4</td>\n",
       "      <td>@ScottHuska rock the boat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550404</th>\n",
       "      <td>4</td>\n",
       "      <td>@DooneyStudio me and the remaining web develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550405</th>\n",
       "      <td>4</td>\n",
       "      <td>Wooh powergun Haha washing away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550406</th>\n",
       "      <td>4</td>\n",
       "      <td>@mileycyrus NO MILEY IM NOT VOTING FOR YOU &amp;gt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "1550401       4  Going to see Ghosts of Girlfriends Past with @...\n",
       "1550402       4  @sarah_cawood can't wait to see the movie, it ...\n",
       "1550403       4                         @ScottHuska rock the boat \n",
       "1550404       4  @DooneyStudio me and the remaining web develop...\n",
       "1550405       4                   Wooh powergun Haha washing away \n",
       "1550406       4  @mileycyrus NO MILEY IM NOT VOTING FOR YOU &gt..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1550401:1550406,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>filtered</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1</td>\n",
       "      <td>Going to see Ghosts of Girlfriends Past with @...</td>\n",
       "      <td>going to see ghosts of girlfriends past with d...</td>\n",
       "      <td>going see ghosts girlfriends past danii245</td>\n",
       "      <td>go see ghost girlfriend past danii245</td>\n",
       "      <td>going see ghost girlfriend past danii245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1</td>\n",
       "      <td>@sarah_cawood can't wait to see the movie, it ...</td>\n",
       "      <td>sarahcawood cant wait to see the movie it look...</td>\n",
       "      <td>sarahcawood cant wait see movie looks so good</td>\n",
       "      <td>sarahcawood cant wait see movi look so good</td>\n",
       "      <td>sarahcawood cant wait see movie look so good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1</td>\n",
       "      <td>@ScottHuska rock the boat</td>\n",
       "      <td>scotthuska rock the boat</td>\n",
       "      <td>scotthuska rock boat</td>\n",
       "      <td>scotthuska rock boat</td>\n",
       "      <td>scotthuska rock boat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1</td>\n",
       "      <td>@DooneyStudio me and the remaining web develop...</td>\n",
       "      <td>dooneystudio me and the remaining web develope...</td>\n",
       "      <td>dooneystudio me remaining web developer have p...</td>\n",
       "      <td>dooneystudio me remain web develop have plan k...</td>\n",
       "      <td>dooneystudio me remaining web developer have p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1</td>\n",
       "      <td>Wooh powergun Haha washing away</td>\n",
       "      <td>wooh powergun haha washing away</td>\n",
       "      <td>wooh powergun haha washing away</td>\n",
       "      <td>wooh powergun haha wash away</td>\n",
       "      <td>wooh powergun haha washing away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1</td>\n",
       "      <td>@mileycyrus NO MILEY IM NOT VOTING FOR YOU &amp;gt...</td>\n",
       "      <td>mileycyrus no miley im not voting for you hhah...</td>\n",
       "      <td>mileycyrus no miley im not voting you hhahahah...</td>\n",
       "      <td>mileycyru no miley im not vote you hhahahah jo...</td>\n",
       "      <td>mileycyrus no miley im not voting you hhahahah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                               text  \\\n",
       "401       1  Going to see Ghosts of Girlfriends Past with @...   \n",
       "402       1  @sarah_cawood can't wait to see the movie, it ...   \n",
       "403       1                         @ScottHuska rock the boat    \n",
       "404       1  @DooneyStudio me and the remaining web develop...   \n",
       "405       1                   Wooh powergun Haha washing away    \n",
       "406       1  @mileycyrus NO MILEY IM NOT VOTING FOR YOU &gt...   \n",
       "\n",
       "                                             tokenized  \\\n",
       "401  going to see ghosts of girlfriends past with d...   \n",
       "402  sarahcawood cant wait to see the movie it look...   \n",
       "403                           scotthuska rock the boat   \n",
       "404  dooneystudio me and the remaining web develope...   \n",
       "405                    wooh powergun haha washing away   \n",
       "406  mileycyrus no miley im not voting for you hhah...   \n",
       "\n",
       "                                              filtered  \\\n",
       "401         going see ghosts girlfriends past danii245   \n",
       "402      sarahcawood cant wait see movie looks so good   \n",
       "403                               scotthuska rock boat   \n",
       "404  dooneystudio me remaining web developer have p...   \n",
       "405                    wooh powergun haha washing away   \n",
       "406  mileycyrus no miley im not voting you hhahahah...   \n",
       "\n",
       "                                               stemmed  \\\n",
       "401              go see ghost girlfriend past danii245   \n",
       "402        sarahcawood cant wait see movi look so good   \n",
       "403                               scotthuska rock boat   \n",
       "404  dooneystudio me remain web develop have plan k...   \n",
       "405                       wooh powergun haha wash away   \n",
       "406  mileycyru no miley im not vote you hhahahah jo...   \n",
       "\n",
       "                                            lemmatized  \n",
       "401           going see ghost girlfriend past danii245  \n",
       "402       sarahcawood cant wait see movie look so good  \n",
       "403                               scotthuska rock boat  \n",
       "404  dooneystudio me remaining web developer have p...  \n",
       "405                    wooh powergun haha washing away  \n",
       "406  mileycyrus no miley im not voting you hhahahah...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.loc[401:406,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
