{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - POC\n",
    "---\n",
    "\n",
    "## 4. Cleaning Pipeline - streamlined BoW\n",
    "\n",
    "This notebook refines the previous notebook's cleaning pipeline by testing a larger dataset and bringing it via a custom module. Simple baseline Logistic Regression and Naive Bayes models are tested again, showing an improvement in accuracy with more data from approx. 66% accuracy for LR and 70% accuracy for NB with approx. 120 observations to 75% for both models with approx. 12,000 observations. \n",
    "\n",
    "**Next Steps**\n",
    "\n",
    "Since the 5% improvement in accuracy seems low for a dataset that is 100x larger, I suspect these simple models are underfitting and adding more data would not improve accuracy that much. Formal modeling with learning curves and more testing would confirm this. Conversely, we could improve the quality of the data representation by adding N-grams, using TF-IDF vectorization, and projecting into a semantic space with SVD, and we could engineer features to further improve model accuracy, but I will save all these steps for later, after testing out a few more models with this basic Bag of Words (BoW) representation, tweaking our vocabulary size, among other tests.\n",
    "\n",
    "- Note: the BoW representation is explained in more detail in this [Document Term Matrices notebook.](10.extra_Document_Term_Matrices.ipynb)\n",
    "\n",
    "\n",
    "## POC Only - Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cleanup_module_POC as Cmod\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# time notebook\n",
    "start_notebook = time.time()\n",
    "\n",
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample down considerably to X, y sample subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.99, random_state=158)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is to forget about the `_rest` datasets and focus on the X, y small subsets, as if they were the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 11974\n",
      "Target distribution: 0.503\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset size: {len(X):0.0f}')\n",
    "print(f'Target distribution: {sum(y[\"target\"])/len(y):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep indices\n",
    "X.insert(3, 'index', X.index)\n",
    "X.index = range(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into arrays\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = Pipeline([\n",
    "    (\"document_to_wordcount\", Cmod.DocumentToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", Cmod.WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11974x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 105899 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.759, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.745, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.742, total=   0.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.725, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.734, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.754, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.725, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.771, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.750, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.759, total=   0.1s\n",
      "Mean accuracy: 0.7464518976908046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_array, cv=10, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.755, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.740, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.751, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.720, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.736, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.749, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.739, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.762, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.763, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.757, total=   0.0s\n",
      "Mean accuracy: 0.7471206536095385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "NB_clf = MultinomialNB()\n",
    "score = cross_val_score(NB_clf, X_train_transformed, y_array, cv=10, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time: 0 minute(s) and 17 second(s).\n"
     ]
    }
   ],
   "source": [
    "# time notebook\n",
    "mins, secs = divmod(time.time() - start_notebook, 60)\n",
    "print(f'Total running time: {mins:0.0f} minute(s) and {secs:0.0f} second(s).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
