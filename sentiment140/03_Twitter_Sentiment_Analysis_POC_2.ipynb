{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - POC\n",
    "---\n",
    "\n",
    "## 4. Cleaning Pipeline\n",
    "\n",
    "This is an involved part and the beginning of the POC per se. For the final project, I will not be sampling the data. Here I'm using a very small sample so that I can quickly iterate and move foward with the project.\n",
    "\n",
    "\n",
    "## POC Only - Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# time notebook\n",
    "start_notebook = time.time()\n",
    "\n",
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample down considerably to X, y sample subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9999, random_state=158)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is to forget about the `_rest` datasets and focus on the X, y small subsets, as if they were the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 119\n",
      "Target distribution: 0.521\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset size: {len(X):0.0f}')\n",
    "print(f'Target distribution: {sum(y[\"target\"])/len(y):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>848825</th>\n",
       "      <td>1827547913</td>\n",
       "      <td>CarissaCruz</td>\n",
       "      <td>@CassXavier hahaha. yes, i know.  it's good fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147277</th>\n",
       "      <td>2204021385</td>\n",
       "      <td>CarlaCh</td>\n",
       "      <td>@shawnieora Been sad lately. Just found out my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755568</th>\n",
       "      <td>2257571603</td>\n",
       "      <td>Joulez217</td>\n",
       "      <td>@conjunkie ah see my hotel only booked till sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15878</th>\n",
       "      <td>1550708931</td>\n",
       "      <td>himmelgarten</td>\n",
       "      <td>Google thinks the Cafe is a spam blog.  They'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291177</th>\n",
       "      <td>1977512948</td>\n",
       "      <td>murnisitanggang</td>\n",
       "      <td>i love sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116805</th>\n",
       "      <td>1981582888</td>\n",
       "      <td>noodles2007</td>\n",
       "      <td>walkin the zoo. already seen the monkeys and b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401889</th>\n",
       "      <td>1827697408</td>\n",
       "      <td>haylz4000</td>\n",
       "      <td>i'm helping my friend with his maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875770</th>\n",
       "      <td>2177034912</td>\n",
       "      <td>blaisegv</td>\n",
       "      <td>@alisonmichalk My pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424813</th>\n",
       "      <td>1983995946</td>\n",
       "      <td>brycefury</td>\n",
       "      <td>No up today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780553</th>\n",
       "      <td>1823735990</td>\n",
       "      <td>msunique85</td>\n",
       "      <td>@deangeloredman @bmarzmusic feeling neglected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID         username                                              tweet\n",
       "848825   1827547913      CarissaCruz  @CassXavier hahaha. yes, i know.  it's good fo...\n",
       "147277   2204021385          CarlaCh  @shawnieora Been sad lately. Just found out my...\n",
       "755568   2257571603        Joulez217  @conjunkie ah see my hotel only booked till sa...\n",
       "15878    1550708931     himmelgarten  Google thinks the Cafe is a spam blog.  They'r...\n",
       "291177   1977512948  murnisitanggang                                     i love sunday \n",
       "1116805  1981582888      noodles2007  walkin the zoo. already seen the monkeys and b...\n",
       "401889   1827697408        haylz4000              i'm helping my friend with his maths \n",
       "875770   2177034912         blaisegv                        @alisonmichalk My pleasure \n",
       "424813   1983995946        brycefury                                       No up today \n",
       "780553   1823735990       msunique85     @deangeloredman @bmarzmusic feeling neglected "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"@CassXavier hahaha. yes, i know.  it's good for him. and us! ;)\",\n",
       "       '@shawnieora Been sad lately. Just found out my sister has stage 1 colon cancer. I already lost a sister ',\n",
       "       \"@conjunkie ah see my hotel only booked till sat. Sorry  I didn't book my room another friend did who can't do full weekend.\",\n",
       "       \"Google thinks the Cafe is a spam blog.  They're recognised by &quot;irrelevant, repetitive, or nonsensical text&quot;.  That's told me \",\n",
       "       'i love sunday ',\n",
       "       'walkin the zoo. already seen the monkeys and birds and hell of a lot of all there animals.. this is fun ',\n",
       "       \"i'm helping my friend with his maths \",\n",
       "       '@alisonmichalk My pleasure ', 'No up today ',\n",
       "       '@deangeloredman @bmarzmusic feeling neglected '], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an array of Tweets\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "X_array[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lose the indices so we need to retain those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1827547913</td>\n",
       "      <td>CarissaCruz</td>\n",
       "      <td>@CassXavier hahaha. yes, i know.  it's good fo...</td>\n",
       "      <td>848825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2204021385</td>\n",
       "      <td>CarlaCh</td>\n",
       "      <td>@shawnieora Been sad lately. Just found out my...</td>\n",
       "      <td>147277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2257571603</td>\n",
       "      <td>Joulez217</td>\n",
       "      <td>@conjunkie ah see my hotel only booked till sa...</td>\n",
       "      <td>755568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1550708931</td>\n",
       "      <td>himmelgarten</td>\n",
       "      <td>Google thinks the Cafe is a spam blog.  They'r...</td>\n",
       "      <td>15878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1977512948</td>\n",
       "      <td>murnisitanggang</td>\n",
       "      <td>i love sunday</td>\n",
       "      <td>291177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         username                                              tweet   index\n",
       "0  1827547913      CarissaCruz  @CassXavier hahaha. yes, i know.  it's good fo...  848825\n",
       "1  2204021385          CarlaCh  @shawnieora Been sad lately. Just found out my...  147277\n",
       "2  2257571603        Joulez217  @conjunkie ah see my hotel only booked till sa...  755568\n",
       "3  1550708931     himmelgarten  Google thinks the Cafe is a spam blog.  They'r...   15878\n",
       "4  1977512948  murnisitanggang                                     i love sunday   291177"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.insert(3, 'index', X.index)\n",
    "X.index = range(len(X))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urlextract\n",
    "\n",
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "           \n",
    "class DocumentToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_case=True, replace_usernames=True,\n",
    "                 unescape_html=True, replace_urls=True, \n",
    "                 replace_numbers=True, remove_junk=True, \n",
    "                 remove_punctuation=True, replace_emojis=True,\n",
    "                 replace_nonascii=True, tokenize=True, \n",
    "                 remove_stopwords=True, lemmatization=True):\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.tokenize = tokenize\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'@([^\\s]+)',' USERNAME ', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '\n",
    "            word_counts = Counter(doc.split())\n",
    "            if self.remove_stopwords:\n",
    "                #25 semantically non-selective words from the Reuters-RCV1 dataset\n",
    "                # plus single-digit letters\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with','t','s','d','m']\n",
    "                for word in stop_words:\n",
    "                    try:\n",
    "                        word_counts.pop(word)\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                lemmatized_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "                    lemmatized_word_counts[lemmatized_word] += count\n",
    "                word_counts = lemmatized_word_counts      \n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wordcounts = DocumentToWordCounterTransformer(tokenize=False).fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'USERNAME': 1, 'hahaha': 1, 'yes': 1, 'i': 1, 'know': 1, 'good': 1, 'him': 1, 'u': 1})\n",
      "{'sister': 2, 'USERNAME': 1, 'been': 1, 'sad': 1, 'lately': 1, 'just': 1, 'found': 1, 'out': 1, 'my': 1, 'stage': 1, 'NUMBER': 1, 'colon': 1, 'cancer': 1, 'i': 1, 'already': 1, 'lost': 1})\n",
      "{'my': 2, 'USERNAME': 1, 'ah': 1, 'see': 1, 'hotel': 1, 'only': 1, 'booked': 1, 'till': 1, 'sat': 1, 'sorry': 1, 'i': 1, 'didn': 1, 'book': 1, 'room': 1, 'another': 1, 'friend': 1, 'did': 1, 'who': 1, 'can': 1, 'do': 1, 'full': 1, 'weekend': 1})\n",
      "{'google': 1, 'think': 1, 'cafe': 1, 'spam': 1, 'blog': 1, 'they': 1, 're': 1, 'recognised': 1, 'irrelevant': 1, 'repetitive': 1, 'or': 1, 'nonsensical': 1, 'text': 1, 'told': 1, 'me': 1})\n",
      "{'i': 1, 'love': 1, 'sunday': 1})\n",
      "{'walkin': 1, 'zoo': 1, 'already': 1, 'seen': 1, 'monkey': 1, 'bird': 1, 'hell': 1, 'lot': 1, 'all': 1, 'there': 1, 'animal': 1, 'this': 1, 'fun': 1})\n",
      "{'i': 1, 'helping': 1, 'my': 1, 'friend': 1, 'his': 1, 'math': 1})\n",
      "{'USERNAME': 1, 'my': 1, 'pleasure': 1})\n",
      "{'no': 1, 'up': 1, 'today': 1})\n",
      "{'USERNAME': 2, 'feeling': 1, 'neglected': 1})\n"
     ]
    }
   ],
   "source": [
    "for counter in X_wordcounts[:10]:\n",
    "    counter = str(counter).split(\"(\")[1]\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 1187 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer()\n",
    "X_vectors = vocab_transformer.fit_transform(X_wordcounts)\n",
    "X_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2063447346</td>\n",
       "      <td>dearscarlett</td>\n",
       "      <td>I accidentally scratched jeremie and she start...</td>\n",
       "      <td>1058998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1562359688</td>\n",
       "      <td>Mizzgp10</td>\n",
       "      <td>@PALMTREEENT lol I'm okay lol but I'm m@dd cuz...</td>\n",
       "      <td>14605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1881043184</td>\n",
       "      <td>markrfletcher</td>\n",
       "      <td>@SaritaAgerman But still 2 more to go  And one...</td>\n",
       "      <td>2906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID       username                                              tweet    index\n",
       "19  2063447346   dearscarlett  I accidentally scratched jeremie and she start...  1058998\n",
       "20  1562359688       Mizzgp10  @PALMTREEENT lol I'm okay lol but I'm m@dd cuz...    14605\n",
       "21  1881043184  markrfletcher  @SaritaAgerman But still 2 more to go  And one...     2906"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from original data\n",
    "X.loc[19:21,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 [0 1 0 0 0 0 0 0 0 0]\n",
      "20 [0 3 2 0 1 0 0 0 0 0]\n",
      "21 [0 1 1 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(22):\n",
    "    if i > 18:\n",
    "        print(i, X_vectors.toarray()[i][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I accidentally scratched jeremie and she started bleeding.  note to self: clip nails...',\n",
       "       \"@PALMTREEENT lol I'm okay lol but I'm m@dd cuz I don't get 2 meet yall nxt week when yall come down here! \",\n",
       "       '@SaritaAgerman But still 2 more to go  And one of them is MediEVIL. I hate it.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[19:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 i\n",
      "2 USERNAME\n",
      "3 my\n",
      "4 NUMBER\n",
      "5 you\n",
      "6 good\n",
      "7 day\n",
      "8 this\n",
      "9 today\n",
      "10 have\n"
     ]
    }
   ],
   "source": [
    "for k,v in vocab_transformer.vocabulary_.items():\n",
    "    if v < 11:\n",
    "        print(v, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'USERNAME': 2,\n",
       "         'lol': 2,\n",
       "         'i': 3,\n",
       "         'okay': 1,\n",
       "         'but': 1,\n",
       "         'cuz': 1,\n",
       "         'don': 1,\n",
       "         'get': 1,\n",
       "         'NUMBER': 1,\n",
       "         'meet': 1,\n",
       "         'yall': 2,\n",
       "         'nxt': 1,\n",
       "         'week': 1,\n",
       "         'when': 1,\n",
       "         'come': 1,\n",
       "         'down': 1,\n",
       "         'here': 1})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wordcounts[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"document_to_wordcount\", DocumentToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 1187 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.708, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.542, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.708, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.542, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.652, total=   0.0s\n",
      "Mean accuracy: 0.6304347826086957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_array, cv=5, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.875, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.667, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.667, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.667, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.609, total=   0.0s\n",
      "Mean accuracy: 0.6967391304347825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "score = cross_val_score(NB_clf, X_train_transformed, y_array, cv=5, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
