{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - POC\n",
    "---\n",
    "\n",
    "## 4. Cleaning Pipeline\n",
    "\n",
    "This is an involved part and the beginning of the POC per se. For the final project, I will not be sampling the data. Here I'm using a very small sample so that I can quickly iterate and move foward with the project.\n",
    "\n",
    "\n",
    "## POC Only - Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# time notebook\n",
    "start_notebook = time.time()\n",
    "\n",
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample down considerably to X, y sample subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9999, random_state=158)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is to forget about the `_rest` datasets and focus on the X, y small subsets, as if they were the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 119\n",
      "Target distribution: 0.521\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset size: {len(X):0.0f}')\n",
    "print(f'Target distribution: {sum(y[\"target\"])/len(y):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>848825</th>\n",
       "      <td>1827547913</td>\n",
       "      <td>CarissaCruz</td>\n",
       "      <td>@CassXavier hahaha. yes, i know.  it's good fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147277</th>\n",
       "      <td>2204021385</td>\n",
       "      <td>CarlaCh</td>\n",
       "      <td>@shawnieora Been sad lately. Just found out my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755568</th>\n",
       "      <td>2257571603</td>\n",
       "      <td>Joulez217</td>\n",
       "      <td>@conjunkie ah see my hotel only booked till sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15878</th>\n",
       "      <td>1550708931</td>\n",
       "      <td>himmelgarten</td>\n",
       "      <td>Google thinks the Cafe is a spam blog.  They'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291177</th>\n",
       "      <td>1977512948</td>\n",
       "      <td>murnisitanggang</td>\n",
       "      <td>i love sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116805</th>\n",
       "      <td>1981582888</td>\n",
       "      <td>noodles2007</td>\n",
       "      <td>walkin the zoo. already seen the monkeys and b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401889</th>\n",
       "      <td>1827697408</td>\n",
       "      <td>haylz4000</td>\n",
       "      <td>i'm helping my friend with his maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875770</th>\n",
       "      <td>2177034912</td>\n",
       "      <td>blaisegv</td>\n",
       "      <td>@alisonmichalk My pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424813</th>\n",
       "      <td>1983995946</td>\n",
       "      <td>brycefury</td>\n",
       "      <td>No up today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780553</th>\n",
       "      <td>1823735990</td>\n",
       "      <td>msunique85</td>\n",
       "      <td>@deangeloredman @bmarzmusic feeling neglected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID         username  \\\n",
       "848825   1827547913      CarissaCruz   \n",
       "147277   2204021385          CarlaCh   \n",
       "755568   2257571603        Joulez217   \n",
       "15878    1550708931     himmelgarten   \n",
       "291177   1977512948  murnisitanggang   \n",
       "1116805  1981582888      noodles2007   \n",
       "401889   1827697408        haylz4000   \n",
       "875770   2177034912         blaisegv   \n",
       "424813   1983995946        brycefury   \n",
       "780553   1823735990       msunique85   \n",
       "\n",
       "                                                     tweet  \n",
       "848825   @CassXavier hahaha. yes, i know.  it's good fo...  \n",
       "147277   @shawnieora Been sad lately. Just found out my...  \n",
       "755568   @conjunkie ah see my hotel only booked till sa...  \n",
       "15878    Google thinks the Cafe is a spam blog.  They'r...  \n",
       "291177                                      i love sunday   \n",
       "1116805  walkin the zoo. already seen the monkeys and b...  \n",
       "401889               i'm helping my friend with his maths   \n",
       "875770                         @alisonmichalk My pleasure   \n",
       "424813                                        No up today   \n",
       "780553      @deangeloredman @bmarzmusic feeling neglected   "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : @CassXavier hahaha. yes, i know.  it's good for him. and us! ;)\n",
      "2 : @shawnieora Been sad lately. Just found out my sister has stage 1 colon cancer. I already lost a sister \n",
      "3 : @conjunkie ah see my hotel only booked till sat. Sorry  I didn't book my room another friend did who can't do full weekend.\n",
      "4 : Google thinks the Cafe is a spam blog.  They're recognised by &quot;irrelevant, repetitive, or nonsensical text&quot;.  That's told me \n",
      "5 : i love sunday \n",
      "6 : walkin the zoo. already seen the monkeys and birds and hell of a lot of all there animals.. this is fun \n",
      "7 : i'm helping my friend with his maths \n",
      "8 : @alisonmichalk My pleasure \n",
      "9 : No up today \n",
      "10 : @deangeloredman @bmarzmusic feeling neglected \n"
     ]
    }
   ],
   "source": [
    "# create an array of Tweets\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "for i, doc in enumerate(X_array[:10]):\n",
    "    print(i+1, \":\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urlextract\n",
    "\n",
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "           \n",
    "class DocumentToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_case=True, replace_usernames=True,\n",
    "                 unescape_html=True, replace_urls=True, \n",
    "                 replace_numbers=True, remove_junk=True, \n",
    "                 remove_punctuation=True, replace_emojis=True,\n",
    "                 replace_nonascii=True, tokenize=True, \n",
    "                 remove_stopwords=True, lemmatization=True):\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.tokenize = tokenize\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'@([^\\s]+)','USERNAME', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '\n",
    "            word_counts = Counter(doc.split())\n",
    "            if self.remove_stopwords:\n",
    "                #25 semantically non-selective words from the Reuters-RCV1 dataset\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with']\n",
    "                for word in stop_words:\n",
    "                    try:\n",
    "                        word_counts.pop(word)\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                lemmatized_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "                    lemmatized_word_counts[lemmatized_word] += count\n",
    "                word_counts = lemmatized_word_counts      \n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wordcounts = DocumentToWordCounterTransformer(tokenize=False).fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'USERNAME': 1, 'hahaha': 1, 'yes': 1, 'i': 1, 'know': 1, 's': 1, 'good': 1, 'him': 1, 'u': 1})\n",
      "{'sister': 2, 'USERNAME': 1, 'been': 1, 'sad': 1, 'lately': 1, 'just': 1, 'found': 1, 'out': 1, 'my': 1, 'stage': 1, 'NUMBER': 1, 'colon': 1, 'cancer': 1, 'i': 1, 'already': 1, 'lost': 1})\n",
      "{'my': 2, 't': 2, 'USERNAME': 1, 'ah': 1, 'see': 1, 'hotel': 1, 'only': 1, 'booked': 1, 'till': 1, 'sat': 1, 'sorry': 1, 'i': 1, 'didn': 1, 'book': 1, 'room': 1, 'another': 1, 'friend': 1, 'did': 1, 'who': 1, 'can': 1, 'do': 1, 'full': 1, 'weekend': 1})\n",
      "{'google': 1, 'think': 1, 'cafe': 1, 'spam': 1, 'blog': 1, 'they': 1, 're': 1, 'recognised': 1, 'irrelevant': 1, 'repetitive': 1, 'or': 1, 'nonsensical': 1, 'text': 1, 's': 1, 'told': 1, 'me': 1})\n",
      "{'i': 1, 'love': 1, 'sunday': 1})\n",
      "{'walkin': 1, 'zoo': 1, 'already': 1, 'seen': 1, 'monkey': 1, 'bird': 1, 'hell': 1, 'lot': 1, 'all': 1, 'there': 1, 'animal': 1, 'this': 1, 'fun': 1})\n",
      "{'i': 1, 'm': 1, 'helping': 1, 'my': 1, 'friend': 1, 'his': 1, 'math': 1})\n",
      "{'USERNAME': 1, 'my': 1, 'pleasure': 1})\n",
      "{'no': 1, 'up': 1, 'today': 1})\n",
      "{'USERNAME': 2, 'feeling': 1, 'neglected': 1})\n"
     ]
    }
   ],
   "source": [
    "for counter in X_wordcounts[:10]:\n",
    "    counter = str(counter).split(\"(\")[1]\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word counts are rarely above 1... so that is a weird problem. We are unlikely to reduce our feature space like A. Geron did with emails by selecting high counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
