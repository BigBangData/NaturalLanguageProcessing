{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - part 2\n",
    "\n",
    "*Pupose*\n",
    "\n",
    "To test importing the new `feature_engineering` module and start exploring the addition of these new features using a baseline Logistic Regression model.\n",
    "\n",
    "\n",
    "*Results*\n",
    "\n",
    "- Stacking these 7 new features onto the 100,000 feature space of our best LR model so far did not improve accuracy: they just get lost.\n",
    "- Using SVD to generate a matrix of 1,000 features and stacking the new features onto this semantic space yields even worse results.\n",
    "- A hypothesis is that a simple Logistic Regression model is not the best way to validate these new representations.\n",
    "\n",
    "*Next Steps*\n",
    "\n",
    "- A possible next step is to visualize the engineered features to try to understand why they are performing above average alone but degrade accuracy when combined with the document-term TF-IDF matrix or SVD semantic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import feature_engineering as Fe\n",
    "\n",
    "import urlextract\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC: sample $10\\%$ of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample 10%\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# create arrays\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119747,), (119747,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape, y_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load contractions map\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "# instantiate url extractor and lemmatizer\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0 m 30 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    clean_docs, X_transformed = Fe.DocumentToFeaturesCounterTransformer().fit_transform(X_array)\n",
    "except RuntimeWarning:\n",
    "    pass\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} m {secs:0.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@trib Lol... winter solstice has it's beauty as well, you get to visit with the ice goddess &amp; it's my b'day  (nearly)\"\n",
      " '@MetHome Aww, I would love to vote!  Somehow, the link is broken ']\n"
     ]
    }
   ],
   "source": [
    "print(X_array[119737:119739])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usr lol winter solstice has it is beauty as well you get to visit with the ice goddess it is my bday nearly',\n",
       " 'usr aww i would love to vote somehow the link is broken']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_docs[119737:119739]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[121.     107.      23.       8.       3.6957   1.7554   0.4118]\n",
      " [ 65.      55.      12.       7.       3.6667   1.6499   0.5   ]]\n"
     ]
    }
   ],
   "source": [
    "#dlen_raw  dlen_cln n_tokns tkn_maxL tkn_meanL tkn_stdL rsr_\n",
    "print(X_transformed[119737:119739])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([('std_scaler', StandardScaler()), \n",
    "                 ('log_reg', LogisticRegression(solver=\"liblinear\", random_state=42))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pipeline just to scale then perform cross validation with a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = pipe['std_scaler'].fit_transform(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5962 (+/- 0.0039)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "score = cross_val_score(log_clf, X_scaled, y_array, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the full pipeline and predicting once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_array, test_size=0.33, random_state=42)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_preds = pipe.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_preds):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dotument-Term Matrix plus engineered features\n",
    "\n",
    "Trigrams with `vocab_size=100000` for best speed and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanup_module as Cmod\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "dtm_pipe = Pipeline([('counter', Cmod.DocumentToNgramCounterTransformer(n_grams=3)),\n",
    "                     ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=100000)),\n",
    "                     ('tfidf', TfidfTransformer(sublinear_tf=True, use_idf=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0 m 57 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X_transformed_dtm = dtm_pipe.fit_transform(X_array)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} m {secs:0.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x100001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2687589 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8030 (+/- 0.0017)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "score = cross_val_score(log_clf, X_transformed_dtm, y_array, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Status**\n",
    "\n",
    "- We have a (119747, 7) dense numpy array and a <119747x100001 sparse matrix. \n",
    "- The former gets 0.5962 (+/- 0.0039) accuracy and the latter 0.8030 (+/- 0.0017). \n",
    "- *What will combining them result in?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "X_stacked = sp.hstack((X_scaled, X_transformed_dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x100008 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3525818 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled vs Unscaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8027 (+/- 0.0020)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "score = cross_val_score(log_clf, X_stacked, y_array, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8027 (+/- 0.0020)\n"
     ]
    }
   ],
   "source": [
    "X_stacked_unscaled = sp.hstack((X_transformed, X_transformed_dtm))\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "score = cross_val_score(log_clf, X_stacked_unscaled, y_array, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Results*\n",
    "\n",
    "- Scalding doesn't matter\n",
    "- Stacking doesn't improve accuracy\n",
    "        * It's likely that the signal from the seven new features gets lost in the 100k features\n",
    "\n",
    "*Next Steps*\n",
    "\n",
    "- What about using SVD for dimentionality reduction and then stacking the new features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD plus New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 12 min 43 sec\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "\n",
    "start_time = time.time()\n",
    "U, Sigma, VT = svds(X_transformed_dtm.T, # transposed to a term-document matrix\n",
    "                    k=1000) # k = number of components / \"topics\"\n",
    "    \n",
    "# reverse outputs\n",
    "Sigma = Sigma[::-1]\n",
    "U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100001, 1000), (1000,), (1000, 119747))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, Sigma.shape, VT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119747, 1000), (119747,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = VT.T\n",
    "V.shape, y_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x1007 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 120585229 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to sparse matrix\n",
    "V_sparse = sp.csr_matrix(V)\n",
    "X_scaled_sparse = sp.csr_matrix(X_scaled)\n",
    "\n",
    "# stack\n",
    "V_stacked = sp.hstack((V_sparse, X_scaled_sparse))\n",
    "V_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVD alone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7743 (+/- 0.0031)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "score = cross_val_score(log_clf, V_sparse, y_array, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVD plus new features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6919 (+/- 0.0032)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "score = cross_val_score(log_clf, V_stacked, y_array, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about just adding raw document length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_stacked = sp.hstack((V_sparse, X_transformed[:,0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7733 (+/- 0.0026)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "score = cross_val_score(log_clf, V_stacked, y_array, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just clean document length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7578 (+/- 0.0028)\n"
     ]
    }
   ],
   "source": [
    "V_stacked = sp.hstack((V_sparse, X_transformed[:,1:2]))\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, V_stacked, y_array, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just number of tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7425 (+/- 0.0035)\n"
     ]
    }
   ],
   "source": [
    "V_stacked = sp.hstack((V_sparse, X_transformed[:,2:3]))\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, V_stacked, y_array, cv=10, verbose=0, scoring='accuracy', n_jobs=-1)\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Final Results*\n",
    "\n",
    "- Adding the new features to SVD also degrade its accuracy.\n",
    "- It's possible that a more complex algorithm such as Random Forests might yield different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
