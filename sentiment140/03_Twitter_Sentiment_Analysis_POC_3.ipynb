{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - POC\n",
    "---\n",
    "\n",
    "## 4. Cleaning Pipeline\n",
    "\n",
    "This is an involved part and the beginning of the POC per se. For the final project, I will not be sampling the data. Here I'm using a very small sample so that I can quickly iterate and move foward with the project.\n",
    "\n",
    "\n",
    "## POC Only - Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# time notebook\n",
    "start_notebook = time.time()\n",
    "\n",
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample down considerably to X, y sample subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=158)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is to forget about the `_rest` datasets and focus on the X, y small subsets, as if they were the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 119747\n",
      "Target distribution: 0.501\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset size: {len(X):0.0f}')\n",
    "print(f'Target distribution: {sum(y[\"target\"])/len(y):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119747"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an array of Tweets\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "len(X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lose the indices so we need to retain those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.insert(3, 'index', X.index)\n",
    "X.index = range(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urlextract\n",
    "\n",
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "           \n",
    "class DocumentToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_case=True, replace_usernames=True,\n",
    "                 unescape_html=True, replace_urls=True, \n",
    "                 replace_numbers=True, remove_junk=True, \n",
    "                 remove_punctuation=True, replace_emojis=True,\n",
    "                 replace_nonascii=True, tokenize=True, \n",
    "                 remove_stopwords=True, lemmatization=True):\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.tokenize = tokenize\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'@([^\\s]+)',' USERNAME ', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '\n",
    "            word_counts = Counter(doc.split())\n",
    "            if self.remove_stopwords:\n",
    "                #25 semantically non-selective words from the Reuters-RCV1 dataset\n",
    "                # plus single-digit letters\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with','t','s','d','m']\n",
    "                for word in stop_words:\n",
    "                    try:\n",
    "                        word_counts.pop(word)\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                lemmatized_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "                    lemmatized_word_counts[lemmatized_word] += count\n",
    "                word_counts = lemmatized_word_counts      \n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wordcounts = DocumentToWordCounterTransformer(tokenize=False).fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119747"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 1040315 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer()\n",
    "X_vectors = vocab_transformer.fit_transform(X_wordcounts)\n",
    "X_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 i\n",
      "2 USERNAME\n",
      "3 NUMBER\n",
      "4 my\n",
      "5 you\n",
      "6 me\n",
      "7 so\n",
      "8 have\n",
      "9 but\n",
      "10 just\n"
     ]
    }
   ],
   "source": [
    "for k,v in vocab_transformer.vocabulary_.items():\n",
    "    if v < 11:\n",
    "        print(v, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"document_to_wordcount\", DocumentToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 1040315 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119747"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.766, total=   1.6s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.759, total=   1.7s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.766, total=   1.6s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.762, total=   1.7s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.767, total=   1.7s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.762, total=   1.6s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.766, total=   1.5s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.763, total=   1.7s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.757, total=   1.7s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.766, total=   1.5s\n",
      "Mean accuracy: 0.7633844324498487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   16.4s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_array, cv=10, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.755, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.749, total=   0.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.755, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.751, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.754, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.750, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.754, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.747, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.745, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.754, total=   0.1s\n",
      "Mean accuracy: 0.7513423440418749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "score = cross_val_score(NB_clf, X_train_transformed, y_array, cv=10, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time: 1 minute(s) and 47 second(s).\n"
     ]
    }
   ],
   "source": [
    "# time notebook\n",
    "mins, secs = divmod(time.time() - start_notebook, 60)\n",
    "print(f'Total running time: {mins:0.0f} minute(s) and {secs:0.0f} second(s).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
