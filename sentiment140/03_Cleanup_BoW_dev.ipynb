{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Pipeline 1\n",
    "\n",
    "This notebook is for developing the cleanup pipeline, not implementing it. Implementation is done via the `cleanup_module.py` which is imported into a notebook or script. \n",
    "\n",
    "Preprocessing is performed for a simple Bag-of-Words representation. Text to DFM representations are explained in more detail in this [Document Term Matrices notebook.](10.extra_Document_Term_Matrices.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urlextract\n",
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1197471, 3), (1197471, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample down\n",
    "\n",
    "Since dealing with the entire training data is time consuming and unnecessary for developing a solution, I'm sampling down considerably here, from ~1.2M to 10k instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9915, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10178\n",
      "Target distribution: 0.502\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset size: {len(X):0.0f}')\n",
    "print(f'Target distribution: {sum(y[\"target\"])/len(y):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>746256</th>\n",
       "      <td>1962093226</td>\n",
       "      <td>phdbre</td>\n",
       "      <td>@lruettimann promise me you'll stop by CPP's b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055340</th>\n",
       "      <td>2232997122</td>\n",
       "      <td>DUNX</td>\n",
       "      <td>On the rooftop floor. Theres a gym I climbed b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452532</th>\n",
       "      <td>1469846269</td>\n",
       "      <td>joeypiet</td>\n",
       "      <td>new incubus song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717583</th>\n",
       "      <td>1969116195</td>\n",
       "      <td>theitalianjob</td>\n",
       "      <td>@theitalianjob: funkciï¿½kkal szï¿½vjak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23382</th>\n",
       "      <td>2183646936</td>\n",
       "      <td>Yema</td>\n",
       "      <td>Teehee, I just looked up &amp;quot;Yema&amp;quot; in t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID       username  \\\n",
       "746256   1962093226         phdbre   \n",
       "1055340  2232997122           DUNX   \n",
       "452532   1469846269       joeypiet   \n",
       "717583   1969116195  theitalianjob   \n",
       "23382    2183646936           Yema   \n",
       "\n",
       "                                                     tweet  \n",
       "746256   @lruettimann promise me you'll stop by CPP's b...  \n",
       "1055340  On the rooftop floor. Theres a gym I climbed b...  \n",
       "452532                                   new incubus song   \n",
       "717583            @theitalianjob: funkciï¿½kkal szï¿½vjak   \n",
       "23382    Teehee, I just looked up &quot;Yema&quot; in t...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an array out of the tweet column, and that array will not contain the shuffled indices in a randomly sampled training dataset so we capture that in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962093226</td>\n",
       "      <td>phdbre</td>\n",
       "      <td>@lruettimann promise me you'll stop by CPP's b...</td>\n",
       "      <td>746256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2232997122</td>\n",
       "      <td>DUNX</td>\n",
       "      <td>On the rooftop floor. Theres a gym I climbed b...</td>\n",
       "      <td>1055340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1469846269</td>\n",
       "      <td>joeypiet</td>\n",
       "      <td>new incubus song</td>\n",
       "      <td>452532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1969116195</td>\n",
       "      <td>theitalianjob</td>\n",
       "      <td>@theitalianjob: funkciï¿½kkal szï¿½vjak</td>\n",
       "      <td>717583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2183646936</td>\n",
       "      <td>Yema</td>\n",
       "      <td>Teehee, I just looked up &amp;quot;Yema&amp;quot; in t...</td>\n",
       "      <td>23382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID       username  \\\n",
       "0  1962093226         phdbre   \n",
       "1  2232997122           DUNX   \n",
       "2  1469846269       joeypiet   \n",
       "3  1969116195  theitalianjob   \n",
       "4  2183646936           Yema   \n",
       "\n",
       "                                               tweet    index  \n",
       "0  @lruettimann promise me you'll stop by CPP's b...   746256  \n",
       "1  On the rooftop floor. Theres a gym I climbed b...  1055340  \n",
       "2                                  new incubus song    452532  \n",
       "3           @theitalianjob: funkciï¿½kkal szï¿½vjak    717583  \n",
       "4  Teehee, I just looked up &quot;Yema&quot; in t...    23382  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.insert(3, 'index', X.index)\n",
    "X.index = range(len(X))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"@lruettimann promise me you'll stop by CPP's booth this year at ASTD!  \",\n",
       "       \"On the rooftop floor. Theres a gym I climbed but sakurity wouldnt let me go down the slide.  shit's craaazy!\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an array of Tweets\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "X_array[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>746256</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055340</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target\n",
       "746256        1\n",
       "1055340       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "The contraction map and `expand_contractions` function were adapted from this [KDnuggests article.](https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html)\n",
    "\n",
    "The **DocumentToWordCounterTransformer** class was inspired by Aurelien Geron's **EmailToWordCounterTransformer** class from his famous [classification notebook](https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb), and the **WordCounterToVectorTransformer** is a straight copy of Geron's same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contractions_map):\n",
    "    \n",
    "    pattern = re.compile('({})'.format('|'.join(contractions_map.keys())), \n",
    "                        flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_map.get(match)\\\n",
    "                                if contractions_map.get(match)\\\n",
    "                                else contractions_map.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "           \n",
    "class DocumentToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, expand_contractions=True, lower_case=True, \n",
    "                 replace_usernames=True, unescape_html=True, \n",
    "                 replace_urls=True, replace_numbers=True, \n",
    "                 remove_junk=True, remove_punctuation=True, \n",
    "                 replace_emojis=True, replace_nonascii=True, \n",
    "                 remove_stopwords=True, lemmatization=True):\n",
    "        self.expand_contractions = expand_contractions\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.expand_contractions and contractions_map is not None:\n",
    "                doc = expand_contractions(doc, contractions_map)\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'^@([^\\s]+)',' USERNAME ', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '\n",
    "            word_counts = Counter(doc.split())\n",
    "            if self.remove_stopwords:\n",
    "                # 25 semantically non-selective words from the Reuters-RCV1 dataset\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with']\n",
    "                for word in stop_words:\n",
    "                    try:\n",
    "                        word_counts.pop(word)\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                lemmatized_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "                    lemmatized_word_counts[lemmatized_word] += count\n",
    "                word_counts = lemmatized_word_counts      \n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wordcounts = DocumentToWordCounterTransformer().fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter 13 => {'USERNAME': 1, 'no': 1, 'question': 1, 'nothing': 1, 'can': 1, 'beat': 1, 'firefox': 1})\n",
      "\n",
      "Counter 14 => {'me': 2, 'USERNAME': 1, 'hiya': 1, 'maybe': 1, 'you': 1, 'able': 1, 'enlighten': 1, 'why': 1, 'NUMBER': 1, 'first': 1, 'song': 1, 'rule': 1, 'concert': 1, 'photographer': 1, 'like': 1, 'wt': 1})\n",
      "\n",
      "Counter 15 => {'am': 2, 'going': 2, 'look': 1, 'like': 1, 'my': 1, 'kizzy': 1, 'okay': 1, 'so': 1, 'mighty': 1, 'relieved': 1, 'missing': 1, 'her': 1, 'though': 1, 'exhausted': 1, 'after': 1, 'work': 1, 'feeding': 1, 'others': 1, 'then': 1, 'bed': 1})\n",
      "\n",
      "Counter 16 => {'congrats': 1, 'all': 1, 'newest': 1, 'linfield': 1, 'wildcat': 1, 'alumnus': 1})\n",
      "\n",
      "Counter 17 => {'USERNAME': 1, 'bte': 1, 'nola': 1, 'tonight': 1})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ix, counter in enumerate(X_wordcounts[13:18]):\n",
    "    counter = str(counter).split(\"(\")[1]\n",
    "    print(f'Counter {ix+13:0.0f} => {counter}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10178x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 89988 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer()\n",
    "X_vectors = vocab_transformer.fit_transform(X_wordcounts) \n",
    "X_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15    looks like my Kizzy will be okay, so am mighty relieved, missing her though  Am exhausted after work. Going feeding others then going bed.\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# from original data\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(X.loc[15:15, 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 0 0 0 1 0 2 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_vectors.toarray()[15][0:11]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5 words are not in the vocabulary\n",
    "- the first 1 is **my** (see below), \"looks like *my* Kizzy\"\n",
    "- the 2 is **am**, \"so *am* mighty... *Am* exhausted\"\n",
    "- the last 1 is **so** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 i\n",
      "2 USERNAME\n",
      "3 NUMBER\n",
      "4 you\n",
      "5 my\n",
      "6 not\n",
      "7 am\n",
      "8 have\n",
      "9 me\n",
      "10 so\n"
     ]
    }
   ],
   "source": [
    "for k,v in vocab_transformer.vocabulary_.items():\n",
    "    if v < 11:\n",
    "        print(v, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['looks like my Kizzy will be okay, so am mighty relieved, missing her though  Am exhausted after work. Going feeding others then going bed.']\n"
     ]
    }
   ],
   "source": [
    "# array version\n",
    "print(X_array[15:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'am': 2, 'going': 2, 'look': 1, 'like': 1, 'my': 1, 'kizzy': 1, 'okay': 1, 'so': 1, 'mighty': 1, 'relieved': 1, 'missing': 1, 'her': 1, 'though': 1, 'exhausted': 1, 'after': 1, 'work': 1, 'feeding': 1, 'others': 1, 'then': 1, 'bed': 1})\n"
     ]
    }
   ],
   "source": [
    "# word counter version\n",
    "print(X_wordcounts[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"document_to_wordcount\", DocumentToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10178x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 89988 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train couple baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.733, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.745, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.727, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.737, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.743, total=   0.0s\n",
      "Mean accuracy: 0.7371788398507456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "score = cross_val_score(NB_clf, X_train_transformed, y_array, cv=5, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.737, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.741, total=   0.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.732, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.730, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.742, total=   0.1s\n",
      "Mean accuracy: 0.7361956526985998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_array, cv=5, verbose=3, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
