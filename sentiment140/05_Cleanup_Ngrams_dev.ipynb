{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Pipeline 3\n",
    "\n",
    "Adding bigrams to custom **DocumentToWordCounterTransformer** class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urlextract\n",
    "from nltk import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import cleanup_module as Cmod\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def expand_contractions(text, contractions_map):\n",
    "    \n",
    "    pattern = re.compile('({})'.format('|'.join(contractions_map.keys())), \n",
    "                        flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_map.get(match)\\\n",
    "                                if contractions_map.get(match)\\\n",
    "                                else contractions_map.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, expand_contractions=True, lower_case=True, \n",
    "                 replace_usernames=True, unescape_html=True, \n",
    "                 replace_urls=True, replace_numbers=True, \n",
    "                 remove_junk=True, remove_punctuation=True, \n",
    "                 replace_emojis=True, replace_nonascii=True, \n",
    "                 remove_stopwords=True, lemmatization=True,\n",
    "                 bigrams=True):\n",
    "        self.expand_contractions = expand_contractions\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_junk = remove_junk\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_emojis = replace_emojis\n",
    "        self.replace_nonascii = replace_nonascii\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "        self.bigrams = bigrams\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()\n",
    "            if self.expand_contractions and contractions_map is not None:\n",
    "                doc = expand_contractions(doc, contractions_map)\n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'^@([^\\s]+)',' USERNAME ', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '\n",
    "            # tokenize\n",
    "            tokens = doc.split()\n",
    "            if self.remove_stopwords:\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with']\n",
    "                tokens = [t for t in tokens if t not in stop_words]\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                unigrams = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            if self.bigrams:\n",
    "                bigrams = ngrams(word_tokenize(doc), 2)\n",
    "                bigrams = ['_'.join(grams) for grams in bigrams]\n",
    "                tokens = [*tokens, *bigrams]       \n",
    "            # include counts\n",
    "            tokens_counts = Counter(tokens)\n",
    "            # append to list\n",
    "            X_transformed.append(tokens_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['You love me', \n",
    "          'You do not love me',\n",
    "          'You really really love food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = DocumentToWordCounterTransformer()\n",
    "X_trans = wordvec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'you': 1, 'love': 1, 'me': 1, 'you_love': 1, 'love_me': 1}),\n",
       "       Counter({'you': 1, 'do': 1, 'not': 1, 'love': 1, 'me': 1, 'you_do': 1, 'do_not': 1, 'not_love': 1, 'love_me': 1}),\n",
       "       Counter({'really': 2, 'you': 1, 'love': 1, 'food': 1, 'you_really': 1, 'really_really': 1, 'really_love': 1, 'love_food': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', DocumentToWordCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=20)),\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_counter = pipe['counter'].fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bow = pipe['bow'].fit(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 1,\n",
       " 'love': 2,\n",
       " 'me': 3,\n",
       " 'love_me': 4,\n",
       " 'really': 5,\n",
       " 'you_love': 6,\n",
       " 'do': 7,\n",
       " 'not': 8,\n",
       " 'you_do': 9,\n",
       " 'do_not': 10,\n",
       " 'not_love': 11,\n",
       " 'food': 12,\n",
       " 'you_really': 13,\n",
       " 'really_really': 14,\n",
       " 'really_love': 15,\n",
       " 'love_food': 16}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pipe_bow.transform(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray() # first col is \"words missing from vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.288,\n",
       " 1.288,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 1.693,\n",
       " 2.386,\n",
       " 2.386,\n",
       " 2.386,\n",
       " 2.386]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # IDF for the pipe_bow.vocabulary_\n",
    "[np.around(x,3) for x in pipe['tfidf'].fit(bow).idf_[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipe['tfidf'].fit_transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.3496, 0.3496, 0.4501, 0.4501, 0.    , 0.5919, 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.2256, 0.2256, 0.2905, 0.2905, 0.    , 0.    , 0.382 ,\n",
       "        0.382 , 0.382 , 0.382 , 0.382 , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.1897, 0.1897, 0.    , 0.    , 0.6422, 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.3211, 0.3211, 0.3211, 0.3211,\n",
       "        0.3211, 0.    , 0.    , 0.    , 0.    ])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,4) for x in tfidf.toarray()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.3496, 0.3496, 0.4501, 0.4501, 0.    , 0.5919, 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.2256, 0.2256, 0.2905, 0.2905, 0.    , 0.    , 0.382 ,\n",
       "        0.382 , 0.382 , 0.382 , 0.382 , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.    ]),\n",
       " array([0.    , 0.1897, 0.1897, 0.    , 0.    , 0.6422, 0.    , 0.    ,\n",
       "        0.    , 0.    , 0.    , 0.    , 0.3211, 0.3211, 0.3211, 0.3211,\n",
       "        0.3211, 0.    , 0.    , 0.    , 0.    ])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entire pipeline produces same result but does't save IDF or vocab\n",
    "end_res = pipe.fit_transform(corpus)\n",
    "[np.around(x,4) for x in end_res.toarray()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC: sample 10% of the training data\n",
    "\n",
    "About 120,000 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample 10%\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# create array\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119747,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipe\n",
    "pipe = Pipeline([('counter', DocumentToWordCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=1000)),\n",
    "                 ('tfidf', TfidfTransformer())])\n",
    "\n",
    "X_end = pipe.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x1001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1279478 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.636, 0.117, 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.727, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.816, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.706, 0.   , 0.085, 0.   , 0.   , 0.   ]),\n",
       " array([0.728, 0.1  , 0.049, 0.078, 0.   , 0.   ]),\n",
       " array([0.318, 0.118, 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.568, 0.087, 0.086, 0.138, 0.   , 0.   ]),\n",
       " array([0.594, 0.   , 0.051, 0.   , 0.   , 0.   ]),\n",
       " array([0.872, 0.052, 0.   , 0.   , 0.076, 0.   ]),\n",
       " array([0.583, 0.067, 0.   , 0.106, 0.   , 0.   ])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,3) for x in X_end.toarray()[:10,:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step by step for vocab and idf\n",
    "pipe_counter = pipe['counter'].fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow = pipe['bow'].fit(pipe_counter) \n",
    "pipe_bow.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 1)\n",
      "('USERNAME', 2)\n",
      "('NUMBER', 3)\n",
      "('my', 4)\n",
      "('you', 5)\n",
      "('not', 6)\n",
      "('am', 7)\n",
      "('have', 8)\n",
      "('i_am', 9)\n",
      "('me', 10)\n"
     ]
    }
   ],
   "source": [
    "for ix, w in enumerate(pipe_bow.vocabulary_.items()):\n",
    "    if ix < 10:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pipe_bow.transform(pipe_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  1,  0, ...,  0,  0,  0],\n",
       "       [ 7,  0,  0, ...,  0,  0,  0],\n",
       "       [ 9,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [21,  0,  1, ...,  0,  0,  0],\n",
       "       [31,  1,  0, ...,  0,  0,  0],\n",
       "       [16,  1,  0, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()[:10] # misses too many words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0094, 1.8634, 1.8303, 2.935, 2.7413, 2.866, 2.9798, 3.2305, 3.2467, 3.3431]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,4) for x in pipe['tfidf'].fit(bow).idf_[:10]] # IDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipe['tfidf'].fit_transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x1001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1279478 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.636, 0.117, 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.727, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.816, 0.   , 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.706, 0.   , 0.085, 0.   , 0.   , 0.   ]),\n",
       " array([0.728, 0.1  , 0.049, 0.078, 0.   , 0.   ]),\n",
       " array([0.318, 0.118, 0.   , 0.   , 0.   , 0.   ]),\n",
       " array([0.568, 0.087, 0.086, 0.138, 0.   , 0.   ]),\n",
       " array([0.594, 0.   , 0.051, 0.   , 0.   , 0.   ]),\n",
       " array([0.872, 0.052, 0.   , 0.   , 0.076, 0.   ]),\n",
       " array([0.583, 0.067, 0.   , 0.106, 0.   , 0.   ])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.around(x,3) for x in tfidf.toarray()[:10,:6]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate couple quick models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', DocumentToWordCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=50000)), # more realistic vocabulary size\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pipe['counter'].fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pipe['bow'].fit(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49991 probably_already\n",
      "49992 booked_the\n",
      "49993 gambling\n",
      "49994 hair_lol\n",
      "49995 lol_xoxo\n",
      "49996 so_watching\n",
      "49997 an_age\n",
      "49998 drove_by\n",
      "49999 the_fat\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(X2.vocabulary_):\n",
    "    if i > 49990:\n",
    "        print(i, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = pipe['bow'].fit_transform(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x50001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 2224008 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 # BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = pipe['tfidf'].fit_transform(X3) # Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7885 (+/- 0.0018)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(NB_clf, X3, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7911 (+/- 0.0028)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Tfidf with bigrams\n",
    "score = cross_val_score(NB_clf, X4, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7899 (+/- 0.0023)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "# BoW with bigrams\n",
    "score = cross_val_score(log_clf, X3, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8008 (+/- 0.0025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   18.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Tfidf with bigrams\n",
    "score = cross_val_score(log_clf, X4, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print(f'Accuracy: {score.mean():0.4f} (+/- {np.std(score):0.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
