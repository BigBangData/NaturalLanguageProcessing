{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis \n",
    "\n",
    "---\n",
    "\n",
    "###  Pre-process cleaned data for machine learning \n",
    "\n",
    "### Part 1: Bag of Words\n",
    "\n",
    "While cleanup involved simply reformatting a Tweet's text by standardizing it and reducing the feature space (less punctuation, replacing usernames and URLs, lower casing, tokenizing, lemmatizing, etc.), pre-processing for machine learning is often more involved. It mainly consists of further data cleanup steps such as imputing NAs, but also feature engineering, and perhaps most importantly, a method of representing text in numerical form, such as [Document Term Matrices](./01_Document_Term_Matrices.ipynb), since most machine-learning algorithms do not accept text as input. This notebook explores the creation of a simple Bag of Words Document-Frequency Matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### Load cleaned TRAIN data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# for ML preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# custom (see script)\n",
    "import loading_module as lm\n",
    "\n",
    "start_time = time.time()\n",
    "X_train, y_train = lm.load_clean_data('X_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save target as npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_dir = os.path.join(\"..\",\"data\",\"3_processed\",\"sentiment140\")\n",
    "y_filepath = os.path.join(proc_dir, \"y_train.npy\")\n",
    "\n",
    "with open(y_filepath, 'wb') as f:\n",
    "    np.save(f, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1199999, 3), (1197471, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISSUE:\n",
    "\n",
    "- loading_module is not reproducible!\n",
    "- probably due to async multiprocessing, order is not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for emojis, get `NaNs` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ix = []\n",
    "emoji_ix = []\n",
    "for i, tweet in enumerate(X_train['lemmatized'][:len(X_train)]):\n",
    "    try:\n",
    "        m = re.search(r'EMOJI', tweet)\n",
    "        if m:\n",
    "            emoji_ix.append(i)\n",
    "    except TypeError as e: \n",
    "        error_ix.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174958</th>\n",
       "      <td>dianamra</td>\n",
       "      <td>and it was</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230300</th>\n",
       "      <td>geegeeludlow</td>\n",
       "      <td>is in IT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279405</th>\n",
       "      <td>Jmoux</td>\n",
       "      <td>are on..</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326761</th>\n",
       "      <td>Spacegirlspif13</td>\n",
       "      <td>Is...                                         ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356389</th>\n",
       "      <td>ChickWithAName</td>\n",
       "      <td>. . . . . and it's on!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510320</th>\n",
       "      <td>LukeOgle</td>\n",
       "      <td>is in IT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607035</th>\n",
       "      <td>sangofsorrow</td>\n",
       "      <td>He is...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629899</th>\n",
       "      <td>SquarahFaggins</td>\n",
       "      <td>to it!!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043379</th>\n",
       "      <td>rooroocachoo</td>\n",
       "      <td>It will</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128263</th>\n",
       "      <td>WMonk</td>\n",
       "      <td>It will</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                username                                               text  \\\n",
       "174958          dianamra                                        and it was    \n",
       "230300      geegeeludlow                                          is in IT    \n",
       "279405             Jmoux                                          are on..    \n",
       "326761   Spacegirlspif13  Is...                                         ...   \n",
       "356389    ChickWithAName                            . . . . . and it's on!    \n",
       "510320          LukeOgle                                          is in IT    \n",
       "607035      sangofsorrow                                       He is...       \n",
       "629899    SquarahFaggins                                           to it!!    \n",
       "1043379     rooroocachoo                                           It will    \n",
       "1128263            WMonk                                           It will    \n",
       "\n",
       "        lemmatized  \n",
       "174958         NaN  \n",
       "230300         NaN  \n",
       "279405         NaN  \n",
       "326761         NaN  \n",
       "356389         NaN  \n",
       "510320         NaN  \n",
       "607035         NaN  \n",
       "629899         NaN  \n",
       "1043379        NaN  \n",
       "1128263        NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only a few NaNs\n",
    "X_train.iloc[error_ix, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Applechic</td>\n",
       "      <td>Like this cover a lot!  Yup tiz anzum. @Gregdt...</td>\n",
       "      <td>like this cover lot yup tiz anzum USERNAME i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Applechic</td>\n",
       "      <td>Hope your now unbirthday is good too! @The_Kra...</td>\n",
       "      <td>hope your now unbirthday good too USERNAME EMO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>d_whiteplume</td>\n",
       "      <td>@panda951  no one makes cooler videos than Bj√É...</td>\n",
       "      <td>USERNAME no one make cooler video than bj EMOJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>JustTooBusy</td>\n",
       "      <td>Car boot was a complete wash out - got soaked ...</td>\n",
       "      <td>car boot complete wash out got soaked supposed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>hindy_cindy</td>\n",
       "      <td>I love how down-to-earth Beyonc√É¬É√Ç¬© is. She di...</td>\n",
       "      <td>i love how downtoearth beyonc EMOJI she ditche...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         username                                               text  \\\n",
       "92      Applechic  Like this cover a lot!  Yup tiz anzum. @Gregdt...   \n",
       "171     Applechic  Hope your now unbirthday is good too! @The_Kra...   \n",
       "224  d_whiteplume  @panda951  no one makes cooler videos than Bj√É...   \n",
       "239   JustTooBusy  Car boot was a complete wash out - got soaked ...   \n",
       "253   hindy_cindy  I love how down-to-earth Beyonc√É¬É√Ç¬© is. She di...   \n",
       "\n",
       "                                            lemmatized  \n",
       "92   like this cover lot yup tiz anzum USERNAME i w...  \n",
       "171  hope your now unbirthday good too USERNAME EMO...  \n",
       "224  USERNAME no one make cooler video than bj EMOJ...  \n",
       "239  car boot complete wash out got soaked supposed...  \n",
       "253  i love how downtoearth beyonc EMOJI she ditche...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emojis\n",
    "X_train.iloc[emoji_ix[:5], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10856"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emoji_ix) # could be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute NAs created during cleanup\n",
    "\n",
    "We do not want to drop since the fact they ended up as empty strings is possibly informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username       0\n",
       "text           0\n",
       "lemmatized    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with NULL as a string\n",
    "error_ix = []\n",
    "NULL_ix = []\n",
    "for i, tweet in enumerate(X_train['lemmatized'][:len(X_train)]):\n",
    "    try:\n",
    "        m = re.search(r'NULL', tweet)\n",
    "        if m:\n",
    "            NULL_ix.append(i)\n",
    "    except TypeError as e: \n",
    "        error_ix.append(i)\n",
    "\n",
    "NA_ix = X_train.loc[X_train['lemmatized'].isnull(), ].index\n",
    "X_train['lemmatized'].loc[list(NA_ix), ] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "#X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174958</th>\n",
       "      <td>dianamra</td>\n",
       "      <td>and it was</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230300</th>\n",
       "      <td>geegeeludlow</td>\n",
       "      <td>is in IT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279405</th>\n",
       "      <td>Jmoux</td>\n",
       "      <td>are on..</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326761</th>\n",
       "      <td>Spacegirlspif13</td>\n",
       "      <td>Is...                                         ...</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356389</th>\n",
       "      <td>ChickWithAName</td>\n",
       "      <td>. . . . . and it's on!</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510320</th>\n",
       "      <td>LukeOgle</td>\n",
       "      <td>is in IT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607035</th>\n",
       "      <td>sangofsorrow</td>\n",
       "      <td>He is...</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629899</th>\n",
       "      <td>SquarahFaggins</td>\n",
       "      <td>to it!!</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043379</th>\n",
       "      <td>rooroocachoo</td>\n",
       "      <td>It will</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128263</th>\n",
       "      <td>WMonk</td>\n",
       "      <td>It will</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                username                                               text  \\\n",
       "174958          dianamra                                        and it was    \n",
       "230300      geegeeludlow                                          is in IT    \n",
       "279405             Jmoux                                          are on..    \n",
       "326761   Spacegirlspif13  Is...                                         ...   \n",
       "356389    ChickWithAName                            . . . . . and it's on!    \n",
       "510320          LukeOgle                                          is in IT    \n",
       "607035      sangofsorrow                                       He is...       \n",
       "629899    SquarahFaggins                                           to it!!    \n",
       "1043379     rooroocachoo                                           It will    \n",
       "1128263            WMonk                                           It will    \n",
       "\n",
       "        lemmatized  \n",
       "174958        NULL  \n",
       "230300        NULL  \n",
       "279405        NULL  \n",
       "326761        NULL  \n",
       "356389        NULL  \n",
       "510320        NULL  \n",
       "607035        NULL  \n",
       "629899        NULL  \n",
       "1043379       NULL  \n",
       "1128263       NULL  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_ix = []\n",
    "for i, tweet in enumerate(X_train['lemmatized'][:len(X_train)]):\n",
    "    try:\n",
    "        m = re.search(r'NULL', tweet)\n",
    "        if m:\n",
    "            NULL_ix.append(i)\n",
    "    except TypeError as e: \n",
    "        continue\n",
    "\n",
    "X_train.iloc[NULL_ix, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BoW DFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatized column (2) as array, ravel will flatten the structure\n",
    "X_array = np.array(X_train.iloc[:, 2]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      she why didn she call just please come hom soon\n",
       "1    had blast studio last friday making my first a...\n",
       "2    USERNAME amazoncom mar caneuon n amazing ar y ...\n",
       "Name: lemmatized, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['she why didn she call just please come hom soon',\n",
       "       'had blast studio last friday making my first album bandlife good',\n",
       "       'USERNAME amazoncom mar caneuon n amazing ar y live chat ma nwn chware darn o turn right'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit A Geron\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "\n",
    "class DocumentToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, replace_numbers=True):\n",
    "        self.replace_numbers = replace_numbers\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', doc)\n",
    "            word_counts = Counter(doc.split())\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'she': 2, 'why': 1, 'didn': 1, 'call': 1, 'just': 1, 'please': 1, 'come': 1, 'hom': 1, 'soon': 1}\n",
      "{'had': 1, 'blast': 1, 'studio': 1, 'last': 1, 'friday': 1, 'making': 1, 'my': 1, 'first': 1, 'album': 1, 'bandlife': 1, 'good': 1}\n",
      "{'USERNAME': 1, 'amazoncom': 1, 'mar': 1, 'caneuon': 1, 'n': 1, 'amazing': 1, 'ar': 1, 'y': 1, 'live': 1, 'chat': 1, 'ma': 1, 'nwn': 1, 'chware': 1, 'darn': 1, 'o': 1, 'turn': 1, 'right': 1}\n",
      "{'i': 3, 'lost': 1, 'NUMBER': 1, 'follower': 1, 'do': 1, 'you': 1, 'not': 1, 'know': 1, 'who': 1, 'am': 1, 'demand': 1, 'steward': 1, 'enquiry': 1}\n",
      "{'have': 3, 'even': 2, 'iphone': 2, 'USERNAME': 1, 'glad': 1, 'youre': 1, 'able': 1, 'i': 1, 'verizon': 1, 'they': 1, 'dont': 1, 'option': 1}\n",
      "{'i': 2, 'USERNAME': 1, 'aww': 1, 'im': 1, 'sorry': 1, 'hope': 1, 'change': 1, 'soon': 1, 'hate': 1, 'people': 1, 'who': 1, 'dont': 1, 'comment': 1, 'when': 1, 'they': 1, 'read': 1}\n",
      "{'playing': 1, 'some': 1, 'combat': 1, 'arm': 1, 'you': 1, 'should': 1, 'check': 1, 'out': 1, 'pretty': 1, 'sweet': 1}\n",
      "{'her': 2, 'took': 1, 'monroe': 1, 'out': 1, 'first': 1, 'walk': 1, 'leash': 1, 'love': 1, 'new': 1, 'hot': 1, 'pink': 1, 'collar': 1}\n",
      "{'urgh': 1, 'full': 1, 'cold': 1}\n",
      "{'USERNAME': 1, 'aaawww': 1, 'battery': 1, 'all': 1, 'charged': 1, 'up': 1, 'nowhere': 1, 'NUMBER': 1, 'go': 1}\n"
     ]
    }
   ],
   "source": [
    "X_few = X_array[:10]\n",
    "X_few_wordcounts = DocumentToWordCounterTransformer().fit_transform(X_few)\n",
    "for i in X_few_wordcounts:\n",
    "    print(str(i).split('Counter(')[1].split(')')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 5) # minimum count\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows, cols, data = [], [], []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 112 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer()\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'USERNAME': 2,\n",
       " 'have': 3,\n",
       " 'she': 4,\n",
       " 'soon': 5,\n",
       " 'first': 6,\n",
       " 'NUMBER': 7,\n",
       " 'you': 8,\n",
       " 'who': 9,\n",
       " 'even': 10,\n",
       " 'iphone': 11,\n",
       " 'they': 12,\n",
       " 'dont': 13,\n",
       " 'out': 14,\n",
       " 'her': 15,\n",
       " 'why': 16,\n",
       " 'didn': 17,\n",
       " 'call': 18,\n",
       " 'just': 19,\n",
       " 'please': 20,\n",
       " 'come': 21,\n",
       " 'hom': 22,\n",
       " 'had': 23,\n",
       " 'blast': 24,\n",
       " 'studio': 25,\n",
       " 'last': 26,\n",
       " 'friday': 27,\n",
       " 'making': 28,\n",
       " 'my': 29,\n",
       " 'album': 30,\n",
       " 'bandlife': 31,\n",
       " 'good': 32,\n",
       " 'amazoncom': 33,\n",
       " 'mar': 34,\n",
       " 'caneuon': 35,\n",
       " 'n': 36,\n",
       " 'amazing': 37,\n",
       " 'ar': 38,\n",
       " 'y': 39,\n",
       " 'live': 40,\n",
       " 'chat': 41,\n",
       " 'ma': 42,\n",
       " 'nwn': 43,\n",
       " 'chware': 44,\n",
       " 'darn': 45,\n",
       " 'o': 46,\n",
       " 'turn': 47,\n",
       " 'right': 48,\n",
       " 'lost': 49,\n",
       " 'follower': 50,\n",
       " 'do': 51,\n",
       " 'not': 52,\n",
       " 'know': 53,\n",
       " 'am': 54,\n",
       " 'demand': 55,\n",
       " 'steward': 56,\n",
       " 'enquiry': 57,\n",
       " 'glad': 58,\n",
       " 'youre': 59,\n",
       " 'able': 60,\n",
       " 'verizon': 61,\n",
       " 'option': 62,\n",
       " 'aww': 63,\n",
       " 'im': 64,\n",
       " 'sorry': 65,\n",
       " 'hope': 66,\n",
       " 'change': 67,\n",
       " 'hate': 68,\n",
       " 'people': 69,\n",
       " 'comment': 70,\n",
       " 'when': 71,\n",
       " 'read': 72,\n",
       " 'playing': 73,\n",
       " 'some': 74,\n",
       " 'combat': 75,\n",
       " 'arm': 76,\n",
       " 'should': 77,\n",
       " 'check': 78,\n",
       " 'pretty': 79,\n",
       " 'sweet': 80,\n",
       " 'took': 81,\n",
       " 'monroe': 82,\n",
       " 'walk': 83,\n",
       " 'leash': 84,\n",
       " 'love': 85,\n",
       " 'new': 86,\n",
       " 'hot': 87,\n",
       " 'pink': 88,\n",
       " 'collar': 89,\n",
       " 'urgh': 90,\n",
       " 'full': 91,\n",
       " 'cold': 92,\n",
       " 'aaawww': 93,\n",
       " 'battery': 94,\n",
       " 'all': 95,\n",
       " 'charged': 96,\n",
       " 'up': 97,\n",
       " 'nowhere': 98,\n",
       " 'go': 99}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#\n",
    "#preprocess_pipeline = Pipeline([\n",
    "#    (\"document_to_wordcount\", DocumentToWordCounterTransformer()),\n",
    "#    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "#])\n",
    "\n",
    "# lose the vocabulary_ ? WHERE IS THE VOCABULARY..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_transformed = preprocess_pipeline.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1199999x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 10232362 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 [[2 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "32 [[1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(X_train_transformed[:33,:30].todense()):\n",
    "    if i > 30:\n",
    "        print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wordcounts = DocumentToWordCounterTransformer().fit_transform(X_array)\n",
    "\n",
    "vocabulary_transformer = WordCounterToVectorTransformer()\n",
    "X_vectors = vocabulary_transformer.fit_transform(X_wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USERNAME': 1,\n",
       " 'i': 2,\n",
       " 'my': 3,\n",
       " 'you': 4,\n",
       " 'NUMBER': 5,\n",
       " 'im': 6,\n",
       " 'me': 7,\n",
       " 'so': 8,\n",
       " 'have': 9,\n",
       " 'but': 10,\n",
       " 'just': 11,\n",
       " 'not': 12,\n",
       " 'day': 13,\n",
       " 'this': 14,\n",
       " 'now': 15,\n",
       " 'good': 16,\n",
       " 'up': 17,\n",
       " 'get': 18,\n",
       " 'URL': 19,\n",
       " 'all': 20,\n",
       " 'out': 21,\n",
       " 'like': 22,\n",
       " 'go': 23,\n",
       " 'no': 24,\n",
       " 'got': 25,\n",
       " 'u': 26,\n",
       " 'love': 27,\n",
       " 'dont': 28,\n",
       " 'work': 29,\n",
       " 'do': 30,\n",
       " 'today': 31,\n",
       " 'your': 32,\n",
       " 'going': 33,\n",
       " 'too': 34,\n",
       " 'time': 35,\n",
       " 'cant': 36,\n",
       " 'back': 37,\n",
       " 'one': 38,\n",
       " 'lol': 39,\n",
       " 'know': 40,\n",
       " 'what': 41,\n",
       " 'we': 42,\n",
       " 'about': 43,\n",
       " 'can': 44,\n",
       " 'really': 45,\n",
       " 'am': 46,\n",
       " 'want': 47,\n",
       " 'had': 48,\n",
       " 'there': 49,\n",
       " 'see': 50,\n",
       " 'some': 51,\n",
       " 'well': 52,\n",
       " 'night': 53,\n",
       " 'think': 54,\n",
       " 'if': 55,\n",
       " 'still': 56,\n",
       " 'new': 57,\n",
       " 'na': 58,\n",
       " 'how': 59,\n",
       " 'need': 60,\n",
       " 'thanks': 61,\n",
       " 'home': 62,\n",
       " 'when': 63,\n",
       " 'oh': 64,\n",
       " 'miss': 65,\n",
       " 'more': 66,\n",
       " 'here': 67,\n",
       " 'much': 68,\n",
       " 'off': 69,\n",
       " 'they': 70,\n",
       " 'last': 71,\n",
       " 'feel': 72,\n",
       " 'hope': 73,\n",
       " 'make': 74,\n",
       " 'morning': 75,\n",
       " 'been': 76,\n",
       " 'then': 77,\n",
       " 'tomorrow': 78,\n",
       " 'great': 79,\n",
       " 'twitter': 80,\n",
       " 'ill': 81,\n",
       " 'or': 82,\n",
       " 'thats': 83,\n",
       " 'her': 84,\n",
       " 'haha': 85,\n",
       " 'wish': 86,\n",
       " 'again': 87,\n",
       " 'sad': 88,\n",
       " 'fun': 89,\n",
       " 'come': 90,\n",
       " 'why': 91,\n",
       " 'only': 92,\n",
       " 'right': 93,\n",
       " 'week': 94,\n",
       " 'sleep': 95,\n",
       " 'didnt': 96,\n",
       " 'bad': 97,\n",
       " 'happy': 98,\n",
       " 'would': 99,\n",
       " 'very': 100,\n",
       " 'thing': 101,\n",
       " 'sorry': 102,\n",
       " 'friend': 103,\n",
       " 'tonight': 104,\n",
       " 'did': 105,\n",
       " 'way': 106,\n",
       " 'them': 107,\n",
       " 'getting': 108,\n",
       " 'gon': 109,\n",
       " 'though': 110,\n",
       " 'look': 111,\n",
       " 'ive': 112,\n",
       " 'over': 113,\n",
       " 'nice': 114,\n",
       " 'EMOJI': 115,\n",
       " 'better': 116,\n",
       " 'watching': 117,\n",
       " 'say': 118,\n",
       " 'should': 119,\n",
       " 'wait': 120,\n",
       " 'she': 121,\n",
       " 'hate': 122,\n",
       " 'bed': 123,\n",
       " 'yeah': 124,\n",
       " 'could': 125,\n",
       " 'people': 126,\n",
       " 'school': 127,\n",
       " 'youre': 128,\n",
       " 'hour': 129,\n",
       " 'guy': 130,\n",
       " 'yes': 131,\n",
       " 'weekend': 132,\n",
       " 'him': 133,\n",
       " 'even': 134,\n",
       " 'hey': 135,\n",
       " 'take': 136,\n",
       " 'after': 137,\n",
       " 'next': 138,\n",
       " 'show': 139,\n",
       " 'who': 140,\n",
       " 'down': 141,\n",
       " 'awesome': 142,\n",
       " 'never': 143,\n",
       " 'tweet': 144,\n",
       " 'thank': 145,\n",
       " 'soon': 146,\n",
       " 'let': 147,\n",
       " 'little': 148,\n",
       " 'long': 149,\n",
       " 'life': 150,\n",
       " 'first': 151,\n",
       " 'wan': 152,\n",
       " 'working': 153,\n",
       " 'best': 154,\n",
       " 'movie': 155,\n",
       " 'doing': 156,\n",
       " 'year': 157,\n",
       " 'please': 158,\n",
       " 'being': 159,\n",
       " 'having': 160,\n",
       " 'tired': 161,\n",
       " 'sick': 162,\n",
       " 'feeling': 163,\n",
       " 'watch': 164,\n",
       " 'everyone': 165,\n",
       " 'ok': 166,\n",
       " 'girl': 167,\n",
       " 'his': 168,\n",
       " 'our': 169,\n",
       " 'wont': 170,\n",
       " 'any': 171,\n",
       " 'done': 172,\n",
       " 'always': 173,\n",
       " 'sure': 174,\n",
       " 'lot': 175,\n",
       " 'already': 176,\n",
       " 'than': 177,\n",
       " 'suck': 178,\n",
       " 'another': 179,\n",
       " 'find': 180,\n",
       " 'cool': 181,\n",
       " 'something': 182,\n",
       " 'phone': 183,\n",
       " 'ready': 184,\n",
       " 'because': 185,\n",
       " 'x': 186,\n",
       " 'made': 187,\n",
       " 'where': 188,\n",
       " 'man': 189,\n",
       " 'keep': 190,\n",
       " 'looking': 191,\n",
       " 'yay': 192,\n",
       " 'song': 193,\n",
       " 'ur': 194,\n",
       " 'doesnt': 195,\n",
       " 'went': 196,\n",
       " 'before': 197,\n",
       " 'house': 198,\n",
       " 'yet': 199,\n",
       " 'hurt': 200,\n",
       " 'help': 201,\n",
       " 'start': 202,\n",
       " 'pretty': 203,\n",
       " 'thought': 204,\n",
       " 'ever': 205,\n",
       " 'trying': 206,\n",
       " 'away': 207,\n",
       " 'sound': 208,\n",
       " 'summer': 209,\n",
       " 'old': 210,\n",
       " 'maybe': 211,\n",
       " 'finally': 212,\n",
       " 'amazing': 213,\n",
       " 'game': 214,\n",
       " 'omg': 215,\n",
       " 'early': 216,\n",
       " 'someone': 217,\n",
       " 'lost': 218,\n",
       " 'bit': 219,\n",
       " 'guess': 220,\n",
       " 'into': 221,\n",
       " 'baby': 222,\n",
       " 'left': 223,\n",
       " 'mean': 224,\n",
       " 'follow': 225,\n",
       " 'damn': 226,\n",
       " 'rain': 227,\n",
       " 'big': 228,\n",
       " 'same': 229,\n",
       " 'missed': 230,\n",
       " 'tell': 231,\n",
       " 'n': 232,\n",
       " 'havent': 233,\n",
       " 'hot': 234,\n",
       " 'nothing': 235,\n",
       " 'while': 236,\n",
       " 'try': 237,\n",
       " 'birthday': 238,\n",
       " 'wow': 239,\n",
       " 'other': 240,\n",
       " 'coming': 241,\n",
       " 'glad': 242,\n",
       " 'also': 243,\n",
       " 'pic': 244,\n",
       " 'party': 245,\n",
       " 'doe': 246,\n",
       " 'live': 247,\n",
       " 'bored': 248,\n",
       " 'weather': 249,\n",
       " 'two': 250,\n",
       " 'sun': 251,\n",
       " 'play': 252,\n",
       " 'hear': 253,\n",
       " 'stuff': 254,\n",
       " 'mom': 255,\n",
       " 'later': 256,\n",
       " 'those': 257,\n",
       " 'actually': 258,\n",
       " 'saw': 259,\n",
       " 'ya': 260,\n",
       " 'isnt': 261,\n",
       " 'might': 262,\n",
       " 'exam': 263,\n",
       " 'ta': 264,\n",
       " 'call': 265,\n",
       " 'excited': 266,\n",
       " 'waiting': 267,\n",
       " 'hard': 268,\n",
       " 'said': 269,\n",
       " 'he': 270,\n",
       " 'car': 271,\n",
       " 'since': 272,\n",
       " 'world': 273,\n",
       " 'god': 274,\n",
       " 'yesterday': 275,\n",
       " 'give': 276,\n",
       " 'until': 277,\n",
       " 'few': 278,\n",
       " 'ugh': 279,\n",
       " 'head': 280,\n",
       " 'job': 281,\n",
       " 'hi': 282,\n",
       " 'such': 283,\n",
       " 'around': 284,\n",
       " 'myself': 285,\n",
       " 'many': 286,\n",
       " 'friday': 287,\n",
       " 'sunday': 288,\n",
       " 'id': 289,\n",
       " 'late': 290,\n",
       " 'video': 291,\n",
       " 'monday': 292,\n",
       " 'luck': 293,\n",
       " 'check': 294,\n",
       " 'found': 295,\n",
       " 'NUMBERth': 296,\n",
       " 'music': 297,\n",
       " 'put': 298,\n",
       " 'talk': 299,\n",
       " 'cold': 300,\n",
       " 'read': 301,\n",
       " 'their': 302,\n",
       " 'making': 303,\n",
       " 'must': 304,\n",
       " 'beautiful': 305,\n",
       " 'follower': 306,\n",
       " 'whats': 307,\n",
       " 'may': 308,\n",
       " 'stop': 309,\n",
       " 'boy': 310,\n",
       " 'gone': 311,\n",
       " 'end': 312,\n",
       " 'least': 313,\n",
       " 'aww': 314,\n",
       " 'missing': 315,\n",
       " 'kid': 316,\n",
       " 'woke': 317,\n",
       " 'anything': 318,\n",
       " 'poor': 319,\n",
       " 'till': 320,\n",
       " 'use': 321,\n",
       " 'most': 322,\n",
       " 'family': 323,\n",
       " 'leave': 324,\n",
       " 'almost': 325,\n",
       " 'hair': 326,\n",
       " 'tho': 327,\n",
       " 'food': 328,\n",
       " 'okay': 329,\n",
       " 'far': 330,\n",
       " 'listening': 331,\n",
       " 'cute': 332,\n",
       " 'lunch': 333,\n",
       " 'picture': 334,\n",
       " 'eat': 335,\n",
       " 'wanted': 336,\n",
       " 'free': 337,\n",
       " 'book': 338,\n",
       " 'month': 339,\n",
       " 'iphone': 340,\n",
       " 'sweet': 341,\n",
       " 'class': 342,\n",
       " 'dinner': 343,\n",
       " 'shes': 344,\n",
       " 'finished': 345,\n",
       " 'funny': 346,\n",
       " 'enjoy': 347,\n",
       " 'playing': 348,\n",
       " 'forward': 349,\n",
       " 'place': 350,\n",
       " 'believe': 351,\n",
       " 'NUMBERam': 352,\n",
       " 'welcome': 353,\n",
       " 'shit': 354,\n",
       " 'r': 355,\n",
       " 'without': 356,\n",
       " 'thinking': 357,\n",
       " 'everything': 358,\n",
       " 'anyone': 359,\n",
       " 'mine': 360,\n",
       " 'cause': 361,\n",
       " 'cry': 362,\n",
       " 'which': 363,\n",
       " 'buy': 364,\n",
       " 'update': 365,\n",
       " 'totally': 366,\n",
       " 'idea': 367,\n",
       " 'dad': 368,\n",
       " 'win': 369,\n",
       " 'these': 370,\n",
       " 'real': 371,\n",
       " 'outside': 372,\n",
       " 'enough': 373,\n",
       " 'coffee': 374,\n",
       " 'o': 375,\n",
       " 'wasnt': 376,\n",
       " 'hahaha': 377,\n",
       " 'wrong': 378,\n",
       " 'stay': 379,\n",
       " 'stupid': 380,\n",
       " 'w': 381,\n",
       " 'every': 382,\n",
       " 'room': 383,\n",
       " 'through': 384,\n",
       " 'anymore': 385,\n",
       " 'probably': 386,\n",
       " 'couldnt': 387,\n",
       " 'dog': 388,\n",
       " 'fan': 389,\n",
       " 'once': 390,\n",
       " 'saturday': 391,\n",
       " 'dream': 392,\n",
       " 'eating': 393,\n",
       " 'money': 394,\n",
       " 'name': 395,\n",
       " 's': 396,\n",
       " 'd': 397,\n",
       " 'hell': 398,\n",
       " 'following': 399,\n",
       " 'p': 400,\n",
       " 'minute': 401,\n",
       " 'xx': 402,\n",
       " 'post': 403,\n",
       " 'sooo': 404,\n",
       " 'tv': 405,\n",
       " 'busy': 406,\n",
       " 'lovely': 407,\n",
       " 'ha': 408,\n",
       " 'brother': 409,\n",
       " 'headache': 410,\n",
       " 'came': 411,\n",
       " 'whole': 412,\n",
       " 'seen': 413,\n",
       " 'kinda': 414,\n",
       " 'taking': 415,\n",
       " 'beach': 416,\n",
       " 'mother': 417,\n",
       " 'run': 418,\n",
       " 'both': 419,\n",
       " 'eye': 420,\n",
       " 'face': 421,\n",
       " 'crazy': 422,\n",
       " 'took': 423,\n",
       " 'hopefully': 424,\n",
       " 'word': 425,\n",
       " 'final': 426,\n",
       " 'computer': 427,\n",
       " 'hello': 428,\n",
       " 'super': 429,\n",
       " 'news': 430,\n",
       " 'able': 431,\n",
       " 'plan': 432,\n",
       " 'true': 433,\n",
       " 'b': 434,\n",
       " 'meet': 435,\n",
       " 'half': 436,\n",
       " 'problem': 437,\n",
       " 'theyre': 438,\n",
       " 'blog': 439,\n",
       " 'hit': 440,\n",
       " 'forgot': 441,\n",
       " 'awww': 442,\n",
       " 'goodnight': 443,\n",
       " 'leaving': 444,\n",
       " 'either': 445,\n",
       " 'reading': 446,\n",
       " 'heart': 447,\n",
       " 'photo': 448,\n",
       " 'shopping': 449,\n",
       " 'part': 450,\n",
       " 'else': 451,\n",
       " 'rest': 452,\n",
       " 'ago': 453,\n",
       " 'sitting': 454,\n",
       " 'used': 455,\n",
       " 'send': 456,\n",
       " 'soo': 457,\n",
       " 'full': 458,\n",
       " 'trip': 459,\n",
       " 'ah': 460,\n",
       " 'kind': 461,\n",
       " 'seems': 462,\n",
       " 'rock': 463,\n",
       " 'sister': 464,\n",
       " 'cuz': 465,\n",
       " 'email': 466,\n",
       " 'raining': 467,\n",
       " 'office': 468,\n",
       " 'internet': 469,\n",
       " 'remember': 470,\n",
       " 'talking': 471,\n",
       " 'break': 472,\n",
       " 'mind': 473,\n",
       " 'change': 474,\n",
       " 'watched': 475,\n",
       " 'alone': 476,\n",
       " 'fuck': 477,\n",
       " 'hug': 478,\n",
       " 'stuck': 479,\n",
       " 'heard': 480,\n",
       " 'own': 481,\n",
       " 'link': 482,\n",
       " 'tried': 483,\n",
       " 'boo': 484,\n",
       " 'course': 485,\n",
       " 'started': 486,\n",
       " 'ticket': 487,\n",
       " 'pain': 488,\n",
       " 'reply': 489,\n",
       " 'site': 490,\n",
       " 'btw': 491,\n",
       " 'care': 492,\n",
       " 'seeing': 493,\n",
       " 'hehe': 494,\n",
       " 'drink': 495,\n",
       " 'using': 496,\n",
       " 'la': 497,\n",
       " 'add': 498,\n",
       " 'quite': 499,\n",
       " 'online': 500,\n",
       " 'season': 501,\n",
       " 'concert': 502,\n",
       " 'wake': 503,\n",
       " 'told': 504,\n",
       " 'dude': 505,\n",
       " 'awake': 506,\n",
       " 'loved': 507,\n",
       " 'favorite': 508,\n",
       " 'text': 509,\n",
       " 'fine': 510,\n",
       " 'til': 511,\n",
       " 'breakfast': 512,\n",
       " 'person': 513,\n",
       " 'pay': 514,\n",
       " 'cat': 515,\n",
       " 'open': 516,\n",
       " 'sunny': 517,\n",
       " 'bought': 518,\n",
       " 'hand': 519,\n",
       " 'seriously': 520,\n",
       " 'boring': 521,\n",
       " 'train': 522,\n",
       " 'youll': 523,\n",
       " 'broke': 524,\n",
       " 'study': 525,\n",
       " 'drive': 526,\n",
       " 'called': 527,\n",
       " 'facebook': 528,\n",
       " 'turn': 529,\n",
       " 'm': 530,\n",
       " 'shower': 531,\n",
       " 'starting': 532,\n",
       " 'walk': 533,\n",
       " 'aw': 534,\n",
       " 'as': 535,\n",
       " 'bring': 536,\n",
       " 'lmao': 537,\n",
       " 'move': 538,\n",
       " 'hungry': 539,\n",
       " 'june': 540,\n",
       " 'instead': 541,\n",
       " 'asleep': 542,\n",
       " 'crap': 543,\n",
       " 'anyway': 544,\n",
       " 'lucky': 545,\n",
       " 'afternoon': 546,\n",
       " 'heading': 547,\n",
       " 'lady': 548,\n",
       " 'sleeping': 549,\n",
       " 'le': 550,\n",
       " 'red': 551,\n",
       " 'test': 552,\n",
       " 'enjoying': 553,\n",
       " 'reason': 554,\n",
       " 'xd': 555,\n",
       " 'jealous': 556,\n",
       " 'bout': 557,\n",
       " 'story': 558,\n",
       " 'wonderful': 559,\n",
       " 'page': 560,\n",
       " 'meeting': 561,\n",
       " 'second': 562,\n",
       " 'NUMBERpm': 563,\n",
       " 'mad': 564,\n",
       " 'city': 565,\n",
       " 'ice': 566,\n",
       " 'set': 567,\n",
       " 'sore': 568,\n",
       " 'yea': 569,\n",
       " 'album': 570,\n",
       " 'soooo': 571,\n",
       " 'together': 572,\n",
       " 'bye': 573,\n",
       " 'homework': 574,\n",
       " 'high': 575,\n",
       " 'finish': 576,\n",
       " 'definitely': 577,\n",
       " 'dead': 578,\n",
       " 'holiday': 579,\n",
       " 'top': 580,\n",
       " 'cut': 581,\n",
       " 'running': 582,\n",
       " 'laptop': 583,\n",
       " 'congrats': 584,\n",
       " 'hoping': 585,\n",
       " 'fucking': 586,\n",
       " 'message': 587,\n",
       " 'write': 588,\n",
       " 'bday': 589,\n",
       " 'couple': 590,\n",
       " 'died': 591,\n",
       " 'happened': 592,\n",
       " 'fail': 593,\n",
       " 'sometimes': 594,\n",
       " 'store': 595,\n",
       " 'ask': 596,\n",
       " 'won': 597,\n",
       " 'goin': 598,\n",
       " 'sigh': 599,\n",
       " 'award': 600,\n",
       " 'moment': 601,\n",
       " 'dear': 602,\n",
       " 'c': 603,\n",
       " 'foot': 604,\n",
       " 'fall': 605,\n",
       " 'nap': 606,\n",
       " 'NUMBERst': 607,\n",
       " 'worry': 608,\n",
       " 'star': 609,\n",
       " 'wouldnt': 610,\n",
       " 'tour': 611,\n",
       " 'tea': 612,\n",
       " 'visit': 613,\n",
       " 'church': 614,\n",
       " 'short': 615,\n",
       " 'side': 616,\n",
       " 'water': 617,\n",
       " 'evening': 618,\n",
       " 'town': 619,\n",
       " 'dance': 620,\n",
       " 'youtube': 621,\n",
       " 'smile': 622,\n",
       " 'perfect': 623,\n",
       " 'ppl': 624,\n",
       " 'arent': 625,\n",
       " 'happen': 626,\n",
       " 'ipod': 627,\n",
       " 'point': 628,\n",
       " 'studying': 629,\n",
       " 'lil': 630,\n",
       " 'weird': 631,\n",
       " 'date': 632,\n",
       " 'ride': 633,\n",
       " 'listen': 634,\n",
       " 'close': 635,\n",
       " 'line': 636,\n",
       " 'list': 637,\n",
       " 'math': 638,\n",
       " 'hang': 639,\n",
       " 'mood': 640,\n",
       " 'yall': 641,\n",
       " 'seem': 642,\n",
       " 'min': 643,\n",
       " 'loving': 644,\n",
       " 'nite': 645,\n",
       " 'cream': 646,\n",
       " 'wonder': 647,\n",
       " 'gym': 648,\n",
       " 'catch': 649,\n",
       " 'mum': 650,\n",
       " 'knew': 651,\n",
       " 'ate': 652,\n",
       " 'english': 653,\n",
       " 'episode': 654,\n",
       " 'agree': 655,\n",
       " 'interesting': 656,\n",
       " 'writing': 657,\n",
       " 'chocolate': 658,\n",
       " 'aint': 659,\n",
       " 'clean': 660,\n",
       " 'account': 661,\n",
       " 'fb': 662,\n",
       " 'pool': 663,\n",
       " 'wedding': 664,\n",
       " 'chance': 665,\n",
       " 'window': 666,\n",
       " 'band': 667,\n",
       " 'worst': 668,\n",
       " 'london': 669,\n",
       " 'ahh': 670,\n",
       " 'saying': 671,\n",
       " 't': 672,\n",
       " 'air': 673,\n",
       " 'broken': 674,\n",
       " 'fast': 675,\n",
       " 'vote': 676,\n",
       " 'team': 677,\n",
       " 'throat': 678,\n",
       " 'unfortunately': 679,\n",
       " 'supposed': 680,\n",
       " 'moving': 681,\n",
       " 'flight': 682,\n",
       " 'via': 683,\n",
       " 'da': 684,\n",
       " 'past': 685,\n",
       " 'mr': 686,\n",
       " 'pick': 687,\n",
       " 'black': 688,\n",
       " 'cleaning': 689,\n",
       " 'xxx': 690,\n",
       " 'sent': 691,\n",
       " 'worth': 692,\n",
       " 'sat': 693,\n",
       " 'v': 694,\n",
       " 'question': 695,\n",
       " 'driving': 696,\n",
       " 'forget': 697,\n",
       " 'gave': 698,\n",
       " 'sunshine': 699,\n",
       " 'park': 700,\n",
       " 'wishing': 701,\n",
       " 'card': 702,\n",
       " 'three': 703,\n",
       " 'parent': 704,\n",
       " 'understand': 705,\n",
       " 'horrible': 706,\n",
       " 'sleepy': 707,\n",
       " 'followfriday': 708,\n",
       " 'answer': 709,\n",
       " 'yep': 710,\n",
       " 'mac': 711,\n",
       " 'jonas': 712,\n",
       " 'tweeting': 713,\n",
       " 'drinking': 714,\n",
       " 'college': 715,\n",
       " 'cake': 716,\n",
       " 'upset': 717,\n",
       " 'em': 718,\n",
       " 'leg': 719,\n",
       " 'green': 720,\n",
       " 'number': 721,\n",
       " 'beer': 722,\n",
       " 'special': 723,\n",
       " 'slow': 724,\n",
       " 'easy': 725,\n",
       " 'moon': 726,\n",
       " 'finger': 727,\n",
       " 'website': 728,\n",
       " 'comment': 729,\n",
       " 'paper': 730,\n",
       " 'tuesday': 731,\n",
       " 'project': 732,\n",
       " 'longer': 733,\n",
       " 'worse': 734,\n",
       " 'rather': 735,\n",
       " 'spent': 736,\n",
       " 'blue': 737,\n",
       " 'y': 738,\n",
       " 'bet': 739,\n",
       " 'bus': 740,\n",
       " 'hmm': 741,\n",
       " 'apparently': 742,\n",
       " 'fell': 743,\n",
       " 'shop': 744,\n",
       " 'youve': 745,\n",
       " 'vacation': 746,\n",
       " 'scared': 747,\n",
       " 'body': 748,\n",
       " 'due': 749,\n",
       " 'shoe': 750,\n",
       " 'under': 751,\n",
       " 'hows': 752,\n",
       " 'beat': 753,\n",
       " 'figure': 754,\n",
       " 'plus': 755,\n",
       " 'huge': 756,\n",
       " 'wear': 757,\n",
       " 'co': 758,\n",
       " 'fair': 759,\n",
       " 'woman': 760,\n",
       " 'dress': 761,\n",
       " 'load': 762,\n",
       " 'hanging': 763,\n",
       " 'white': 764,\n",
       " 'cousin': 765,\n",
       " 'spend': 766,\n",
       " 'kill': 767,\n",
       " 'earlier': 768,\n",
       " 'voice': 769,\n",
       " 'flu': 770,\n",
       " 'nope': 771,\n",
       " 'mtv': 772,\n",
       " 'thx': 773,\n",
       " 'join': 774,\n",
       " 'support': 775,\n",
       " 'wtf': 776,\n",
       " 'during': 777,\n",
       " 'wondering': 778,\n",
       " 'miley': 779,\n",
       " 'shame': 780,\n",
       " 'uk': 781,\n",
       " 'thursday': 782,\n",
       " 'camera': 783,\n",
       " 'forever': 784,\n",
       " 'chat': 785,\n",
       " 'cheer': 786,\n",
       " 'lazy': 787,\n",
       " 'looked': 788,\n",
       " 'stomach': 789,\n",
       " 'cd': 790,\n",
       " 'babe': 791,\n",
       " 'age': 792,\n",
       " 'light': 793,\n",
       " 'shot': 794,\n",
       " 'ahhh': 795,\n",
       " 'son': 796,\n",
       " 'slept': 797,\n",
       " 'power': 798,\n",
       " 'sadly': 799,\n",
       " 'david': 800,\n",
       " 'bike': 801,\n",
       " 'garden': 802,\n",
       " 'apple': 803,\n",
       " 'boyfriend': 804,\n",
       " 'die': 805,\n",
       " 'idk': 806,\n",
       " 'warm': 807,\n",
       " 'different': 808,\n",
       " 'learn': 809,\n",
       " 'inside': 810,\n",
       " 'NUMBERday': 811,\n",
       " 'tom': 812,\n",
       " 'especially': 813,\n",
       " 'dm': 814,\n",
       " 'fix': 815,\n",
       " 'july': 816,\n",
       " 'save': 817,\n",
       " 'meant': 818,\n",
       " 'sign': 819,\n",
       " 'google': 820,\n",
       " 'airport': 821,\n",
       " 'itll': 822,\n",
       " 'liked': 823,\n",
       " 'father': 824,\n",
       " 'l': 825,\n",
       " 'pizza': 826,\n",
       " 'case': 827,\n",
       " 'sims': 828,\n",
       " 'myspace': 829,\n",
       " 'bbq': 830,\n",
       " 'road': 831,\n",
       " 'service': 832,\n",
       " 'officially': 833,\n",
       " 'tummy': 834,\n",
       " 'safe': 835,\n",
       " 'laugh': 836,\n",
       " 'worked': 837,\n",
       " 'note': 838,\n",
       " 'hr': 839,\n",
       " 'felt': 840,\n",
       " 'rainy': 841,\n",
       " 'luv': 842,\n",
       " 'bitch': 843,\n",
       " 'chicken': 844,\n",
       " 'met': 845,\n",
       " 'shall': 846,\n",
       " 'si': 847,\n",
       " 'absolutely': 848,\n",
       " 'box': 849,\n",
       " 'shirt': 850,\n",
       " 'small': 851,\n",
       " 'doctor': 852,\n",
       " 'hospital': 853,\n",
       " 'yummy': 854,\n",
       " 'rip': 855,\n",
       " 'hill': 856,\n",
       " 'goodbye': 857,\n",
       " 'radio': 858,\n",
       " 'each': 859,\n",
       " 'proud': 860,\n",
       " 'share': 861,\n",
       " 'wit': 862,\n",
       " 'film': 863,\n",
       " 'graduation': 864,\n",
       " 'cup': 865,\n",
       " 'club': 866,\n",
       " 'decided': 867,\n",
       " 'NUMBERnd': 868,\n",
       " 'fly': 869,\n",
       " 'order': 870,\n",
       " 'except': 871,\n",
       " 'yourself': 872,\n",
       " 'played': 873,\n",
       " 'hubby': 874,\n",
       " 'lonely': 875,\n",
       " 'fact': 876,\n",
       " 'exciting': 877,\n",
       " 'twilight': 878,\n",
       " 'dvd': 879,\n",
       " 'gorgeous': 880,\n",
       " 'needed': 881,\n",
       " 'wine': 882,\n",
       " 'french': 883,\n",
       " 'alright': 884,\n",
       " 'wednesday': 885,\n",
       " 'annoying': 886,\n",
       " 'interview': 887,\n",
       " 'front': 888,\n",
       " 'glass': 889,\n",
       " 'company': 890,\n",
       " 'smell': 891,\n",
       " 'issue': 892,\n",
       " 'taken': 893,\n",
       " 'exactly': 894,\n",
       " 'bag': 895,\n",
       " 'lame': 896,\n",
       " 'bar': 897,\n",
       " 'yr': 898,\n",
       " 'storm': 899,\n",
       " 'scary': 900,\n",
       " 'living': 901,\n",
       " 'yup': 902,\n",
       " 'near': 903,\n",
       " 'xoxo': 904,\n",
       " 'business': 905,\n",
       " 're': 906,\n",
       " 'woo': 907,\n",
       " 'packing': 908,\n",
       " 'mate': 909,\n",
       " 'turned': 910,\n",
       " 'behind': 911,\n",
       " 'sold': 912,\n",
       " 'waking': 913,\n",
       " 'realized': 914,\n",
       " 'bc': 915,\n",
       " 'wife': 916,\n",
       " 'yours': 917,\n",
       " 'version': 918,\n",
       " 'daughter': 919,\n",
       " 'touch': 920,\n",
       " 'jus': 921,\n",
       " 'happens': 922,\n",
       " 'ouch': 923,\n",
       " 'killing': 924,\n",
       " 'gettin': 925,\n",
       " 'download': 926,\n",
       " 'ball': 927,\n",
       " 'guitar': 928,\n",
       " 'drunk': 929,\n",
       " 'giving': 930,\n",
       " 'lose': 931,\n",
       " 'pas': 932,\n",
       " 'mommy': 933,\n",
       " 'matter': 934,\n",
       " 'terrible': 935,\n",
       " 'revision': 936,\n",
       " 'walking': 937,\n",
       " 'event': 938,\n",
       " 'state': 939,\n",
       " 'round': 940,\n",
       " 'yo': 941,\n",
       " 'door': 942,\n",
       " 'vega': 943,\n",
       " 'along': 944,\n",
       " 'mile': 945,\n",
       " 'hold': 946,\n",
       " 'app': 947,\n",
       " 'puppy': 948,\n",
       " 'everybody': 949,\n",
       " 'single': 950,\n",
       " 'e': 951,\n",
       " 'ear': 952,\n",
       " 'deal': 953,\n",
       " 'although': 954,\n",
       " 'clothes': 955,\n",
       " 'art': 956,\n",
       " 'hasnt': 957,\n",
       " 'country': 958,\n",
       " 'enjoyed': 959,\n",
       " 'bro': 960,\n",
       " 'arm': 961,\n",
       " 'sale': 962,\n",
       " 'posted': 963,\n",
       " 'hangover': 964,\n",
       " 'plane': 965,\n",
       " 'fantastic': 966,\n",
       " 'tear': 967,\n",
       " 'hotel': 968,\n",
       " 'hahah': 969,\n",
       " 'sit': 970,\n",
       " 'asked': 971,\n",
       " 'whatever': 972,\n",
       " 'staying': 973,\n",
       " 'relaxing': 974,\n",
       " 'taste': 975,\n",
       " 'random': 976,\n",
       " 'group': 977,\n",
       " 'ahead': 978,\n",
       " 'shoot': 979,\n",
       " 'passed': 980,\n",
       " 'bb': 981,\n",
       " 'singing': 982,\n",
       " 'web': 983,\n",
       " 'indeed': 984,\n",
       " 'history': 985,\n",
       " 'vip': 986,\n",
       " 'hmmm': 987,\n",
       " 'alot': 988,\n",
       " 'upload': 989,\n",
       " 'profile': 990,\n",
       " 'freaking': 991,\n",
       " 'fit': 992,\n",
       " 'kiss': 993,\n",
       " 'camp': 994,\n",
       " 'currently': 995,\n",
       " 'completely': 996,\n",
       " 'death': 997,\n",
       " 'dark': 998,\n",
       " 'cook': 999,\n",
       " 'child': 1000}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_transformer.vocabulary_ # 1000! yay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1199999x1001 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 10232362 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix,vals=[],[]\n",
    "for i, v in enumerate(X_vectors[4].toarray()[0]):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        if v != 0:\n",
    "            ix.append(i)\n",
    "            vals.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1001 in length, the first is how many terms are missing from the vocab in this doc\n",
    "X_vectors[4].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 i\n",
      "2 my\n",
      "6 me\n",
      "25 u\n",
      "26 love\n",
      "55 still\n",
      "58 how\n",
      "109 though\n",
      "169 wont\n",
      "264 call\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(vocabulary_transformer.vocabulary_):\n",
    "    if i in ix:\n",
    "        print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 6, 25, 26, 55, 58, 109, 169, 264], [4, 1, 1, 1, 2, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['please pray my house there major water leakage thats causing my entire house crack possibly fall apart',\n",
       "       'URL bump this', 'USERNAME just saw not part your last message',\n",
       "       'USERNAME lol i put up 100 track u havent retweeted 1 hun smh',\n",
       "       'USERNAME u got ta endorse application if u gon na our courtside tweep USERNAME im USERNAME i USERNAME',\n",
       "       'best night ever got spend better part half hour letting off firework'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BoW DFM\n",
    "bow_vectorizer_ung = CountVectorizer(max_features=10000) \n",
    "bow_vectorizer_big = CountVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "X_bow_ung = bow_vectorizer_ung.fit_transform(X_array)\n",
    "X_bow_big = bow_vectorizer_big.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1199999x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 10626030 stored elements in Compressed Sparse Row format>,\n",
       " <1199999x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 13212223 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total space <1199999x329492 sparse matrix of type '<class 'numpy.int64'>' with 11446957 stored elements\n",
    "X_bow_ung, X_bow_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 0.0885503238 % nonzero.\n"
     ]
    }
   ],
   "source": [
    "# Only 0.0028951048 % nonzero for ALL features\n",
    "def calc_sparsity(X):\n",
    "    total_space = X.shape[0] * X.shape[1]\n",
    "    total_store = X.getnnz()\n",
    "    pct_zeroes = 100 * (total_store/total_space)\n",
    "    print(f'Only {pct_zeroes:0.10f} % nonzero.')\n",
    "\n",
    "calc_sparsity(X_bow_ung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 0.1101019501 % nonzero.\n"
     ]
    }
   ],
   "source": [
    "calc_sparsity(X_bow_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 7 minute(s) and 5 second(s)\n"
     ]
    }
   ],
   "source": [
    "savepath = os.path.join(\"..\",\"data\",\"3_processed\",\"sentiment140\")\n",
    "sp.save_npz(os.path.join(savepath, 'X_bow_ung.npz'), X_bow_ung)\n",
    "sp.save_npz(os.path.join(savepath, 'X_bow_big.npz'), X_bow_big)\n",
    "\n",
    "# print total running time\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed Time: {mins:0.0f} minute(s) and {secs:0.0f} second(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
