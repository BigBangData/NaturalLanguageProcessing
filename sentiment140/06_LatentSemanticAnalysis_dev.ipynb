{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "This notebook is for developing the LSA solution, not implementing it.\n",
    "\n",
    "Preprocessing was performed for a TF-IDF representation. Since this representation still suffers from the curse of dimensionality, we apply LSA, in particular matrix decomposition using SVD, to reduce the Tfidf representation from a document-term matrix to a document-\"topic\" (aka component) matrix.\n",
    "\n",
    "So far, reducing from 50,000 features to 300 components or topics did not help accuracy. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cleanup_module as Cmod\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 10% of the training data for POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample for dev\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# create array\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119747,), (119747,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape, y_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', Cmod.DocumentToNgramCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=50000)),\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pipe.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x50001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2224008 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "Point of departure was this [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized algo\n",
    "start_time = time.time()\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=300, # topics\n",
    "                         algorithm='randomized', \n",
    "                         n_iter=200, \n",
    "                         random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(source)](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/decomposition/_truncated_svd.py#L24)\n",
    "```\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit LSI model to X and perform dimensionality reduction on X.\n",
    "        \n",
    "        [...]\n",
    "        \n",
    "        if self.algorithm == \"arpack\":\n",
    "            U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol)\n",
    "            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n",
    "            # conventions, so reverse its outputs.\n",
    "            Sigma = Sigma[::-1]\n",
    "            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n",
    "\n",
    "        elif self.algorithm == \"randomized\":\n",
    "            k = self.n_components\n",
    "            n_features = X.shape[1]\n",
    "            if k >= n_features:\n",
    "                raise ValueError(\"n_components must be < n_features;\"\n",
    "                                 \" got %d >= %d\" % (k, n_features))\n",
    "            U, Sigma, VT = randomized_svd(X, self.n_components,\n",
    "                                          n_iter=self.n_iter,\n",
    "                                          random_state=random_state)\n",
    "        else:\n",
    "            raise ValueError(\"unknown algorithm %r\" % self.algorithm)\n",
    "\n",
    "        self.components_ = VT\n",
    "\n",
    "        # Calculate explained variance & explained variance ratio\n",
    "        X_transformed = U * Sigma\n",
    "        \n",
    "        [...]\n",
    "        \n",
    "        self.singular_values_ = Sigma  # Store the singular values.\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Perform dimensionality reduction on X.\n",
    "        [...]\n",
    "        X = check_array(X, accept_sparse=['csr', 'csc'])\n",
    "        check_is_fitted(self)\n",
    "        return safe_sparse_dot(X, self.components_.T)\n",
    "```                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 18 min 25 sec\n"
     ]
    }
   ],
   "source": [
    "svd_model.fit(X_train_transformed)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50001"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.n_features_in_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svd_model.components_) # VT (num features out...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svd_model.singular_values_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50001"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svd_model.components_[0]) # first component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "# transform method in sklearn's docs\n",
    "svd_model_transformed = safe_sparse_dot(X_train_transformed, svd_model.components_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119747, 300)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 17 min 38 sec\n"
     ]
    }
   ],
   "source": [
    "# same as this, X_topics == svd_model_transformed\n",
    "start_time = time.time()\n",
    "X_topics = svd_model.fit_transform(X_train_transformed)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119747, 300)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   15.7s remaining:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   17.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7513\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_topics, y_array, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print('Mean accuracy: ' + str(round(score.mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7999\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_array, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print('Mean accuracy: ' + str(round(score.mean(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoiding Naive Bayes on SVD since it implies strong independence between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quoting the same [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/) ~\n",
    "\n",
    "\"*Apart from LSA, there are other advanced and efficient topic modeling techniques such as Latent Dirichlet Allocation (LDA) and lda2Vec. We have a wonderful article on LDA which you can check out [here](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/). lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arpack \n",
    "start_time = time.time()\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=300, # topics\n",
    "                         algorithm='arpack', \n",
    "                         n_iter=200, \n",
    "                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1 min 25 sec\n"
     ]
    }
   ],
   "source": [
    "svd_model.fit(X_train_transformed)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model_transformed = safe_sparse_dot(X_train_transformed, svd_model.components_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   14.0s remaining:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   16.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7513\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, svd_model_transformed, y_array, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print('Mean accuracy: ' + str(round(score.mean(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot faster training time and same accuracy as the random algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arpack \n",
    "start_time = time.time()\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=500, # topics\n",
    "                         algorithm='arpack', \n",
    "                         n_iter=200, \n",
    "                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 69 min 41 sec\n"
     ]
    }
   ],
   "source": [
    "#svd_model.fit(X_train_transformed)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model_transformed = safe_sparse_dot(X_train_transformed, svd_model.components_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, svd_model_transformed, y_array, cv=5, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "print('Mean accuracy: ' + str(round(score.mean(),4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
