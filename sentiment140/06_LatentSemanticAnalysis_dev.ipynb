{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "This notebook is for developing the LSA solution, not implementing it.\n",
    "\n",
    "Preprocessing was performed for a TF-IDF representation. Since this representation still suffers from the curse of dimensionality, we apply LSA, in particular matrix decomposition using SVD, to reduce the Tfidf representation from a document-term matrix to a document-\"topic\" (aka component) matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cleanup_module as Cmod\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 10% of the training data for POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample for dev\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
    "\n",
    "# create array\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119747,), (119747,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape, y_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counter', Cmod.DocumentToNgramCounterTransformer()),\n",
    "                 ('bow', Cmod.WordCounterToVectorTransformer(vocabulary_size=50000)),\n",
    "                 ('tfidf', TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pipe.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119747x50001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2224008 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "Code from the [Analytics Vidhya Tutorial](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0 min 56 sec\n"
     ]
    }
   ],
   "source": [
    "# SVD represent documents and terms in vectors \n",
    "start_time = time.time()\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=20, # 20 topics\n",
    "                         algorithm='randomized', \n",
    "                         n_iter=100, \n",
    "                         random_state=122)\n",
    "\n",
    "svd_model.fit(X_train_transformed)\n",
    "\n",
    "mins, secs = divmod(time.time() - start_time, 60)\n",
    "print(f'Elapsed: {mins:0.0f} min {secs:0.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50001"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svd_model.components_) # n_features_out_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79.97041978, 26.43187451, 22.59517475, 18.50297893, 18.3121478 ,\n",
       "       17.372291  , 16.78251612, 16.09946305, 15.86258119, 15.05910139,\n",
       "       14.89085909, 14.51385208, 14.23353069, 14.07930775, 13.90572399,\n",
       "       13.69576191, 13.52584642, 13.48107192, 13.35254595, 13.12763369])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svd_model.components_[0]) # first component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94860277, 0.12504544, 0.11471194, 0.07550355, 0.06407802,\n",
       "       0.06050408, 0.0490459 , 0.04268702, 0.04106846, 0.03921248,\n",
       "       0.03968058, 0.03613236, 0.03218715, 0.03267205, 0.02960873,\n",
       "       0.03013664, 0.02674532, 0.02565623, 0.02446787, 0.02438652,\n",
       "       0.02285762, 0.02493416, 0.02166465, 0.03104746, 0.02302004,\n",
       "       0.02319094, 0.02228861, 0.02512052, 0.02007365, 0.0203596 ,\n",
       "       0.02010738, 0.01985315, 0.02214465, 0.02016377, 0.01679119,\n",
       "       0.01915747, 0.01981817, 0.01975959, 0.01759534, 0.0198492 ,\n",
       "       0.01679392, 0.01695766, 0.01705169, 0.01784087, 0.01582815,\n",
       "       0.01571794, 0.01614131, 0.01638356, 0.01504842, 0.0156314 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_[0][:50] # first 50 items of the first component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so svd_model.components_ is a numpy array with 20 numpy arrays of len 50001\n",
    "# we can transpose it to a 50001 by 20 but... we wanted a 119747 by 20 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50001, 20)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119747, 50001)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terms = pipe['tfidf'].get_feature_names() # that didn't work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, comp in enumerate(svd_model.components_):\n",
    "#    terms_comp = zip(terms, comp)\n",
    "#    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "#    print(\"Topic \"+str(i)+\": \")\n",
    "#    for t in sorted_terms:\n",
    "#        print(t[0])\n",
    "#        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import umap\n",
    "#\n",
    "#X_topics = svd_model.fit_transform(X)\n",
    "#embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)\n",
    "#\n",
    "#plt.figure(figsize=(7,5))\n",
    "#plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "#c = dataset.target,\n",
    "#s = 10, # size\n",
    "#edgecolor='none'\n",
    "#)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from LSA, there are other advanced and efficient topic modeling techniques such as Latent Dirichlet Allocation (LDA) and lda2Vec. We have a wonderful article on LDA which you can check out [here](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/). lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119747, 50001)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.6984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(round(score.mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "score = cross_val_score(NB_clf, X_train_transformed, y_array, cv=10, verbose=1, scoring='accuracy')\n",
    "print('Mean accuracy: ' + str(round(score.mean(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
