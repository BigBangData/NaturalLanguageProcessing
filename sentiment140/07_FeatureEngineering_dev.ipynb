{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "*Pupose*\n",
    "\n",
    "- Develop Feature Engineering, such as:\n",
    "\n",
    "    * TextLength, etc\n",
    "    * Cosine Similarity \n",
    "    * count of: punctuations, ascii chars, USERNAMEs, EMOJIs, URLs\n",
    "    * tweet starts with USERNAME, or EMOJI, or URL, etc.\n",
    "    * count of swear words, or negative words, or positive words, etc. (need lists)\n",
    "    * semantic analysis research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urlextract\n",
    "from html import unescape\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# load contractions map\n",
    "with open(\"contractions_map.json\") as f:\n",
    "    contractions_map = json.load(f)\n",
    "\n",
    "# functions\n",
    "def expand_contractions(text, contractions_map):\n",
    "    \n",
    "    pattern = re.compile('({})'.format('|'.join(contractions_map.keys())), \n",
    "                        flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_map.get(match)\\\n",
    "                                if contractions_map.get(match)\\\n",
    "                                else contractions_map.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def is_ascii(doc):\n",
    "    try:\n",
    "        doc.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "# instantiate url extractor and lemmatizer\n",
    "url_extractor = urlextract.URLExtract()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC: sample $0.1\\%$ of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minimally prepared X, y train subsets\n",
    "raw_path = os.path.join(\"..\",\"data\",\"1_raw\",\"sentiment140\")\n",
    "X_train = pd.read_csv(os.path.join(raw_path, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(raw_path, \"y_train.csv\"))\n",
    "\n",
    "# sample 0.1%\n",
    "X, X_rest, y, y_rest = train_test_split(X_train, y_train, test_size=0.999, random_state=42)\n",
    "\n",
    "# create arrays\n",
    "X_array = np.array(X.iloc[:, 2]).ravel()\n",
    "y_array = y.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1197,), (1197,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape, y_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@kmaira, taffitni ena mat9ollich sbe7 el 5ir ?!  9aaaaaaaaaaa',\n",
       "       'has a lot of things to do  http://plurk.com/p/12qe8g',\n",
       "       'Hoping it rains during graduation! But only cause i love the rain, not because i want the ceremony to be ruined ',\n",
       "       '@HenryStreeten Yeh I know duh! Oh wait - I forgot the &quot;O&quot; haha sorry about that ',\n",
       "       '@Prochaine Tys jeÅ¡tÄ\\x9b nikdy nekupoval zajÃ\\xadce v pytli?  TakovÃ¡ early registration je prostÄ\\x9b risk...',\n",
       "       'going to dinner with my mamma. miss california ',\n",
       "       'trusty Mini got me to Gatwick in time   much fun!',\n",
       "       'One day i will accomplish my mission and when that day comes i will be content ',\n",
       "       '@ksekher I want to samepinch your mom  coz I am too!',\n",
       "       'At work, doing recruitment stuff! '], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_len(X):\n",
    "    X_out = []\n",
    "    for doc in X:\n",
    "        X_out.append(len(doc))\n",
    "    return np.array(X_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_out = get_total_len(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 61,  52, 112, ...,  62,  12,  76])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('@HenryStreeten Yeh I know duh! Oh wait - I forgot the &quot;O&quot; haha sorry about that ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanup_module as Cmod\n",
    "\n",
    "transformer_ = Cmod.DocumentToNgramCounterTransformer(n_grams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = transformer_.fit_transform(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'NUMBER': 4, 'USERNAME': 1, 'taffitni': 1, 'ena': 1, 'mat': 1, 'ollich': 1, 'sbe': 1, 'el': 1, 'ir': 1, 'aaaaaaaaaaa': 1}),\n",
       "       Counter({'lot': 1, 'thing': 1, 'do': 1, 'URL': 1}),\n",
       "       Counter({'rain': 2, 'i': 2, 'hoping': 1, 'during': 1, 'graduation': 1, 'but': 1, 'only': 1, 'cause': 1, 'love': 1, 'not': 1, 'because': 1, 'want': 1, 'ceremony': 1, 'ruined': 1}),\n",
       "       Counter({'i': 2, 'USERNAME': 1, 'yeh': 1, 'know': 1, 'duh': 1, 'oh': 1, 'wait': 1, 'forgot': 1, 'o': 1, 'haha': 1, 'sorry': 1, 'about': 1}),\n",
       "       Counter({'EMOJI': 5, 'je': 2, 'USERNAME': 1, 'tys': 1, 't': 1, 'nikdy': 1, 'nekupoval': 1, 'zaj': 1, 'ce': 1, 'v': 1, 'pytli': 1, 'takov': 1, 'early': 1, 'registration': 1, 'prost': 1, 'risk': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCounterToFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, textlen=True):\n",
    "        self.textlen = textlen  \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        if self.textlen:\n",
    "            for word in X:\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows, cols, data = [], [], []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'USERNAME'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentToFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, expand_contractions=True, lower_case=True, \n",
    "                 replace_usernames=True, unescape_html=True, \n",
    "                 replace_urls=True, replace_numbers=True, \n",
    "                 \n",
    "                 remove_junk=True, remove_punctuation=True,\n",
    "                 \n",
    "                 textlen=True, cos_similarity=True, \n",
    "                 num_usernames=True, num_emojis=True, \n",
    "                 num_urls=True, num_punctuations=True,  \n",
    "                 num_asciichars=True, starts_username=True, \n",
    "                 starts_emoji=True, starts_url=True\n",
    "                ):\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_usernames = replace_usernames\n",
    "        self.unescape_html = unescape_html\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        \n",
    "        self.textlen = textlen\n",
    "        self.cos_similarity = cos_similarity\n",
    "        self.num_usernames = num_usernames\n",
    "        self.num_emojis = num_emojis\n",
    "        self.num_urls = num_urls\n",
    "        self.num_punctuations = num_punctuations\n",
    "        self.num_asciichars = num_asciichars\n",
    "        self.starts_username = starts_username\n",
    "        self.starts_emoji = starts_emoji\n",
    "        self.starts_url = starts_url\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for doc in X:\n",
    "            if self.lower_case:\n",
    "                doc = doc.lower()  \n",
    "            if self.replace_usernames:\n",
    "                doc = re.sub(r'^@([^\\s]+)',' USERNAME ', doc)\n",
    "            if self.unescape_html:\n",
    "                doc = unescape(doc)\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(doc)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    doc = doc.replace(url, ' URL ')\n",
    "            if self.replace_numbers:\n",
    "                doc = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', doc)\n",
    "            if self.remove_junk:\n",
    "                pattern = r'\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿|\\x82\\\n",
    "                            |\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                            |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "                doc = re.sub(pattern,'', doc)\n",
    "            if self.remove_punctuation:\n",
    "                doc = re.sub(r'\\W+', ' ', doc, flags=re.M)\n",
    "            if self.replace_emojis:\n",
    "                doc = re.sub(r'[^\\x00-\\x7F]+', ' EMOJI ', doc)\n",
    "            if self.replace_nonascii:\n",
    "                if is_ascii(doc) == False:\n",
    "                    doc = ' NONASCII '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            # tokenize\n",
    "            tokens = doc.split()\n",
    "            if self.remove_stopwords:\n",
    "                stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                              'has','he','in','is','it','its','of','on','that','the',\n",
    "                              'to','was','were','will','with']\n",
    "                tokens = [t for t in tokens if t not in stop_words]\n",
    "            if self.lemmatization and lemmatizer is not None:\n",
    "                tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            if self.n_grams:\n",
    "                for i in range(2, self.n_grams+1): # fix doubling of unigrams\n",
    "                    grams = ngrams(word_tokenize(doc), i)\n",
    "                    grams = ['_'.join(gram) for gram in grams]\n",
    "                    tokens = [*tokens, *grams]\n",
    "            # include counts\n",
    "            tokens_counts = Counter(tokens)\n",
    "            # append to list\n",
    "            X_transformed.append(tokens_counts)\n",
    "        return np.array(X_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
