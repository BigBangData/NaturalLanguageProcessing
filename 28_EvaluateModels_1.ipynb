{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation 1\n",
    "\n",
    "---\n",
    "\n",
    "__This Notebook__\n",
    "\n",
    "The original goal was to build a classifier that detected `spam` well (high sensitivity) but also detected `ham` well, possibly even better (high specificity), since it would be bad to send a legitimate message to the spam folder.\n",
    "\n",
    "Since specificity seemed not to be a problem, the entire modeling phase focused on sensitivity - this was also because of a confusion with the original tutorial which classified `ham` as the positive case (it was a ham detector not a spam detector) and so increasing sensitivity made sense in that scenario. \n",
    "\n",
    "\n",
    "__Results__\n",
    "\n",
    "A quick evaluation of a single confusion matrix per classifier shows that, at first glance, they generalize more or less well - as expected. The worst is perhaps the AdaBoost classifier, also as expected.\n",
    "\n",
    "__Next__\n",
    "\n",
    "Plotting some learning curves with the entire data (including the test set) and comparing them to the training learning curves might help us determine better how well the models generalize.\n",
    "\n",
    "Also verifying what kinds of mistakes (as in, what are the texts it misclassifies) trip up the classifiers in the training and test sets and comparing them might help understand whay needed to be done - this should've been done during training and modeling but I just thought about it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised on: 2021-02-21\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "    ShuffleSplit, StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import make_scorer, accuracy_score, \\\n",
    "    recall_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, \\\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \\\n",
    "    VotingClassifier\n",
    "\n",
    "import custom.evaluate_models as E\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "day, T = str(dt_object).split('.')[0].split(' ')\n",
    "print('Revised on: ' + day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target(data):\n",
    "    raw_path = os.path.join(\"data\",\"1_raw\")\n",
    "    filename = ''.join([data, \".csv\"])\n",
    "    out_dfm = pd.read_csv(os.path.join(raw_path, filename))\n",
    "    out_arr = np.array(out_dfm.iloc[:,0].ravel())\n",
    "    return out_arr\n",
    "\n",
    "y_train_array = load_target(\"y_train\")\n",
    "y_test_array = load_target(\"y_test\") \n",
    "\n",
    "def make_int(y_array):\n",
    "    y = y_array.copy()\n",
    "    y[y=='ham'] = 0\n",
    "    y[y=='spam'] = 1\n",
    "    y = y.astype('int')\n",
    "    return y\n",
    "\n",
    "y_train = make_int(y_train_array)\n",
    "y_test = make_int(y_test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_X(filename):\n",
    "    proc_dir = os.path.join(\"data\", \"2_processed\")\n",
    "    filename = ''.join([filename, '.npz'])\n",
    "    X = sp.load_npz(os.path.join(proc_dir, filename))\n",
    "    return X\n",
    "\n",
    "X_train = load_X('X_train_processed')\n",
    "X_test = load_X('X_test_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<3900x801 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 3123099 stored elements in COOrdinate format>,\n",
       " <1672x801 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1338471 stored elements in COOrdinate format>,\n",
       " (3900,),\n",
       " (1672,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity checks\n",
    "X_train, X_test, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Candidate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to use warm_start=True for learning curves\n",
    "\n",
    "rnd_clf1 = RandomForestClassifier(\n",
    "    random_state=42, n_estimators=100, max_features=150, \n",
    "    max_depth=8, min_samples_split=3, n_jobs=1) \n",
    "\n",
    "rnd_clf2 = RandomForestClassifier(\n",
    "    random_state=42, n_estimators=100, max_features=300, \n",
    "    max_depth=8, min_samples_split=3, n_jobs=1)\n",
    "    \n",
    "ada_clf =  AdaBoostClassifier(\n",
    "    random_state=42 , n_estimators=10, \n",
    "    learning_rate=0.001)\n",
    "\n",
    "gbc1a = GradientBoostingClassifier(\n",
    "    random_state=42, n_estimators=50, max_features=None, \n",
    "    max_depth=1, min_samples_split=2)\n",
    "\n",
    "gbc2a = GradientBoostingClassifier(\n",
    "    random_state=42, n_estimators=100, max_features=300, \n",
    "    max_depth=8, min_samples_split=5)\n",
    "\n",
    "gbc2c = GradientBoostingClassifier(\n",
    "    random_state=42, n_estimators=50, max_features=300, \n",
    "    max_depth=3, min_samples_split=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(clf, sets):\n",
    "    X_train, y_train, X_test, y_test = sets\n",
    "    E.fit_clf(clf, X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    E.eval_clf(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1m 1s\n",
      "          pred_neg  pred_pos\n",
      "cond_neg      1433         9\n",
      "cond_pos         7       223\n",
      "acc: 0.9904\n",
      "tpr: 0.9696\n",
      "tnr: 0.9938\n"
     ]
    }
   ],
   "source": [
    "eval_classifier(rnd_clf1, sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0m 56s\n",
      "          pred_neg  pred_pos\n",
      "cond_neg      1432        10\n",
      "cond_pos         7       223\n",
      "acc: 0.9898\n",
      "tpr: 0.9696\n",
      "tnr: 0.9931\n"
     ]
    }
   ],
   "source": [
    "eval_classifier(rnd_clf2, sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0m 3s\n",
      "          pred_neg  pred_pos\n",
      "cond_neg      1432        10\n",
      "cond_pos         8       222\n",
      "acc: 0.9892\n",
      "tpr: 0.9652\n",
      "tnr: 0.9931\n"
     ]
    }
   ],
   "source": [
    "eval_classifier(ada_clf, sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0m 14s\n",
      "          pred_neg  pred_pos\n",
      "cond_neg      1431        11\n",
      "cond_pos         7       223\n",
      "acc: 0.9892\n",
      "tpr: 0.9696\n",
      "tnr: 0.9924\n"
     ]
    }
   ],
   "source": [
    "eval_classifier(gbc1a, sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1m 18s\n",
      "          pred_neg  pred_pos\n",
      "cond_neg      1431        11\n",
      "cond_pos         6       224\n",
      "acc: 0.9898\n",
      "tpr: 0.9739\n",
      "tnr: 0.9924\n"
     ]
    }
   ],
   "source": [
    "eval_classifier(gbc2a, sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0m 15s\n",
      "          pred_neg  pred_pos\n",
      "cond_neg      1433         9\n",
      "cond_pos         7       223\n",
      "acc: 0.9904\n",
      "tpr: 0.9696\n",
      "tnr: 0.9938\n"
     ]
    }
   ],
   "source": [
    "eval_classifier(gbc2c, sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
