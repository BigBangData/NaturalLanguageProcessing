{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checklist**\n",
    "\n",
    "- check for dupes in ingestion? \n",
    "    - not needed, no dupes, and text dupes should be checked after cleaning\n",
    "- use textBlob as yet another way to perform basic sentiment analysis?\n",
    "    - yes, TODO, but maybe not during cleanup\n",
    "- create an incremental cleanup module?\n",
    "    - overkill, just reprocess everything bc of dupes, \n",
    "      script takes 8 secs for 20k tweets, that's ~6 min for a million if linear\n",
    "- improve script?\n",
    "    - add logs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import string\n",
    "import datetime\n",
    "import urlextract\n",
    "import pandas as pd\n",
    "\n",
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_data():\n",
    "    filepath = os.path.join(\"data\",\"raw\",\"tweets\") \n",
    "\n",
    "    dfm = []\n",
    "    for f in os.listdir(filepath):\n",
    "        dfm.append(pd.read_csv(os.path.join(filepath,f)))\n",
    "        \n",
    "    df = pd.concat(dfm)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for duplicated IDs\n",
    "ids = df[\"ID\"]\n",
    "df[ids.isin(ids[ids.duplicated()])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5882, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for duplicated Text\n",
    "txt = df[\"Text\"]\n",
    "df[txt.isin(txt[txt.duplicated()])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Iouisogolden</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OfValimaiyana</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MydoghatesMe2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosby_by</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diialexandria</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID\n",
       "User             \n",
       "Iouisogolden    4\n",
       "OfValimaiyana   4\n",
       "MydoghatesMe2   4\n",
       "cosby_by        3\n",
       "diialexandria   3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at users with more than 1 tweet?\n",
    "grouped = df[['User', 'ID']].groupby('User').count().sort_values('ID', ascending=False)\n",
    "\n",
    "grouped[grouped['ID']>1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Retweet Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_retweet(col):\n",
    "\n",
    "    for i in range(len(col)):\n",
    "        if re.match(r'^RT', col) is not None:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0      \n",
    "        \n",
    "def map_is_retweet(col):\n",
    "   \n",
    "    bool_map = map(lambda x: is_retweet(x), col)       \n",
    "    return(list(bool_map)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Retweet'] = map_is_retweet(df['Text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified and improved version\n",
    "def cleanup_tweet(tweet):\n",
    "\n",
    "    # make all lower case\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # remove URLs\n",
    "    # URL_pattern = r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*'\n",
    "    # tweet = re.sub(URL_pattern, '', tweet, flags=re.MULTILINE)\n",
    "    # better, albeit slower, version\n",
    "    urls = list(set(url_extractor.find_urls(tweet)))\n",
    "    if len(urls) > 0:\n",
    "        for url in urls:\n",
    "            tweet = tweet.replace(url, \"\")\n",
    "\n",
    "    # unescape HTML entities like &quot\n",
    "    tweet = unescape(tweet)\n",
    "    \n",
    "    # remove user references (including username) or hashtags, etc.     \n",
    "    pattern = r'\\@\\w+|\\#|\\¥|\\â|\\«|\\»|\\Ñ|\\Ð|\\¼|\\½|\\¾|\\!|\\?|\\¿\\\n",
    "                |\\x82|\\x83|\\x84|\\x85|\\x86|\\x87|\\x88|\\x89|\\\n",
    "                |\\x8a|\\x8b|\\x8c|\\x8d|\\x8e|\\°|\\µ|\\´|\\º|\\¹|\\³'\n",
    "    \n",
    "    tweet = re.sub(pattern, '', tweet)\n",
    "\n",
    "    # remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove emojis\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet).strip()\n",
    "    \n",
    "    # final test, if after utf-8 encoding it's not ascii decodable, discard\n",
    "    def is_ascii(text):\n",
    "        try:\n",
    "            text.encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    if is_ascii(tweet) == False:\n",
    "        return \" \"\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # remove stopwords\n",
    "    # NLTK's set(stopwords.words('english')) removes too many words\n",
    "    # using list of 25 semantically non-selective words (Reuters-RCV1 dataset)\n",
    "    stop_words = ['a','an','and','are','as','at','be','by','for','from',\n",
    "                  'has','he','in','is','it','its','of','on','that','the',\n",
    "                  'to','was','were','will','with','rt'] # plus rt for retweet\n",
    "\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup Tweet text\n",
    "# note: a map function takes just as long if not longer\n",
    "url_extractor = urlextract.URLExtract()\n",
    "df['Text'] = [cleanup_tweet(tweet) for tweet in df.loc[:,'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset with cols of interest\n",
    "sub_df = df[['ID','Retweet','Text','Polarity']].copy()\n",
    "\n",
    "# dedupe (text duplicates)\n",
    "dupes = sub_df[sub_df['Text'].duplicated(keep='first')]\n",
    "\n",
    "final_df = sub_df[~sub_df.ID.isin(list(dupes['ID']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>Text</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1302406168791470081</td>\n",
       "      <td>1</td>\n",
       "      <td>total inspir bykianamaiart peach so i tri make...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1302406168766078976</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1302406168170696706</td>\n",
       "      <td>0</td>\n",
       "      <td>some yall retweet realli have me look you side...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1302406168120365056</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you jen just miss old me</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1302406167956602880</td>\n",
       "      <td>0</td>\n",
       "      <td>my favorit part bad boy gangster mascot eunhyu...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID  Retweet  \\\n",
       "0  1302406168791470081        1   \n",
       "1  1302406168766078976        0   \n",
       "2  1302406168170696706        0   \n",
       "3  1302406168120365056        0   \n",
       "4  1302406167956602880        0   \n",
       "\n",
       "                                                Text  Polarity  \n",
       "0  total inspir bykianamaiart peach so i tri make...        -1  \n",
       "1                                          thank you        -1  \n",
       "2  some yall retweet realli have me look you side...        -1  \n",
       "3                     thank you jen just miss old me        -1  \n",
       "4  my favorit part bad boy gangster mascot eunhyu...        -1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_retweets = round(100*sum(final_df['Retweet'])/final_df.shape[0],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.02"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct_retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
